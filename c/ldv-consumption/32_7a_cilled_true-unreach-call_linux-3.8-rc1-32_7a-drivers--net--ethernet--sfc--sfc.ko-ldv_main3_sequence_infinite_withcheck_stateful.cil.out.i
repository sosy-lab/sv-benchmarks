extern void __VERIFIER_error() __attribute__ ((__noreturn__));
typedef signed char __s8;
typedef unsigned char __u8;
typedef short __s16;
typedef unsigned short __u16;
typedef int __s32;
typedef unsigned int __u32;
typedef long long __s64;
typedef unsigned long long __u64;
typedef signed char s8;
typedef unsigned char u8;
typedef short s16;
typedef unsigned short u16;
typedef int s32;
typedef unsigned int u32;
typedef long long s64;
typedef unsigned long long u64;
typedef long __kernel_long_t;
typedef unsigned long __kernel_ulong_t;
typedef int __kernel_pid_t;
typedef unsigned int __kernel_uid32_t;
typedef unsigned int __kernel_gid32_t;
typedef __kernel_ulong_t __kernel_size_t;
typedef __kernel_long_t __kernel_ssize_t;
typedef __kernel_long_t __kernel_off_t;
typedef long long __kernel_loff_t;
typedef __kernel_long_t __kernel_time_t;
typedef __kernel_long_t __kernel_clock_t;
typedef int __kernel_timer_t;
typedef int __kernel_clockid_t;
typedef __u16 __be16;
typedef __u32 __le32;
typedef __u32 __be32;
typedef __u64 __le64;
typedef __u32 __wsum;
typedef __u32 __kernel_dev_t;
typedef __kernel_dev_t dev_t;
typedef unsigned short umode_t;
typedef __u32 nlink_t;
typedef __kernel_off_t off_t;
typedef __kernel_pid_t pid_t;
typedef __kernel_clockid_t clockid_t;
typedef _Bool bool;
typedef __kernel_uid32_t uid_t;
typedef __kernel_gid32_t gid_t;
typedef __kernel_loff_t loff_t;
typedef __kernel_size_t size_t;
typedef __kernel_ssize_t ssize_t;
typedef __kernel_time_t time_t;
typedef __s32 int32_t;
typedef __u8 uint8_t;
typedef __u32 uint32_t;
typedef __u64 uint64_t;
typedef unsigned long sector_t;
typedef unsigned long blkcnt_t;
typedef u64 dma_addr_t;
typedef unsigned int gfp_t;
typedef unsigned int fmode_t;
typedef unsigned int oom_flags_t;
typedef u64 phys_addr_t;
typedef phys_addr_t resource_size_t;
struct __anonstruct_atomic_t_6 {
   int counter ;
};
typedef struct __anonstruct_atomic_t_6 atomic_t;
struct __anonstruct_atomic64_t_7 {
   long counter ;
};
typedef struct __anonstruct_atomic64_t_7 atomic64_t;
struct list_head {
   struct list_head *next ;
   struct list_head *prev ;
};
struct hlist_node;
struct hlist_head {
   struct hlist_node *first ;
};
struct hlist_node {
   struct hlist_node *next ;
   struct hlist_node **pprev ;
};
struct callback_head {
   struct callback_head *next ;
   void (*func)(struct callback_head * ) ;
};
struct module;
typedef void (*ctor_fn_t)(void);
struct file_operations;
struct _ddebug {
   char const *modname ;
   char const *function ;
   char const *filename ;
   char const *format ;
   unsigned int lineno : 18 ;
   unsigned char flags ;
};
struct device;
struct net_device;
struct completion;
struct pt_regs;
struct pid;
typedef u16 __ticket_t;
typedef u32 __ticketpair_t;
struct __raw_tickets {
   __ticket_t head ;
   __ticket_t tail ;
};
union __anonunion_ldv_2024_8 {
   __ticketpair_t head_tail ;
   struct __raw_tickets tickets ;
};
struct arch_spinlock {
   union __anonunion_ldv_2024_8 ldv_2024 ;
};
typedef struct arch_spinlock arch_spinlock_t;
struct __anonstruct_ldv_2031_10 {
   u32 read ;
   s32 write ;
};
union __anonunion_arch_rwlock_t_9 {
   s64 lock ;
   struct __anonstruct_ldv_2031_10 ldv_2031 ;
};
typedef union __anonunion_arch_rwlock_t_9 arch_rwlock_t;
struct task_struct;
struct lockdep_map;
struct mm_struct;
struct pt_regs {
   unsigned long r15 ;
   unsigned long r14 ;
   unsigned long r13 ;
   unsigned long r12 ;
   unsigned long bp ;
   unsigned long bx ;
   unsigned long r11 ;
   unsigned long r10 ;
   unsigned long r9 ;
   unsigned long r8 ;
   unsigned long ax ;
   unsigned long cx ;
   unsigned long dx ;
   unsigned long si ;
   unsigned long di ;
   unsigned long orig_ax ;
   unsigned long ip ;
   unsigned long cs ;
   unsigned long flags ;
   unsigned long sp ;
   unsigned long ss ;
};
struct __anonstruct_ldv_2096_12 {
   unsigned int a ;
   unsigned int b ;
};
struct __anonstruct_ldv_2111_13 {
   u16 limit0 ;
   u16 base0 ;
   unsigned char base1 ;
   unsigned char type : 4 ;
   unsigned char s : 1 ;
   unsigned char dpl : 2 ;
   unsigned char p : 1 ;
   unsigned char limit : 4 ;
   unsigned char avl : 1 ;
   unsigned char l : 1 ;
   unsigned char d : 1 ;
   unsigned char g : 1 ;
   unsigned char base2 ;
};
union __anonunion_ldv_2112_11 {
   struct __anonstruct_ldv_2096_12 ldv_2096 ;
   struct __anonstruct_ldv_2111_13 ldv_2111 ;
};
struct desc_struct {
   union __anonunion_ldv_2112_11 ldv_2112 ;
};
typedef unsigned long pgdval_t;
typedef unsigned long pgprotval_t;
struct pgprot {
   pgprotval_t pgprot ;
};
typedef struct pgprot pgprot_t;
struct __anonstruct_pgd_t_15 {
   pgdval_t pgd ;
};
typedef struct __anonstruct_pgd_t_15 pgd_t;
struct page;
typedef struct page *pgtable_t;
struct file;
struct seq_file;
struct thread_struct;
struct cpumask;
struct kernel_vm86_regs {
   struct pt_regs pt ;
   unsigned short es ;
   unsigned short __esh ;
   unsigned short ds ;
   unsigned short __dsh ;
   unsigned short fs ;
   unsigned short __fsh ;
   unsigned short gs ;
   unsigned short __gsh ;
};
union __anonunion_ldv_2767_18 {
   struct pt_regs *regs ;
   struct kernel_vm86_regs *vm86 ;
};
struct math_emu_info {
   long ___orig_eip ;
   union __anonunion_ldv_2767_18 ldv_2767 ;
};
struct bug_entry {
   int bug_addr_disp ;
   int file_disp ;
   unsigned short line ;
   unsigned short flags ;
};
struct cpumask {
   unsigned long bits[64U] ;
};
typedef struct cpumask cpumask_t;
typedef struct cpumask *cpumask_var_t;
struct static_key;
struct seq_operations;
struct i387_fsave_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20U] ;
   u32 status ;
};
struct __anonstruct_ldv_5125_23 {
   u64 rip ;
   u64 rdp ;
};
struct __anonstruct_ldv_5131_24 {
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
};
union __anonunion_ldv_5132_22 {
   struct __anonstruct_ldv_5125_23 ldv_5125 ;
   struct __anonstruct_ldv_5131_24 ldv_5131 ;
};
union __anonunion_ldv_5141_25 {
   u32 padding1[12U] ;
   u32 sw_reserved[12U] ;
};
struct i387_fxsave_struct {
   u16 cwd ;
   u16 swd ;
   u16 twd ;
   u16 fop ;
   union __anonunion_ldv_5132_22 ldv_5132 ;
   u32 mxcsr ;
   u32 mxcsr_mask ;
   u32 st_space[32U] ;
   u32 xmm_space[64U] ;
   u32 padding[12U] ;
   union __anonunion_ldv_5141_25 ldv_5141 ;
};
struct i387_soft_struct {
   u32 cwd ;
   u32 swd ;
   u32 twd ;
   u32 fip ;
   u32 fcs ;
   u32 foo ;
   u32 fos ;
   u32 st_space[20U] ;
   u8 ftop ;
   u8 changed ;
   u8 lookahead ;
   u8 no_update ;
   u8 rm ;
   u8 alimit ;
   struct math_emu_info *info ;
   u32 entry_eip ;
};
struct ymmh_struct {
   u32 ymmh_space[64U] ;
};
struct xsave_hdr_struct {
   u64 xstate_bv ;
   u64 reserved1[2U] ;
   u64 reserved2[5U] ;
};
struct xsave_struct {
   struct i387_fxsave_struct i387 ;
   struct xsave_hdr_struct xsave_hdr ;
   struct ymmh_struct ymmh ;
};
union thread_xstate {
   struct i387_fsave_struct fsave ;
   struct i387_fxsave_struct fxsave ;
   struct i387_soft_struct soft ;
   struct xsave_struct xsave ;
};
struct fpu {
   unsigned int last_cpu ;
   unsigned int has_fpu ;
   union thread_xstate *state ;
};
struct kmem_cache;
struct perf_event;
struct thread_struct {
   struct desc_struct tls_array[3U] ;
   unsigned long sp0 ;
   unsigned long sp ;
   unsigned long usersp ;
   unsigned short es ;
   unsigned short ds ;
   unsigned short fsindex ;
   unsigned short gsindex ;
   unsigned long fs ;
   unsigned long gs ;
   struct perf_event *ptrace_bps[4U] ;
   unsigned long debugreg6 ;
   unsigned long ptrace_dr7 ;
   unsigned long cr2 ;
   unsigned long trap_nr ;
   unsigned long error_code ;
   struct fpu fpu ;
   unsigned long *io_bitmap_ptr ;
   unsigned long iopl ;
   unsigned int io_bitmap_max ;
};
typedef atomic64_t atomic_long_t;
struct stack_trace {
   unsigned int nr_entries ;
   unsigned int max_entries ;
   unsigned long *entries ;
   int skip ;
};
struct lockdep_subclass_key {
   char __one_byte ;
} __attribute__((__packed__)) ;
struct lock_class_key {
   struct lockdep_subclass_key subkeys[8U] ;
};
struct lock_class {
   struct list_head hash_entry ;
   struct list_head lock_entry ;
   struct lockdep_subclass_key *key ;
   unsigned int subclass ;
   unsigned int dep_gen_id ;
   unsigned long usage_mask ;
   struct stack_trace usage_traces[13U] ;
   struct list_head locks_after ;
   struct list_head locks_before ;
   unsigned int version ;
   unsigned long ops ;
   char const *name ;
   int name_version ;
   unsigned long contention_point[4U] ;
   unsigned long contending_point[4U] ;
};
struct lockdep_map {
   struct lock_class_key *key ;
   struct lock_class *class_cache[2U] ;
   char const *name ;
   int cpu ;
   unsigned long ip ;
};
struct held_lock {
   u64 prev_chain_key ;
   unsigned long acquire_ip ;
   struct lockdep_map *instance ;
   struct lockdep_map *nest_lock ;
   u64 waittime_stamp ;
   u64 holdtime_stamp ;
   unsigned short class_idx : 13 ;
   unsigned char irq_context : 2 ;
   unsigned char trylock : 1 ;
   unsigned char read : 2 ;
   unsigned char check : 2 ;
   unsigned char hardirqs_off : 1 ;
   unsigned short references : 11 ;
};
struct raw_spinlock {
   arch_spinlock_t raw_lock ;
   unsigned int magic ;
   unsigned int owner_cpu ;
   void *owner ;
   struct lockdep_map dep_map ;
};
typedef struct raw_spinlock raw_spinlock_t;
struct __anonstruct_ldv_5960_29 {
   u8 __padding[24U] ;
   struct lockdep_map dep_map ;
};
union __anonunion_ldv_5961_28 {
   struct raw_spinlock rlock ;
   struct __anonstruct_ldv_5960_29 ldv_5960 ;
};
struct spinlock {
   union __anonunion_ldv_5961_28 ldv_5961 ;
};
typedef struct spinlock spinlock_t;
struct __anonstruct_rwlock_t_30 {
   arch_rwlock_t raw_lock ;
   unsigned int magic ;
   unsigned int owner_cpu ;
   void *owner ;
   struct lockdep_map dep_map ;
};
typedef struct __anonstruct_rwlock_t_30 rwlock_t;
struct mutex {
   atomic_t count ;
   spinlock_t wait_lock ;
   struct list_head wait_list ;
   struct task_struct *owner ;
   char const *name ;
   void *magic ;
   struct lockdep_map dep_map ;
};
struct mutex_waiter {
   struct list_head list ;
   struct task_struct *task ;
   void *magic ;
};
struct timespec;
struct __anonstruct_seqlock_t_35 {
   unsigned int sequence ;
   spinlock_t lock ;
};
typedef struct __anonstruct_seqlock_t_35 seqlock_t;
struct seqcount {
   unsigned int sequence ;
};
typedef struct seqcount seqcount_t;
struct timespec {
   __kernel_time_t tv_sec ;
   long tv_nsec ;
};
struct user_namespace;
typedef uid_t kuid_t;
typedef gid_t kgid_t;
struct kstat {
   u64 ino ;
   dev_t dev ;
   umode_t mode ;
   unsigned int nlink ;
   kuid_t uid ;
   kgid_t gid ;
   dev_t rdev ;
   loff_t size ;
   struct timespec atime ;
   struct timespec mtime ;
   struct timespec ctime ;
   unsigned long blksize ;
   unsigned long long blocks ;
};
struct __wait_queue_head {
   spinlock_t lock ;
   struct list_head task_list ;
};
typedef struct __wait_queue_head wait_queue_head_t;
struct __anonstruct_nodemask_t_36 {
   unsigned long bits[16U] ;
};
typedef struct __anonstruct_nodemask_t_36 nodemask_t;
struct rw_semaphore;
struct rw_semaphore {
   long count ;
   raw_spinlock_t wait_lock ;
   struct list_head wait_list ;
   struct lockdep_map dep_map ;
};
struct completion {
   unsigned int done ;
   wait_queue_head_t wait ;
};
struct notifier_block;
union ktime {
   s64 tv64 ;
};
typedef union ktime ktime_t;
struct tvec_base;
struct timer_list {
   struct list_head entry ;
   unsigned long expires ;
   struct tvec_base *base ;
   void (*function)(unsigned long ) ;
   unsigned long data ;
   int slack ;
   int start_pid ;
   void *start_site ;
   char start_comm[16U] ;
   struct lockdep_map lockdep_map ;
};
struct hrtimer;
enum hrtimer_restart;
struct workqueue_struct;
struct work_struct;
struct work_struct {
   atomic_long_t data ;
   struct list_head entry ;
   void (*func)(struct work_struct * ) ;
   struct lockdep_map lockdep_map ;
};
struct delayed_work {
   struct work_struct work ;
   struct timer_list timer ;
   int cpu ;
};
struct notifier_block {
   int (*notifier_call)(struct notifier_block * , unsigned long , void * ) ;
   struct notifier_block *next ;
   int priority ;
};
struct blocking_notifier_head {
   struct rw_semaphore rwsem ;
   struct notifier_block *head ;
};
struct ctl_table;
struct resource {
   resource_size_t start ;
   resource_size_t end ;
   char const *name ;
   unsigned long flags ;
   struct resource *parent ;
   struct resource *sibling ;
   struct resource *child ;
};
struct pci_dev;
struct pm_message {
   int event ;
};
typedef struct pm_message pm_message_t;
struct dev_pm_ops {
   int (*prepare)(struct device * ) ;
   void (*complete)(struct device * ) ;
   int (*suspend)(struct device * ) ;
   int (*resume)(struct device * ) ;
   int (*freeze)(struct device * ) ;
   int (*thaw)(struct device * ) ;
   int (*poweroff)(struct device * ) ;
   int (*restore)(struct device * ) ;
   int (*suspend_late)(struct device * ) ;
   int (*resume_early)(struct device * ) ;
   int (*freeze_late)(struct device * ) ;
   int (*thaw_early)(struct device * ) ;
   int (*poweroff_late)(struct device * ) ;
   int (*restore_early)(struct device * ) ;
   int (*suspend_noirq)(struct device * ) ;
   int (*resume_noirq)(struct device * ) ;
   int (*freeze_noirq)(struct device * ) ;
   int (*thaw_noirq)(struct device * ) ;
   int (*poweroff_noirq)(struct device * ) ;
   int (*restore_noirq)(struct device * ) ;
   int (*runtime_suspend)(struct device * ) ;
   int (*runtime_resume)(struct device * ) ;
   int (*runtime_idle)(struct device * ) ;
};
enum rpm_status {
    RPM_ACTIVE = 0,
    RPM_RESUMING = 1,
    RPM_SUSPENDED = 2,
    RPM_SUSPENDING = 3
} ;
enum rpm_request {
    RPM_REQ_NONE = 0,
    RPM_REQ_IDLE = 1,
    RPM_REQ_SUSPEND = 2,
    RPM_REQ_AUTOSUSPEND = 3,
    RPM_REQ_RESUME = 4
} ;
struct wakeup_source;
struct pm_subsys_data {
   spinlock_t lock ;
   unsigned int refcount ;
};
struct dev_pm_qos;
struct dev_pm_info {
   pm_message_t power_state ;
   unsigned char can_wakeup : 1 ;
   unsigned char async_suspend : 1 ;
   bool is_prepared ;
   bool is_suspended ;
   bool ignore_children ;
   bool early_init ;
   spinlock_t lock ;
   struct list_head entry ;
   struct completion completion ;
   struct wakeup_source *wakeup ;
   bool wakeup_path ;
   bool syscore ;
   struct timer_list suspend_timer ;
   unsigned long timer_expires ;
   struct work_struct work ;
   wait_queue_head_t wait_queue ;
   atomic_t usage_count ;
   atomic_t child_count ;
   unsigned char disable_depth : 3 ;
   unsigned char idle_notification : 1 ;
   unsigned char request_pending : 1 ;
   unsigned char deferred_resume : 1 ;
   unsigned char run_wake : 1 ;
   unsigned char runtime_auto : 1 ;
   unsigned char no_callbacks : 1 ;
   unsigned char irq_safe : 1 ;
   unsigned char use_autosuspend : 1 ;
   unsigned char timer_autosuspends : 1 ;
   enum rpm_request request ;
   enum rpm_status runtime_status ;
   int runtime_error ;
   int autosuspend_delay ;
   unsigned long last_busy ;
   unsigned long active_jiffies ;
   unsigned long suspended_jiffies ;
   unsigned long accounting_timestamp ;
   struct pm_subsys_data *subsys_data ;
   struct dev_pm_qos *qos ;
};
struct dev_pm_domain {
   struct dev_pm_ops ops ;
};
struct pci_bus;
struct __anonstruct_mm_context_t_101 {
   void *ldt ;
   int size ;
   unsigned short ia32_compat ;
   struct mutex lock ;
   void *vdso ;
};
typedef struct __anonstruct_mm_context_t_101 mm_context_t;
struct vm_area_struct;
struct rb_node {
   unsigned long __rb_parent_color ;
   struct rb_node *rb_right ;
   struct rb_node *rb_left ;
};
struct rb_root {
   struct rb_node *rb_node ;
};
struct nsproxy;
struct ctl_table_root;
struct ctl_table_header;
struct ctl_dir;
typedef int proc_handler(struct ctl_table * , int , void * , size_t * , loff_t * );
struct ctl_table_poll {
   atomic_t event ;
   wait_queue_head_t wait ;
};
struct ctl_table {
   char const *procname ;
   void *data ;
   int maxlen ;
   umode_t mode ;
   struct ctl_table *child ;
   proc_handler *proc_handler ;
   struct ctl_table_poll *poll ;
   void *extra1 ;
   void *extra2 ;
};
struct ctl_node {
   struct rb_node node ;
   struct ctl_table_header *header ;
};
struct __anonstruct_ldv_13267_129 {
   struct ctl_table *ctl_table ;
   int used ;
   int count ;
   int nreg ;
};
union __anonunion_ldv_13269_128 {
   struct __anonstruct_ldv_13267_129 ldv_13267 ;
   struct callback_head rcu ;
};
struct ctl_table_set;
struct ctl_table_header {
   union __anonunion_ldv_13269_128 ldv_13269 ;
   struct completion *unregistering ;
   struct ctl_table *ctl_table_arg ;
   struct ctl_table_root *root ;
   struct ctl_table_set *set ;
   struct ctl_dir *parent ;
   struct ctl_node *node ;
};
struct ctl_dir {
   struct ctl_table_header header ;
   struct rb_root root ;
};
struct ctl_table_set {
   int (*is_seen)(struct ctl_table_set * ) ;
   struct ctl_dir dir ;
};
struct ctl_table_root {
   struct ctl_table_set default_set ;
   struct ctl_table_set *(*lookup)(struct ctl_table_root * , struct nsproxy * ) ;
   int (*permissions)(struct ctl_table_header * , struct ctl_table * ) ;
};
struct cred;
typedef __u64 Elf64_Addr;
typedef __u16 Elf64_Half;
typedef __u32 Elf64_Word;
typedef __u64 Elf64_Xword;
struct elf64_sym {
   Elf64_Word st_name ;
   unsigned char st_info ;
   unsigned char st_other ;
   Elf64_Half st_shndx ;
   Elf64_Addr st_value ;
   Elf64_Xword st_size ;
};
typedef struct elf64_sym Elf64_Sym;
struct sock;
struct kobject;
enum kobj_ns_type {
    KOBJ_NS_TYPE_NONE = 0,
    KOBJ_NS_TYPE_NET = 1,
    KOBJ_NS_TYPES = 2
} ;
struct kobj_ns_type_operations {
   enum kobj_ns_type type ;
   void *(*grab_current_ns)(void) ;
   void const *(*netlink_ns)(struct sock * ) ;
   void const *(*initial_ns)(void) ;
   void (*drop_ns)(void * ) ;
};
struct attribute {
   char const *name ;
   umode_t mode ;
   bool ignore_lockdep ;
   struct lock_class_key *key ;
   struct lock_class_key skey ;
};
struct attribute_group {
   char const *name ;
   umode_t (*is_visible)(struct kobject * , struct attribute * , int ) ;
   struct attribute **attrs ;
};
struct bin_attribute {
   struct attribute attr ;
   size_t size ;
   void *private ;
   ssize_t (*read)(struct file * , struct kobject * , struct bin_attribute * , char * ,
                   loff_t , size_t ) ;
   ssize_t (*write)(struct file * , struct kobject * , struct bin_attribute * , char * ,
                    loff_t , size_t ) ;
   int (*mmap)(struct file * , struct kobject * , struct bin_attribute * , struct vm_area_struct * ) ;
};
struct sysfs_ops {
   ssize_t (*show)(struct kobject * , struct attribute * , char * ) ;
   ssize_t (*store)(struct kobject * , struct attribute * , char const * , size_t ) ;
   void const *(*namespace)(struct kobject * , struct attribute const * ) ;
};
struct sysfs_dirent;
struct kref {
   atomic_t refcount ;
};
struct kset;
struct kobj_type;
struct kobject {
   char const *name ;
   struct list_head entry ;
   struct kobject *parent ;
   struct kset *kset ;
   struct kobj_type *ktype ;
   struct sysfs_dirent *sd ;
   struct kref kref ;
   unsigned char state_initialized : 1 ;
   unsigned char state_in_sysfs : 1 ;
   unsigned char state_add_uevent_sent : 1 ;
   unsigned char state_remove_uevent_sent : 1 ;
   unsigned char uevent_suppress : 1 ;
};
struct kobj_type {
   void (*release)(struct kobject * ) ;
   struct sysfs_ops const *sysfs_ops ;
   struct attribute **default_attrs ;
   struct kobj_ns_type_operations const *(*child_ns_type)(struct kobject * ) ;
   void const *(*namespace)(struct kobject * ) ;
};
struct kobj_uevent_env {
   char *envp[32U] ;
   int envp_idx ;
   char buf[2048U] ;
   int buflen ;
};
struct kset_uevent_ops {
   int (* const filter)(struct kset * , struct kobject * ) ;
   char const *(* const name)(struct kset * , struct kobject * ) ;
   int (* const uevent)(struct kset * , struct kobject * , struct kobj_uevent_env * ) ;
};
struct kset {
   struct list_head list ;
   spinlock_t list_lock ;
   struct kobject kobj ;
   struct kset_uevent_ops const *uevent_ops ;
};
struct kernel_param;
struct kernel_param_ops {
   int (*set)(char const * , struct kernel_param const * ) ;
   int (*get)(char * , struct kernel_param const * ) ;
   void (*free)(void * ) ;
};
struct kparam_string;
struct kparam_array;
union __anonunion_ldv_14047_134 {
   void *arg ;
   struct kparam_string const *str ;
   struct kparam_array const *arr ;
};
struct kernel_param {
   char const *name ;
   struct kernel_param_ops const *ops ;
   u16 perm ;
   s16 level ;
   union __anonunion_ldv_14047_134 ldv_14047 ;
};
struct kparam_string {
   unsigned int maxlen ;
   char *string ;
};
struct kparam_array {
   unsigned int max ;
   unsigned int elemsize ;
   unsigned int *num ;
   struct kernel_param_ops const *ops ;
   void *elem ;
};
struct static_key {
   atomic_t enabled ;
};
struct tracepoint;
struct tracepoint_func {
   void *func ;
   void *data ;
};
struct tracepoint {
   char const *name ;
   struct static_key key ;
   void (*regfunc)(void) ;
   void (*unregfunc)(void) ;
   struct tracepoint_func *funcs ;
};
struct kernel_symbol {
   unsigned long value ;
   char const *name ;
};
struct mod_arch_specific {
};
struct module_param_attrs;
struct module_kobject {
   struct kobject kobj ;
   struct module *mod ;
   struct kobject *drivers_dir ;
   struct module_param_attrs *mp ;
};
struct module_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct module_attribute * , struct module_kobject * , char * ) ;
   ssize_t (*store)(struct module_attribute * , struct module_kobject * , char const * ,
                    size_t ) ;
   void (*setup)(struct module * , char const * ) ;
   int (*test)(struct module * ) ;
   void (*free)(struct module * ) ;
};
struct exception_table_entry;
enum module_state {
    MODULE_STATE_LIVE = 0,
    MODULE_STATE_COMING = 1,
    MODULE_STATE_GOING = 2
} ;
struct module_ref {
   unsigned long incs ;
   unsigned long decs ;
};
struct module_sect_attrs;
struct module_notes_attrs;
struct ftrace_event_call;
struct module {
   enum module_state state ;
   struct list_head list ;
   char name[56U] ;
   struct module_kobject mkobj ;
   struct module_attribute *modinfo_attrs ;
   char const *version ;
   char const *srcversion ;
   struct kobject *holders_dir ;
   struct kernel_symbol const *syms ;
   unsigned long const *crcs ;
   unsigned int num_syms ;
   struct kernel_param *kp ;
   unsigned int num_kp ;
   unsigned int num_gpl_syms ;
   struct kernel_symbol const *gpl_syms ;
   unsigned long const *gpl_crcs ;
   struct kernel_symbol const *unused_syms ;
   unsigned long const *unused_crcs ;
   unsigned int num_unused_syms ;
   unsigned int num_unused_gpl_syms ;
   struct kernel_symbol const *unused_gpl_syms ;
   unsigned long const *unused_gpl_crcs ;
   struct kernel_symbol const *gpl_future_syms ;
   unsigned long const *gpl_future_crcs ;
   unsigned int num_gpl_future_syms ;
   unsigned int num_exentries ;
   struct exception_table_entry *extable ;
   int (*init)(void) ;
   void *module_init ;
   void *module_core ;
   unsigned int init_size ;
   unsigned int core_size ;
   unsigned int init_text_size ;
   unsigned int core_text_size ;
   unsigned int init_ro_size ;
   unsigned int core_ro_size ;
   struct mod_arch_specific arch ;
   unsigned int taints ;
   unsigned int num_bugs ;
   struct list_head bug_list ;
   struct bug_entry *bug_table ;
   Elf64_Sym *symtab ;
   Elf64_Sym *core_symtab ;
   unsigned int num_symtab ;
   unsigned int core_num_syms ;
   char *strtab ;
   char *core_strtab ;
   struct module_sect_attrs *sect_attrs ;
   struct module_notes_attrs *notes_attrs ;
   char *args ;
   void *percpu ;
   unsigned int percpu_size ;
   unsigned int num_tracepoints ;
   struct tracepoint * const *tracepoints_ptrs ;
   unsigned int num_trace_bprintk_fmt ;
   char const **trace_bprintk_fmt_start ;
   struct ftrace_event_call **trace_events ;
   unsigned int num_trace_events ;
   struct list_head source_list ;
   struct list_head target_list ;
   struct task_struct *waiter ;
   void (*exit)(void) ;
   struct module_ref *refptr ;
   ctor_fn_t (**ctors)(void) ;
   unsigned int num_ctors ;
};
typedef unsigned long kernel_ulong_t;
struct pci_device_id {
   __u32 vendor ;
   __u32 device ;
   __u32 subvendor ;
   __u32 subdevice ;
   __u32 class ;
   __u32 class_mask ;
   kernel_ulong_t driver_data ;
};
struct acpi_device_id {
   __u8 id[16U] ;
   kernel_ulong_t driver_data ;
};
struct of_device_id {
   char name[32U] ;
   char type[32U] ;
   char compatible[128U] ;
   void const *data ;
};
struct klist_node;
struct klist_node {
   void *n_klist ;
   struct list_head n_node ;
   struct kref n_ref ;
};
struct dma_map_ops;
struct dev_archdata {
   struct dma_map_ops *dma_ops ;
   void *iommu ;
};
struct device_private;
struct device_driver;
struct driver_private;
struct class;
struct subsys_private;
struct bus_type;
struct device_node;
struct iommu_ops;
struct iommu_group;
struct bus_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct bus_type * , char * ) ;
   ssize_t (*store)(struct bus_type * , char const * , size_t ) ;
};
struct device_attribute;
struct driver_attribute;
struct bus_type {
   char const *name ;
   char const *dev_name ;
   struct device *dev_root ;
   struct bus_attribute *bus_attrs ;
   struct device_attribute *dev_attrs ;
   struct driver_attribute *drv_attrs ;
   int (*match)(struct device * , struct device_driver * ) ;
   int (*uevent)(struct device * , struct kobj_uevent_env * ) ;
   int (*probe)(struct device * ) ;
   int (*remove)(struct device * ) ;
   void (*shutdown)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct dev_pm_ops const *pm ;
   struct iommu_ops *iommu_ops ;
   struct subsys_private *p ;
};
struct device_type;
struct device_driver {
   char const *name ;
   struct bus_type *bus ;
   struct module *owner ;
   char const *mod_name ;
   bool suppress_bind_attrs ;
   struct of_device_id const *of_match_table ;
   struct acpi_device_id const *acpi_match_table ;
   int (*probe)(struct device * ) ;
   int (*remove)(struct device * ) ;
   void (*shutdown)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct attribute_group const **groups ;
   struct dev_pm_ops const *pm ;
   struct driver_private *p ;
};
struct driver_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct device_driver * , char * ) ;
   ssize_t (*store)(struct device_driver * , char const * , size_t ) ;
};
struct class_attribute;
struct class {
   char const *name ;
   struct module *owner ;
   struct class_attribute *class_attrs ;
   struct device_attribute *dev_attrs ;
   struct bin_attribute *dev_bin_attrs ;
   struct kobject *dev_kobj ;
   int (*dev_uevent)(struct device * , struct kobj_uevent_env * ) ;
   char *(*devnode)(struct device * , umode_t * ) ;
   void (*class_release)(struct class * ) ;
   void (*dev_release)(struct device * ) ;
   int (*suspend)(struct device * , pm_message_t ) ;
   int (*resume)(struct device * ) ;
   struct kobj_ns_type_operations const *ns_type ;
   void const *(*namespace)(struct device * ) ;
   struct dev_pm_ops const *pm ;
   struct subsys_private *p ;
};
struct class_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct class * , struct class_attribute * , char * ) ;
   ssize_t (*store)(struct class * , struct class_attribute * , char const * , size_t ) ;
   void const *(*namespace)(struct class * , struct class_attribute const * ) ;
};
struct device_type {
   char const *name ;
   struct attribute_group const **groups ;
   int (*uevent)(struct device * , struct kobj_uevent_env * ) ;
   char *(*devnode)(struct device * , umode_t * ) ;
   void (*release)(struct device * ) ;
   struct dev_pm_ops const *pm ;
};
struct device_attribute {
   struct attribute attr ;
   ssize_t (*show)(struct device * , struct device_attribute * , char * ) ;
   ssize_t (*store)(struct device * , struct device_attribute * , char const * ,
                    size_t ) ;
};
struct device_dma_parameters {
   unsigned int max_segment_size ;
   unsigned long segment_boundary_mask ;
};
struct acpi_dev_node {
   void *handle ;
};
struct dma_coherent_mem;
struct device {
   struct device *parent ;
   struct device_private *p ;
   struct kobject kobj ;
   char const *init_name ;
   struct device_type const *type ;
   struct mutex mutex ;
   struct bus_type *bus ;
   struct device_driver *driver ;
   void *platform_data ;
   struct dev_pm_info power ;
   struct dev_pm_domain *pm_domain ;
   int numa_node ;
   u64 *dma_mask ;
   u64 coherent_dma_mask ;
   struct device_dma_parameters *dma_parms ;
   struct list_head dma_pools ;
   struct dma_coherent_mem *dma_mem ;
   struct dev_archdata archdata ;
   struct device_node *of_node ;
   struct acpi_dev_node acpi_node ;
   dev_t devt ;
   u32 id ;
   spinlock_t devres_lock ;
   struct list_head devres_head ;
   struct klist_node knode_class ;
   struct class *class ;
   struct attribute_group const **groups ;
   void (*release)(struct device * ) ;
   struct iommu_group *iommu_group ;
};
struct wakeup_source {
   char const *name ;
   struct list_head entry ;
   spinlock_t lock ;
   struct timer_list timer ;
   unsigned long timer_expires ;
   ktime_t total_time ;
   ktime_t max_time ;
   ktime_t last_time ;
   ktime_t start_prevent_time ;
   ktime_t prevent_sleep_time ;
   unsigned long event_count ;
   unsigned long active_count ;
   unsigned long relax_count ;
   unsigned long expire_count ;
   unsigned long wakeup_count ;
   bool active ;
   bool autosleep_enabled ;
};
struct hotplug_slot;
struct pci_slot {
   struct pci_bus *bus ;
   struct list_head list ;
   struct hotplug_slot *hotplug ;
   unsigned char number ;
   struct kobject kobj ;
};
typedef int pci_power_t;
typedef unsigned int pci_channel_state_t;
enum pci_channel_state {
    pci_channel_io_normal = 1,
    pci_channel_io_frozen = 2,
    pci_channel_io_perm_failure = 3
} ;
typedef unsigned short pci_dev_flags_t;
typedef unsigned short pci_bus_flags_t;
struct pcie_link_state;
struct pci_vpd;
struct pci_sriov;
struct pci_ats;
struct proc_dir_entry;
struct pci_driver;
union __anonunion_ldv_15757_136 {
   struct pci_sriov *sriov ;
   struct pci_dev *physfn ;
};
struct pci_dev {
   struct list_head bus_list ;
   struct pci_bus *bus ;
   struct pci_bus *subordinate ;
   void *sysdata ;
   struct proc_dir_entry *procent ;
   struct pci_slot *slot ;
   unsigned int devfn ;
   unsigned short vendor ;
   unsigned short device ;
   unsigned short subsystem_vendor ;
   unsigned short subsystem_device ;
   unsigned int class ;
   u8 revision ;
   u8 hdr_type ;
   u8 pcie_cap ;
   unsigned char pcie_mpss : 3 ;
   u8 rom_base_reg ;
   u8 pin ;
   u16 pcie_flags_reg ;
   struct pci_driver *driver ;
   u64 dma_mask ;
   struct device_dma_parameters dma_parms ;
   pci_power_t current_state ;
   int pm_cap ;
   unsigned char pme_support : 5 ;
   unsigned char pme_interrupt : 1 ;
   unsigned char pme_poll : 1 ;
   unsigned char d1_support : 1 ;
   unsigned char d2_support : 1 ;
   unsigned char no_d1d2 : 1 ;
   unsigned char no_d3cold : 1 ;
   unsigned char d3cold_allowed : 1 ;
   unsigned char mmio_always_on : 1 ;
   unsigned char wakeup_prepared : 1 ;
   unsigned char runtime_d3cold : 1 ;
   unsigned int d3_delay ;
   unsigned int d3cold_delay ;
   struct pcie_link_state *link_state ;
   pci_channel_state_t error_state ;
   struct device dev ;
   int cfg_size ;
   unsigned int irq ;
   struct resource resource[17U] ;
   unsigned char transparent : 1 ;
   unsigned char multifunction : 1 ;
   unsigned char is_added : 1 ;
   unsigned char is_busmaster : 1 ;
   unsigned char no_msi : 1 ;
   unsigned char block_cfg_access : 1 ;
   unsigned char broken_parity_status : 1 ;
   unsigned char irq_reroute_variant : 2 ;
   unsigned char msi_enabled : 1 ;
   unsigned char msix_enabled : 1 ;
   unsigned char ari_enabled : 1 ;
   unsigned char is_managed : 1 ;
   unsigned char is_pcie : 1 ;
   unsigned char needs_freset : 1 ;
   unsigned char state_saved : 1 ;
   unsigned char is_physfn : 1 ;
   unsigned char is_virtfn : 1 ;
   unsigned char reset_fn : 1 ;
   unsigned char is_hotplug_bridge : 1 ;
   unsigned char __aer_firmware_first_valid : 1 ;
   unsigned char __aer_firmware_first : 1 ;
   unsigned char broken_intx_masking : 1 ;
   unsigned char io_window_1k : 1 ;
   pci_dev_flags_t dev_flags ;
   atomic_t enable_cnt ;
   u32 saved_config_space[16U] ;
   struct hlist_head saved_cap_space ;
   struct bin_attribute *rom_attr ;
   int rom_attr_enabled ;
   struct bin_attribute *res_attr[17U] ;
   struct bin_attribute *res_attr_wc[17U] ;
   struct list_head msi_list ;
   struct kset *msi_kset ;
   struct pci_vpd *vpd ;
   union __anonunion_ldv_15757_136 ldv_15757 ;
   struct pci_ats *ats ;
   phys_addr_t rom ;
   size_t romlen ;
};
struct pci_ops;
struct pci_bus {
   struct list_head node ;
   struct pci_bus *parent ;
   struct list_head children ;
   struct list_head devices ;
   struct pci_dev *self ;
   struct list_head slots ;
   struct resource *resource[4U] ;
   struct list_head resources ;
   struct resource busn_res ;
   struct pci_ops *ops ;
   void *sysdata ;
   struct proc_dir_entry *procdir ;
   unsigned char number ;
   unsigned char primary ;
   unsigned char max_bus_speed ;
   unsigned char cur_bus_speed ;
   char name[48U] ;
   unsigned short bridge_ctl ;
   pci_bus_flags_t bus_flags ;
   struct device *bridge ;
   struct device dev ;
   struct bin_attribute *legacy_io ;
   struct bin_attribute *legacy_mem ;
   unsigned char is_added : 1 ;
};
struct pci_ops {
   int (*read)(struct pci_bus * , unsigned int , int , int , u32 * ) ;
   int (*write)(struct pci_bus * , unsigned int , int , int , u32 ) ;
};
struct pci_dynids {
   spinlock_t lock ;
   struct list_head list ;
};
typedef unsigned int pci_ers_result_t;
struct pci_error_handlers {
   pci_ers_result_t (*error_detected)(struct pci_dev * , enum pci_channel_state ) ;
   pci_ers_result_t (*mmio_enabled)(struct pci_dev * ) ;
   pci_ers_result_t (*link_reset)(struct pci_dev * ) ;
   pci_ers_result_t (*slot_reset)(struct pci_dev * ) ;
   void (*resume)(struct pci_dev * ) ;
};
struct pci_driver {
   struct list_head node ;
   char const *name ;
   struct pci_device_id const *id_table ;
   int (*probe)(struct pci_dev * , struct pci_device_id const * ) ;
   void (*remove)(struct pci_dev * ) ;
   int (*suspend)(struct pci_dev * , pm_message_t ) ;
   int (*suspend_late)(struct pci_dev * , pm_message_t ) ;
   int (*resume_early)(struct pci_dev * ) ;
   int (*resume)(struct pci_dev * ) ;
   void (*shutdown)(struct pci_dev * ) ;
   int (*sriov_configure)(struct pci_dev * , int ) ;
   struct pci_error_handlers const *err_handler ;
   struct device_driver driver ;
   struct pci_dynids dynids ;
};
struct scatterlist {
   unsigned long sg_magic ;
   unsigned long page_link ;
   unsigned int offset ;
   unsigned int length ;
   dma_addr_t dma_address ;
   unsigned int dma_length ;
};
struct msix_entry {
   u32 vector ;
   u16 entry ;
};
struct inode;
struct arch_uprobe_task {
   unsigned long saved_scratch_register ;
   unsigned int saved_trap_nr ;
   unsigned int saved_tf ;
};
enum uprobe_task_state {
    UTASK_RUNNING = 0,
    UTASK_SSTEP = 1,
    UTASK_SSTEP_ACK = 2,
    UTASK_SSTEP_TRAPPED = 3
} ;
struct uprobe;
struct uprobe_task {
   enum uprobe_task_state state ;
   struct arch_uprobe_task autask ;
   struct uprobe *active_uprobe ;
   unsigned long xol_vaddr ;
   unsigned long vaddr ;
};
struct xol_area {
   wait_queue_head_t wq ;
   atomic_t slot_count ;
   unsigned long *bitmap ;
   struct page *page ;
   unsigned long vaddr ;
};
struct uprobes_state {
   struct xol_area *xol_area ;
};
struct address_space;
union __anonunion_ldv_16788_138 {
   unsigned long index ;
   void *freelist ;
   bool pfmemalloc ;
};
struct __anonstruct_ldv_16798_142 {
   unsigned short inuse ;
   unsigned short objects : 15 ;
   unsigned char frozen : 1 ;
};
union __anonunion_ldv_16800_141 {
   atomic_t _mapcount ;
   struct __anonstruct_ldv_16798_142 ldv_16798 ;
   int units ;
};
struct __anonstruct_ldv_16802_140 {
   union __anonunion_ldv_16800_141 ldv_16800 ;
   atomic_t _count ;
};
union __anonunion_ldv_16803_139 {
   unsigned long counters ;
   struct __anonstruct_ldv_16802_140 ldv_16802 ;
};
struct __anonstruct_ldv_16804_137 {
   union __anonunion_ldv_16788_138 ldv_16788 ;
   union __anonunion_ldv_16803_139 ldv_16803 ;
};
struct __anonstruct_ldv_16811_144 {
   struct page *next ;
   int pages ;
   int pobjects ;
};
struct slab;
union __anonunion_ldv_16815_143 {
   struct list_head lru ;
   struct __anonstruct_ldv_16811_144 ldv_16811 ;
   struct list_head list ;
   struct slab *slab_page ;
};
union __anonunion_ldv_16820_145 {
   unsigned long private ;
   struct kmem_cache *slab_cache ;
   struct page *first_page ;
};
struct page {
   unsigned long flags ;
   struct address_space *mapping ;
   struct __anonstruct_ldv_16804_137 ldv_16804 ;
   union __anonunion_ldv_16815_143 ldv_16815 ;
   union __anonunion_ldv_16820_145 ldv_16820 ;
   unsigned long debug_flags ;
   int _last_nid ;
};
struct page_frag {
   struct page *page ;
   __u32 offset ;
   __u32 size ;
};
struct __anonstruct_linear_147 {
   struct rb_node rb ;
   unsigned long rb_subtree_last ;
};
union __anonunion_shared_146 {
   struct __anonstruct_linear_147 linear ;
   struct list_head nonlinear ;
};
struct anon_vma;
struct vm_operations_struct;
struct mempolicy;
struct vm_area_struct {
   unsigned long vm_start ;
   unsigned long vm_end ;
   struct vm_area_struct *vm_next ;
   struct vm_area_struct *vm_prev ;
   struct rb_node vm_rb ;
   unsigned long rb_subtree_gap ;
   struct mm_struct *vm_mm ;
   pgprot_t vm_page_prot ;
   unsigned long vm_flags ;
   union __anonunion_shared_146 shared ;
   struct list_head anon_vma_chain ;
   struct anon_vma *anon_vma ;
   struct vm_operations_struct const *vm_ops ;
   unsigned long vm_pgoff ;
   struct file *vm_file ;
   void *vm_private_data ;
   struct mempolicy *vm_policy ;
};
struct core_thread {
   struct task_struct *task ;
   struct core_thread *next ;
};
struct core_state {
   atomic_t nr_threads ;
   struct core_thread dumper ;
   struct completion startup ;
};
struct mm_rss_stat {
   atomic_long_t count[3U] ;
};
struct linux_binfmt;
struct mmu_notifier_mm;
struct mm_struct {
   struct vm_area_struct *mmap ;
   struct rb_root mm_rb ;
   struct vm_area_struct *mmap_cache ;
   unsigned long (*get_unmapped_area)(struct file * , unsigned long , unsigned long ,
                                      unsigned long , unsigned long ) ;
   void (*unmap_area)(struct mm_struct * , unsigned long ) ;
   unsigned long mmap_base ;
   unsigned long task_size ;
   unsigned long cached_hole_size ;
   unsigned long free_area_cache ;
   unsigned long highest_vm_end ;
   pgd_t *pgd ;
   atomic_t mm_users ;
   atomic_t mm_count ;
   int map_count ;
   spinlock_t page_table_lock ;
   struct rw_semaphore mmap_sem ;
   struct list_head mmlist ;
   unsigned long hiwater_rss ;
   unsigned long hiwater_vm ;
   unsigned long total_vm ;
   unsigned long locked_vm ;
   unsigned long pinned_vm ;
   unsigned long shared_vm ;
   unsigned long exec_vm ;
   unsigned long stack_vm ;
   unsigned long def_flags ;
   unsigned long nr_ptes ;
   unsigned long start_code ;
   unsigned long end_code ;
   unsigned long start_data ;
   unsigned long end_data ;
   unsigned long start_brk ;
   unsigned long brk ;
   unsigned long start_stack ;
   unsigned long arg_start ;
   unsigned long arg_end ;
   unsigned long env_start ;
   unsigned long env_end ;
   unsigned long saved_auxv[44U] ;
   struct mm_rss_stat rss_stat ;
   struct linux_binfmt *binfmt ;
   cpumask_var_t cpu_vm_mask_var ;
   mm_context_t context ;
   unsigned long flags ;
   struct core_state *core_state ;
   spinlock_t ioctx_lock ;
   struct hlist_head ioctx_list ;
   struct task_struct *owner ;
   struct file *exe_file ;
   struct mmu_notifier_mm *mmu_notifier_mm ;
   pgtable_t pmd_huge_pte ;
   struct cpumask cpumask_allocation ;
   unsigned long numa_next_scan ;
   unsigned long numa_next_reset ;
   unsigned long numa_scan_offset ;
   int numa_scan_seq ;
   int first_nid ;
   struct uprobes_state uprobes_state ;
};
struct shrink_control {
   gfp_t gfp_mask ;
   unsigned long nr_to_scan ;
};
struct shrinker {
   int (*shrink)(struct shrinker * , struct shrink_control * ) ;
   int seeks ;
   long batch ;
   struct list_head list ;
   atomic_long_t nr_in_batch ;
};
struct file_ra_state;
struct user_struct;
struct writeback_control;
struct vm_fault {
   unsigned int flags ;
   unsigned long pgoff ;
   void *virtual_address ;
   struct page *page ;
};
struct vm_operations_struct {
   void (*open)(struct vm_area_struct * ) ;
   void (*close)(struct vm_area_struct * ) ;
   int (*fault)(struct vm_area_struct * , struct vm_fault * ) ;
   int (*page_mkwrite)(struct vm_area_struct * , struct vm_fault * ) ;
   int (*access)(struct vm_area_struct * , unsigned long , void * , int , int ) ;
   int (*set_policy)(struct vm_area_struct * , struct mempolicy * ) ;
   struct mempolicy *(*get_policy)(struct vm_area_struct * , unsigned long ) ;
   int (*migrate)(struct vm_area_struct * , nodemask_t const * , nodemask_t const * ,
                  unsigned long ) ;
   int (*remap_pages)(struct vm_area_struct * , unsigned long , unsigned long ,
                      unsigned long ) ;
};
struct mem_cgroup;
struct __anonstruct_ldv_19579_149 {
   struct mem_cgroup *memcg ;
   struct list_head list ;
   struct kmem_cache *root_cache ;
   bool dead ;
   atomic_t nr_pages ;
   struct work_struct destroy ;
};
union __anonunion_ldv_19580_148 {
   struct kmem_cache *memcg_caches[0U] ;
   struct __anonstruct_ldv_19579_149 ldv_19579 ;
};
struct memcg_cache_params {
   bool is_root_cache ;
   union __anonunion_ldv_19580_148 ldv_19580 ;
};
struct kmem_cache_cpu {
   void **freelist ;
   unsigned long tid ;
   struct page *page ;
   struct page *partial ;
   unsigned int stat[26U] ;
};
struct kmem_cache_node {
   spinlock_t list_lock ;
   unsigned long nr_partial ;
   struct list_head partial ;
   atomic_long_t nr_slabs ;
   atomic_long_t total_objects ;
   struct list_head full ;
};
struct kmem_cache_order_objects {
   unsigned long x ;
};
struct kmem_cache {
   struct kmem_cache_cpu *cpu_slab ;
   unsigned long flags ;
   unsigned long min_partial ;
   int size ;
   int object_size ;
   int offset ;
   int cpu_partial ;
   struct kmem_cache_order_objects oo ;
   struct kmem_cache_order_objects max ;
   struct kmem_cache_order_objects min ;
   gfp_t allocflags ;
   int refcount ;
   void (*ctor)(void * ) ;
   int inuse ;
   int align ;
   int reserved ;
   char const *name ;
   struct list_head list ;
   struct kobject kobj ;
   struct memcg_cache_params *memcg_params ;
   int max_attr_size ;
   int remote_node_defrag_ratio ;
   struct kmem_cache_node *node[1024U] ;
};
struct dma_attrs {
   unsigned long flags[1U] ;
};
enum dma_data_direction {
    DMA_BIDIRECTIONAL = 0,
    DMA_TO_DEVICE = 1,
    DMA_FROM_DEVICE = 2,
    DMA_NONE = 3
} ;
struct sg_table {
   struct scatterlist *sgl ;
   unsigned int nents ;
   unsigned int orig_nents ;
};
struct dma_map_ops {
   void *(*alloc)(struct device * , size_t , dma_addr_t * , gfp_t , struct dma_attrs * ) ;
   void (*free)(struct device * , size_t , void * , dma_addr_t , struct dma_attrs * ) ;
   int (*mmap)(struct device * , struct vm_area_struct * , void * , dma_addr_t ,
               size_t , struct dma_attrs * ) ;
   int (*get_sgtable)(struct device * , struct sg_table * , void * , dma_addr_t ,
                      size_t , struct dma_attrs * ) ;
   dma_addr_t (*map_page)(struct device * , struct page * , unsigned long , size_t ,
                          enum dma_data_direction , struct dma_attrs * ) ;
   void (*unmap_page)(struct device * , dma_addr_t , size_t , enum dma_data_direction ,
                      struct dma_attrs * ) ;
   int (*map_sg)(struct device * , struct scatterlist * , int , enum dma_data_direction ,
                 struct dma_attrs * ) ;
   void (*unmap_sg)(struct device * , struct scatterlist * , int , enum dma_data_direction ,
                    struct dma_attrs * ) ;
   void (*sync_single_for_cpu)(struct device * , dma_addr_t , size_t , enum dma_data_direction ) ;
   void (*sync_single_for_device)(struct device * , dma_addr_t , size_t , enum dma_data_direction ) ;
   void (*sync_sg_for_cpu)(struct device * , struct scatterlist * , int , enum dma_data_direction ) ;
   void (*sync_sg_for_device)(struct device * , struct scatterlist * , int , enum dma_data_direction ) ;
   int (*mapping_error)(struct device * , dma_addr_t ) ;
   int (*dma_supported)(struct device * , u64 ) ;
   int (*set_dma_mask)(struct device * , u64 ) ;
   int is_phys ;
};
struct plist_head {
   struct list_head node_list ;
};
struct plist_node {
   int prio ;
   struct list_head prio_list ;
   struct list_head node_list ;
};
struct pm_qos_request {
   struct plist_node node ;
   int pm_qos_class ;
   struct delayed_work work ;
};
struct pm_qos_flags_request {
   struct list_head node ;
   s32 flags ;
};
enum dev_pm_qos_req_type {
    DEV_PM_QOS_LATENCY = 1,
    DEV_PM_QOS_FLAGS = 2
} ;
union __anonunion_data_150 {
   struct plist_node pnode ;
   struct pm_qos_flags_request flr ;
};
struct dev_pm_qos_request {
   enum dev_pm_qos_req_type type ;
   union __anonunion_data_150 data ;
   struct device *dev ;
};
enum pm_qos_type {
    PM_QOS_UNITIALIZED = 0,
    PM_QOS_MAX = 1,
    PM_QOS_MIN = 2
} ;
struct pm_qos_constraints {
   struct plist_head list ;
   s32 target_value ;
   s32 default_value ;
   enum pm_qos_type type ;
   struct blocking_notifier_head *notifiers ;
};
struct pm_qos_flags {
   struct list_head list ;
   s32 effective_flags ;
};
struct dev_pm_qos {
   struct pm_qos_constraints latency ;
   struct pm_qos_flags flags ;
   struct dev_pm_qos_request *latency_req ;
   struct dev_pm_qos_request *flags_req ;
};
struct iovec {
   void *iov_base ;
   __kernel_size_t iov_len ;
};
typedef s32 dma_cookie_t;
struct dql {
   unsigned int num_queued ;
   unsigned int adj_limit ;
   unsigned int last_obj_cnt ;
   unsigned int limit ;
   unsigned int num_completed ;
   unsigned int prev_ovlimit ;
   unsigned int prev_num_queued ;
   unsigned int prev_last_obj_cnt ;
   unsigned int lowest_slack ;
   unsigned long slack_start_time ;
   unsigned int max_limit ;
   unsigned int min_limit ;
   unsigned int slack_hold_time ;
};
struct sem_undo_list;
struct sysv_sem {
   struct sem_undo_list *undo_list ;
};
typedef unsigned short __kernel_sa_family_t;
typedef __kernel_sa_family_t sa_family_t;
struct sockaddr {
   sa_family_t sa_family ;
   char sa_data[14U] ;
};
struct msghdr {
   void *msg_name ;
   int msg_namelen ;
   struct iovec *msg_iov ;
   __kernel_size_t msg_iovlen ;
   void *msg_control ;
   __kernel_size_t msg_controllen ;
   unsigned int msg_flags ;
};
struct __anonstruct_sync_serial_settings_152 {
   unsigned int clock_rate ;
   unsigned int clock_type ;
   unsigned short loopback ;
};
typedef struct __anonstruct_sync_serial_settings_152 sync_serial_settings;
struct __anonstruct_te1_settings_153 {
   unsigned int clock_rate ;
   unsigned int clock_type ;
   unsigned short loopback ;
   unsigned int slot_map ;
};
typedef struct __anonstruct_te1_settings_153 te1_settings;
struct __anonstruct_raw_hdlc_proto_154 {
   unsigned short encoding ;
   unsigned short parity ;
};
typedef struct __anonstruct_raw_hdlc_proto_154 raw_hdlc_proto;
struct __anonstruct_fr_proto_155 {
   unsigned int t391 ;
   unsigned int t392 ;
   unsigned int n391 ;
   unsigned int n392 ;
   unsigned int n393 ;
   unsigned short lmi ;
   unsigned short dce ;
};
typedef struct __anonstruct_fr_proto_155 fr_proto;
struct __anonstruct_fr_proto_pvc_156 {
   unsigned int dlci ;
};
typedef struct __anonstruct_fr_proto_pvc_156 fr_proto_pvc;
struct __anonstruct_fr_proto_pvc_info_157 {
   unsigned int dlci ;
   char master[16U] ;
};
typedef struct __anonstruct_fr_proto_pvc_info_157 fr_proto_pvc_info;
struct __anonstruct_cisco_proto_158 {
   unsigned int interval ;
   unsigned int timeout ;
};
typedef struct __anonstruct_cisco_proto_158 cisco_proto;
struct ifmap {
   unsigned long mem_start ;
   unsigned long mem_end ;
   unsigned short base_addr ;
   unsigned char irq ;
   unsigned char dma ;
   unsigned char port ;
};
union __anonunion_ifs_ifsu_159 {
   raw_hdlc_proto *raw_hdlc ;
   cisco_proto *cisco ;
   fr_proto *fr ;
   fr_proto_pvc *fr_pvc ;
   fr_proto_pvc_info *fr_pvc_info ;
   sync_serial_settings *sync ;
   te1_settings *te1 ;
};
struct if_settings {
   unsigned int type ;
   unsigned int size ;
   union __anonunion_ifs_ifsu_159 ifs_ifsu ;
};
union __anonunion_ifr_ifrn_160 {
   char ifrn_name[16U] ;
};
union __anonunion_ifr_ifru_161 {
   struct sockaddr ifru_addr ;
   struct sockaddr ifru_dstaddr ;
   struct sockaddr ifru_broadaddr ;
   struct sockaddr ifru_netmask ;
   struct sockaddr ifru_hwaddr ;
   short ifru_flags ;
   int ifru_ivalue ;
   int ifru_mtu ;
   struct ifmap ifru_map ;
   char ifru_slave[16U] ;
   char ifru_newname[16U] ;
   void *ifru_data ;
   struct if_settings ifru_settings ;
};
struct ifreq {
   union __anonunion_ifr_ifrn_160 ifr_ifrn ;
   union __anonunion_ifr_ifru_161 ifr_ifru ;
};
struct hlist_bl_node;
struct hlist_bl_head {
   struct hlist_bl_node *first ;
};
struct hlist_bl_node {
   struct hlist_bl_node *next ;
   struct hlist_bl_node **pprev ;
};
struct nameidata;
struct path;
struct vfsmount;
struct __anonstruct_ldv_22231_164 {
   u32 hash ;
   u32 len ;
};
union __anonunion_ldv_22233_163 {
   struct __anonstruct_ldv_22231_164 ldv_22231 ;
   u64 hash_len ;
};
struct qstr {
   union __anonunion_ldv_22233_163 ldv_22233 ;
   unsigned char const *name ;
};
struct dentry_operations;
struct super_block;
union __anonunion_d_u_165 {
   struct list_head d_child ;
   struct callback_head d_rcu ;
};
struct dentry {
   unsigned int d_flags ;
   seqcount_t d_seq ;
   struct hlist_bl_node d_hash ;
   struct dentry *d_parent ;
   struct qstr d_name ;
   struct inode *d_inode ;
   unsigned char d_iname[32U] ;
   unsigned int d_count ;
   spinlock_t d_lock ;
   struct dentry_operations const *d_op ;
   struct super_block *d_sb ;
   unsigned long d_time ;
   void *d_fsdata ;
   struct list_head d_lru ;
   union __anonunion_d_u_165 d_u ;
   struct list_head d_subdirs ;
   struct hlist_node d_alias ;
};
struct dentry_operations {
   int (*d_revalidate)(struct dentry * , unsigned int ) ;
   int (*d_hash)(struct dentry const * , struct inode const * , struct qstr * ) ;
   int (*d_compare)(struct dentry const * , struct inode const * , struct dentry const * ,
                    struct inode const * , unsigned int , char const * , struct qstr const * ) ;
   int (*d_delete)(struct dentry const * ) ;
   void (*d_release)(struct dentry * ) ;
   void (*d_prune)(struct dentry * ) ;
   void (*d_iput)(struct dentry * , struct inode * ) ;
   char *(*d_dname)(struct dentry * , char * , int ) ;
   struct vfsmount *(*d_automount)(struct path * ) ;
   int (*d_manage)(struct dentry * , bool ) ;
};
struct path {
   struct vfsmount *mnt ;
   struct dentry *dentry ;
};
struct radix_tree_node;
struct radix_tree_root {
   unsigned int height ;
   gfp_t gfp_mask ;
   struct radix_tree_node *rnode ;
};
enum pid_type {
    PIDTYPE_PID = 0,
    PIDTYPE_PGID = 1,
    PIDTYPE_SID = 2,
    PIDTYPE_MAX = 3
} ;
struct pid_namespace;
struct upid {
   int nr ;
   struct pid_namespace *ns ;
   struct hlist_node pid_chain ;
};
struct pid {
   atomic_t count ;
   unsigned int level ;
   struct hlist_head tasks[3U] ;
   struct callback_head rcu ;
   struct upid numbers[1U] ;
};
struct pid_link {
   struct hlist_node node ;
   struct pid *pid ;
};
struct kernel_cap_struct {
   __u32 cap[2U] ;
};
typedef struct kernel_cap_struct kernel_cap_t;
struct fiemap_extent {
   __u64 fe_logical ;
   __u64 fe_physical ;
   __u64 fe_length ;
   __u64 fe_reserved64[2U] ;
   __u32 fe_flags ;
   __u32 fe_reserved[3U] ;
};
enum migrate_mode {
    MIGRATE_ASYNC = 0,
    MIGRATE_SYNC_LIGHT = 1,
    MIGRATE_SYNC = 2
} ;
struct block_device;
struct io_context;
struct cgroup_subsys_state;
struct export_operations;
struct kiocb;
struct pipe_inode_info;
struct poll_table_struct;
struct kstatfs;
struct swap_info_struct;
struct iattr {
   unsigned int ia_valid ;
   umode_t ia_mode ;
   kuid_t ia_uid ;
   kgid_t ia_gid ;
   loff_t ia_size ;
   struct timespec ia_atime ;
   struct timespec ia_mtime ;
   struct timespec ia_ctime ;
   struct file *ia_file ;
};
struct percpu_counter {
   raw_spinlock_t lock ;
   s64 count ;
   struct list_head list ;
   s32 *counters ;
};
struct fs_disk_quota {
   __s8 d_version ;
   __s8 d_flags ;
   __u16 d_fieldmask ;
   __u32 d_id ;
   __u64 d_blk_hardlimit ;
   __u64 d_blk_softlimit ;
   __u64 d_ino_hardlimit ;
   __u64 d_ino_softlimit ;
   __u64 d_bcount ;
   __u64 d_icount ;
   __s32 d_itimer ;
   __s32 d_btimer ;
   __u16 d_iwarns ;
   __u16 d_bwarns ;
   __s32 d_padding2 ;
   __u64 d_rtb_hardlimit ;
   __u64 d_rtb_softlimit ;
   __u64 d_rtbcount ;
   __s32 d_rtbtimer ;
   __u16 d_rtbwarns ;
   __s16 d_padding3 ;
   char d_padding4[8U] ;
};
struct fs_qfilestat {
   __u64 qfs_ino ;
   __u64 qfs_nblks ;
   __u32 qfs_nextents ;
};
typedef struct fs_qfilestat fs_qfilestat_t;
struct fs_quota_stat {
   __s8 qs_version ;
   __u16 qs_flags ;
   __s8 qs_pad ;
   fs_qfilestat_t qs_uquota ;
   fs_qfilestat_t qs_gquota ;
   __u32 qs_incoredqs ;
   __s32 qs_btimelimit ;
   __s32 qs_itimelimit ;
   __s32 qs_rtbtimelimit ;
   __u16 qs_bwarnlimit ;
   __u16 qs_iwarnlimit ;
};
struct dquot;
typedef __kernel_uid32_t projid_t;
typedef projid_t kprojid_t;
struct if_dqinfo {
   __u64 dqi_bgrace ;
   __u64 dqi_igrace ;
   __u32 dqi_flags ;
   __u32 dqi_valid ;
};
enum quota_type {
    USRQUOTA = 0,
    GRPQUOTA = 1,
    PRJQUOTA = 2
} ;
typedef long long qsize_t;
union __anonunion_ldv_23236_167 {
   kuid_t uid ;
   kgid_t gid ;
   kprojid_t projid ;
};
struct kqid {
   union __anonunion_ldv_23236_167 ldv_23236 ;
   enum quota_type type ;
};
struct mem_dqblk {
   qsize_t dqb_bhardlimit ;
   qsize_t dqb_bsoftlimit ;
   qsize_t dqb_curspace ;
   qsize_t dqb_rsvspace ;
   qsize_t dqb_ihardlimit ;
   qsize_t dqb_isoftlimit ;
   qsize_t dqb_curinodes ;
   time_t dqb_btime ;
   time_t dqb_itime ;
};
struct quota_format_type;
struct mem_dqinfo {
   struct quota_format_type *dqi_format ;
   int dqi_fmt_id ;
   struct list_head dqi_dirty_list ;
   unsigned long dqi_flags ;
   unsigned int dqi_bgrace ;
   unsigned int dqi_igrace ;
   qsize_t dqi_maxblimit ;
   qsize_t dqi_maxilimit ;
   void *dqi_priv ;
};
struct dquot {
   struct hlist_node dq_hash ;
   struct list_head dq_inuse ;
   struct list_head dq_free ;
   struct list_head dq_dirty ;
   struct mutex dq_lock ;
   atomic_t dq_count ;
   wait_queue_head_t dq_wait_unused ;
   struct super_block *dq_sb ;
   struct kqid dq_id ;
   loff_t dq_off ;
   unsigned long dq_flags ;
   struct mem_dqblk dq_dqb ;
};
struct quota_format_ops {
   int (*check_quota_file)(struct super_block * , int ) ;
   int (*read_file_info)(struct super_block * , int ) ;
   int (*write_file_info)(struct super_block * , int ) ;
   int (*free_file_info)(struct super_block * , int ) ;
   int (*read_dqblk)(struct dquot * ) ;
   int (*commit_dqblk)(struct dquot * ) ;
   int (*release_dqblk)(struct dquot * ) ;
};
struct dquot_operations {
   int (*write_dquot)(struct dquot * ) ;
   struct dquot *(*alloc_dquot)(struct super_block * , int ) ;
   void (*destroy_dquot)(struct dquot * ) ;
   int (*acquire_dquot)(struct dquot * ) ;
   int (*release_dquot)(struct dquot * ) ;
   int (*mark_dirty)(struct dquot * ) ;
   int (*write_info)(struct super_block * , int ) ;
   qsize_t *(*get_reserved_space)(struct inode * ) ;
};
struct quotactl_ops {
   int (*quota_on)(struct super_block * , int , int , struct path * ) ;
   int (*quota_on_meta)(struct super_block * , int , int ) ;
   int (*quota_off)(struct super_block * , int ) ;
   int (*quota_sync)(struct super_block * , int ) ;
   int (*get_info)(struct super_block * , int , struct if_dqinfo * ) ;
   int (*set_info)(struct super_block * , int , struct if_dqinfo * ) ;
   int (*get_dqblk)(struct super_block * , struct kqid , struct fs_disk_quota * ) ;
   int (*set_dqblk)(struct super_block * , struct kqid , struct fs_disk_quota * ) ;
   int (*get_xstate)(struct super_block * , struct fs_quota_stat * ) ;
   int (*set_xstate)(struct super_block * , unsigned int , int ) ;
};
struct quota_format_type {
   int qf_fmt_id ;
   struct quota_format_ops const *qf_ops ;
   struct module *qf_owner ;
   struct quota_format_type *qf_next ;
};
struct quota_info {
   unsigned int flags ;
   struct mutex dqio_mutex ;
   struct mutex dqonoff_mutex ;
   struct rw_semaphore dqptr_sem ;
   struct inode *files[2U] ;
   struct mem_dqinfo info[2U] ;
   struct quota_format_ops const *ops[2U] ;
};
union __anonunion_arg_169 {
   char *buf ;
   void *data ;
};
struct __anonstruct_read_descriptor_t_168 {
   size_t written ;
   size_t count ;
   union __anonunion_arg_169 arg ;
   int error ;
};
typedef struct __anonstruct_read_descriptor_t_168 read_descriptor_t;
struct address_space_operations {
   int (*writepage)(struct page * , struct writeback_control * ) ;
   int (*readpage)(struct file * , struct page * ) ;
   int (*writepages)(struct address_space * , struct writeback_control * ) ;
   int (*set_page_dirty)(struct page * ) ;
   int (*readpages)(struct file * , struct address_space * , struct list_head * ,
                    unsigned int ) ;
   int (*write_begin)(struct file * , struct address_space * , loff_t , unsigned int ,
                      unsigned int , struct page ** , void ** ) ;
   int (*write_end)(struct file * , struct address_space * , loff_t , unsigned int ,
                    unsigned int , struct page * , void * ) ;
   sector_t (*bmap)(struct address_space * , sector_t ) ;
   void (*invalidatepage)(struct page * , unsigned long ) ;
   int (*releasepage)(struct page * , gfp_t ) ;
   void (*freepage)(struct page * ) ;
   ssize_t (*direct_IO)(int , struct kiocb * , struct iovec const * , loff_t ,
                        unsigned long ) ;
   int (*get_xip_mem)(struct address_space * , unsigned long , int , void ** , unsigned long * ) ;
   int (*migratepage)(struct address_space * , struct page * , struct page * , enum migrate_mode ) ;
   int (*launder_page)(struct page * ) ;
   int (*is_partially_uptodate)(struct page * , read_descriptor_t * , unsigned long ) ;
   int (*error_remove_page)(struct address_space * , struct page * ) ;
   int (*swap_activate)(struct swap_info_struct * , struct file * , sector_t * ) ;
   void (*swap_deactivate)(struct file * ) ;
};
struct backing_dev_info;
struct address_space {
   struct inode *host ;
   struct radix_tree_root page_tree ;
   spinlock_t tree_lock ;
   unsigned int i_mmap_writable ;
   struct rb_root i_mmap ;
   struct list_head i_mmap_nonlinear ;
   struct mutex i_mmap_mutex ;
   unsigned long nrpages ;
   unsigned long writeback_index ;
   struct address_space_operations const *a_ops ;
   unsigned long flags ;
   struct backing_dev_info *backing_dev_info ;
   spinlock_t private_lock ;
   struct list_head private_list ;
   void *private_data ;
};
struct request_queue;
struct hd_struct;
struct gendisk;
struct block_device {
   dev_t bd_dev ;
   int bd_openers ;
   struct inode *bd_inode ;
   struct super_block *bd_super ;
   struct mutex bd_mutex ;
   struct list_head bd_inodes ;
   void *bd_claiming ;
   void *bd_holder ;
   int bd_holders ;
   bool bd_write_holder ;
   struct list_head bd_holder_disks ;
   struct block_device *bd_contains ;
   unsigned int bd_block_size ;
   struct hd_struct *bd_part ;
   unsigned int bd_part_count ;
   int bd_invalidated ;
   struct gendisk *bd_disk ;
   struct request_queue *bd_queue ;
   struct list_head bd_list ;
   unsigned long bd_private ;
   int bd_fsfreeze_count ;
   struct mutex bd_fsfreeze_mutex ;
};
struct posix_acl;
struct inode_operations;
union __anonunion_ldv_23670_170 {
   unsigned int const i_nlink ;
   unsigned int __i_nlink ;
};
union __anonunion_ldv_23690_171 {
   struct hlist_head i_dentry ;
   struct callback_head i_rcu ;
};
struct file_lock;
struct cdev;
union __anonunion_ldv_23706_172 {
   struct pipe_inode_info *i_pipe ;
   struct block_device *i_bdev ;
   struct cdev *i_cdev ;
};
struct inode {
   umode_t i_mode ;
   unsigned short i_opflags ;
   kuid_t i_uid ;
   kgid_t i_gid ;
   unsigned int i_flags ;
   struct posix_acl *i_acl ;
   struct posix_acl *i_default_acl ;
   struct inode_operations const *i_op ;
   struct super_block *i_sb ;
   struct address_space *i_mapping ;
   void *i_security ;
   unsigned long i_ino ;
   union __anonunion_ldv_23670_170 ldv_23670 ;
   dev_t i_rdev ;
   loff_t i_size ;
   struct timespec i_atime ;
   struct timespec i_mtime ;
   struct timespec i_ctime ;
   spinlock_t i_lock ;
   unsigned short i_bytes ;
   unsigned int i_blkbits ;
   blkcnt_t i_blocks ;
   unsigned long i_state ;
   struct mutex i_mutex ;
   unsigned long dirtied_when ;
   struct hlist_node i_hash ;
   struct list_head i_wb_list ;
   struct list_head i_lru ;
   struct list_head i_sb_list ;
   union __anonunion_ldv_23690_171 ldv_23690 ;
   u64 i_version ;
   atomic_t i_count ;
   atomic_t i_dio_count ;
   atomic_t i_writecount ;
   struct file_operations const *i_fop ;
   struct file_lock *i_flock ;
   struct address_space i_data ;
   struct dquot *i_dquot[2U] ;
   struct list_head i_devices ;
   union __anonunion_ldv_23706_172 ldv_23706 ;
   __u32 i_generation ;
   __u32 i_fsnotify_mask ;
   struct hlist_head i_fsnotify_marks ;
   atomic_t i_readcount ;
   void *i_private ;
};
struct fown_struct {
   rwlock_t lock ;
   struct pid *pid ;
   enum pid_type pid_type ;
   kuid_t uid ;
   kuid_t euid ;
   int signum ;
};
struct file_ra_state {
   unsigned long start ;
   unsigned int size ;
   unsigned int async_size ;
   unsigned int ra_pages ;
   unsigned int mmap_miss ;
   loff_t prev_pos ;
};
union __anonunion_f_u_173 {
   struct list_head fu_list ;
   struct callback_head fu_rcuhead ;
};
struct file {
   union __anonunion_f_u_173 f_u ;
   struct path f_path ;
   struct file_operations const *f_op ;
   spinlock_t f_lock ;
   int f_sb_list_cpu ;
   atomic_long_t f_count ;
   unsigned int f_flags ;
   fmode_t f_mode ;
   loff_t f_pos ;
   struct fown_struct f_owner ;
   struct cred const *f_cred ;
   struct file_ra_state f_ra ;
   u64 f_version ;
   void *f_security ;
   void *private_data ;
   struct list_head f_ep_links ;
   struct list_head f_tfile_llink ;
   struct address_space *f_mapping ;
   unsigned long f_mnt_write_state ;
};
struct files_struct;
typedef struct files_struct *fl_owner_t;
struct file_lock_operations {
   void (*fl_copy_lock)(struct file_lock * , struct file_lock * ) ;
   void (*fl_release_private)(struct file_lock * ) ;
};
struct lock_manager_operations {
   int (*lm_compare_owner)(struct file_lock * , struct file_lock * ) ;
   void (*lm_notify)(struct file_lock * ) ;
   int (*lm_grant)(struct file_lock * , struct file_lock * , int ) ;
   void (*lm_break)(struct file_lock * ) ;
   int (*lm_change)(struct file_lock ** , int ) ;
};
struct net;
struct nlm_lockowner;
struct nfs_lock_info {
   u32 state ;
   struct nlm_lockowner *owner ;
   struct list_head list ;
};
struct nfs4_lock_state;
struct nfs4_lock_info {
   struct nfs4_lock_state *owner ;
};
struct fasync_struct;
struct __anonstruct_afs_175 {
   struct list_head link ;
   int state ;
};
union __anonunion_fl_u_174 {
   struct nfs_lock_info nfs_fl ;
   struct nfs4_lock_info nfs4_fl ;
   struct __anonstruct_afs_175 afs ;
};
struct file_lock {
   struct file_lock *fl_next ;
   struct list_head fl_link ;
   struct list_head fl_block ;
   fl_owner_t fl_owner ;
   unsigned int fl_flags ;
   unsigned char fl_type ;
   unsigned int fl_pid ;
   struct pid *fl_nspid ;
   wait_queue_head_t fl_wait ;
   struct file *fl_file ;
   loff_t fl_start ;
   loff_t fl_end ;
   struct fasync_struct *fl_fasync ;
   unsigned long fl_break_time ;
   unsigned long fl_downgrade_time ;
   struct file_lock_operations const *fl_ops ;
   struct lock_manager_operations const *fl_lmops ;
   union __anonunion_fl_u_174 fl_u ;
};
struct fasync_struct {
   spinlock_t fa_lock ;
   int magic ;
   int fa_fd ;
   struct fasync_struct *fa_next ;
   struct file *fa_file ;
   struct callback_head fa_rcu ;
};
struct sb_writers {
   struct percpu_counter counter[3U] ;
   wait_queue_head_t wait ;
   int frozen ;
   wait_queue_head_t wait_unfrozen ;
   struct lockdep_map lock_map[3U] ;
};
struct file_system_type;
struct super_operations;
struct xattr_handler;
struct mtd_info;
struct super_block {
   struct list_head s_list ;
   dev_t s_dev ;
   unsigned char s_blocksize_bits ;
   unsigned long s_blocksize ;
   loff_t s_maxbytes ;
   struct file_system_type *s_type ;
   struct super_operations const *s_op ;
   struct dquot_operations const *dq_op ;
   struct quotactl_ops const *s_qcop ;
   struct export_operations const *s_export_op ;
   unsigned long s_flags ;
   unsigned long s_magic ;
   struct dentry *s_root ;
   struct rw_semaphore s_umount ;
   int s_count ;
   atomic_t s_active ;
   void *s_security ;
   struct xattr_handler const **s_xattr ;
   struct list_head s_inodes ;
   struct hlist_bl_head s_anon ;
   struct list_head *s_files ;
   struct list_head s_mounts ;
   struct list_head s_dentry_lru ;
   int s_nr_dentry_unused ;
   spinlock_t s_inode_lru_lock ;
   struct list_head s_inode_lru ;
   int s_nr_inodes_unused ;
   struct block_device *s_bdev ;
   struct backing_dev_info *s_bdi ;
   struct mtd_info *s_mtd ;
   struct hlist_node s_instances ;
   struct quota_info s_dquot ;
   struct sb_writers s_writers ;
   char s_id[32U] ;
   u8 s_uuid[16U] ;
   void *s_fs_info ;
   unsigned int s_max_links ;
   fmode_t s_mode ;
   u32 s_time_gran ;
   struct mutex s_vfs_rename_mutex ;
   char *s_subtype ;
   char *s_options ;
   struct dentry_operations const *s_d_op ;
   int cleancache_poolid ;
   struct shrinker s_shrink ;
   atomic_long_t s_remove_count ;
   int s_readonly_remount ;
};
struct fiemap_extent_info {
   unsigned int fi_flags ;
   unsigned int fi_extents_mapped ;
   unsigned int fi_extents_max ;
   struct fiemap_extent *fi_extents_start ;
};
struct file_operations {
   struct module *owner ;
   loff_t (*llseek)(struct file * , loff_t , int ) ;
   ssize_t (*read)(struct file * , char * , size_t , loff_t * ) ;
   ssize_t (*write)(struct file * , char const * , size_t , loff_t * ) ;
   ssize_t (*aio_read)(struct kiocb * , struct iovec const * , unsigned long ,
                       loff_t ) ;
   ssize_t (*aio_write)(struct kiocb * , struct iovec const * , unsigned long ,
                        loff_t ) ;
   int (*readdir)(struct file * , void * , int (*)(void * , char const * , int ,
                                                   loff_t , u64 , unsigned int ) ) ;
   unsigned int (*poll)(struct file * , struct poll_table_struct * ) ;
   long (*unlocked_ioctl)(struct file * , unsigned int , unsigned long ) ;
   long (*compat_ioctl)(struct file * , unsigned int , unsigned long ) ;
   int (*mmap)(struct file * , struct vm_area_struct * ) ;
   int (*open)(struct inode * , struct file * ) ;
   int (*flush)(struct file * , fl_owner_t ) ;
   int (*release)(struct inode * , struct file * ) ;
   int (*fsync)(struct file * , loff_t , loff_t , int ) ;
   int (*aio_fsync)(struct kiocb * , int ) ;
   int (*fasync)(int , struct file * , int ) ;
   int (*lock)(struct file * , int , struct file_lock * ) ;
   ssize_t (*sendpage)(struct file * , struct page * , int , size_t , loff_t * ,
                       int ) ;
   unsigned long (*get_unmapped_area)(struct file * , unsigned long , unsigned long ,
                                      unsigned long , unsigned long ) ;
   int (*check_flags)(int ) ;
   int (*flock)(struct file * , int , struct file_lock * ) ;
   ssize_t (*splice_write)(struct pipe_inode_info * , struct file * , loff_t * , size_t ,
                           unsigned int ) ;
   ssize_t (*splice_read)(struct file * , loff_t * , struct pipe_inode_info * , size_t ,
                          unsigned int ) ;
   int (*setlease)(struct file * , long , struct file_lock ** ) ;
   long (*fallocate)(struct file * , int , loff_t , loff_t ) ;
   int (*show_fdinfo)(struct seq_file * , struct file * ) ;
};
struct inode_operations {
   struct dentry *(*lookup)(struct inode * , struct dentry * , unsigned int ) ;
   void *(*follow_link)(struct dentry * , struct nameidata * ) ;
   int (*permission)(struct inode * , int ) ;
   struct posix_acl *(*get_acl)(struct inode * , int ) ;
   int (*readlink)(struct dentry * , char * , int ) ;
   void (*put_link)(struct dentry * , struct nameidata * , void * ) ;
   int (*create)(struct inode * , struct dentry * , umode_t , bool ) ;
   int (*link)(struct dentry * , struct inode * , struct dentry * ) ;
   int (*unlink)(struct inode * , struct dentry * ) ;
   int (*symlink)(struct inode * , struct dentry * , char const * ) ;
   int (*mkdir)(struct inode * , struct dentry * , umode_t ) ;
   int (*rmdir)(struct inode * , struct dentry * ) ;
   int (*mknod)(struct inode * , struct dentry * , umode_t , dev_t ) ;
   int (*rename)(struct inode * , struct dentry * , struct inode * , struct dentry * ) ;
   int (*setattr)(struct dentry * , struct iattr * ) ;
   int (*getattr)(struct vfsmount * , struct dentry * , struct kstat * ) ;
   int (*setxattr)(struct dentry * , char const * , void const * , size_t , int ) ;
   ssize_t (*getxattr)(struct dentry * , char const * , void * , size_t ) ;
   ssize_t (*listxattr)(struct dentry * , char * , size_t ) ;
   int (*removexattr)(struct dentry * , char const * ) ;
   int (*fiemap)(struct inode * , struct fiemap_extent_info * , u64 , u64 ) ;
   int (*update_time)(struct inode * , struct timespec * , int ) ;
   int (*atomic_open)(struct inode * , struct dentry * , struct file * , unsigned int ,
                      umode_t , int * ) ;
};
struct super_operations {
   struct inode *(*alloc_inode)(struct super_block * ) ;
   void (*destroy_inode)(struct inode * ) ;
   void (*dirty_inode)(struct inode * , int ) ;
   int (*write_inode)(struct inode * , struct writeback_control * ) ;
   int (*drop_inode)(struct inode * ) ;
   void (*evict_inode)(struct inode * ) ;
   void (*put_super)(struct super_block * ) ;
   int (*sync_fs)(struct super_block * , int ) ;
   int (*freeze_fs)(struct super_block * ) ;
   int (*unfreeze_fs)(struct super_block * ) ;
   int (*statfs)(struct dentry * , struct kstatfs * ) ;
   int (*remount_fs)(struct super_block * , int * , char * ) ;
   void (*umount_begin)(struct super_block * ) ;
   int (*show_options)(struct seq_file * , struct dentry * ) ;
   int (*show_devname)(struct seq_file * , struct dentry * ) ;
   int (*show_path)(struct seq_file * , struct dentry * ) ;
   int (*show_stats)(struct seq_file * , struct dentry * ) ;
   ssize_t (*quota_read)(struct super_block * , int , char * , size_t , loff_t ) ;
   ssize_t (*quota_write)(struct super_block * , int , char const * , size_t ,
                          loff_t ) ;
   int (*bdev_try_to_free_page)(struct super_block * , struct page * , gfp_t ) ;
   int (*nr_cached_objects)(struct super_block * ) ;
   void (*free_cached_objects)(struct super_block * , int ) ;
};
struct file_system_type {
   char const *name ;
   int fs_flags ;
   struct dentry *(*mount)(struct file_system_type * , int , char const * , void * ) ;
   void (*kill_sb)(struct super_block * ) ;
   struct module *owner ;
   struct file_system_type *next ;
   struct hlist_head fs_supers ;
   struct lock_class_key s_lock_key ;
   struct lock_class_key s_umount_key ;
   struct lock_class_key s_vfs_rename_key ;
   struct lock_class_key s_writers_key[3U] ;
   struct lock_class_key i_lock_key ;
   struct lock_class_key i_mutex_key ;
   struct lock_class_key i_mutex_dir_key ;
};
struct io_event {
   __u64 data ;
   __u64 obj ;
   __s64 res ;
   __s64 res2 ;
};
typedef unsigned long cputime_t;
struct __anonstruct_sigset_t_176 {
   unsigned long sig[1U] ;
};
typedef struct __anonstruct_sigset_t_176 sigset_t;
struct siginfo;
typedef void __signalfn_t(int );
typedef __signalfn_t *__sighandler_t;
typedef void __restorefn_t(void);
typedef __restorefn_t *__sigrestore_t;
struct sigaction {
   __sighandler_t sa_handler ;
   unsigned long sa_flags ;
   __sigrestore_t sa_restorer ;
   sigset_t sa_mask ;
};
struct k_sigaction {
   struct sigaction sa ;
};
union sigval {
   int sival_int ;
   void *sival_ptr ;
};
typedef union sigval sigval_t;
struct __anonstruct__kill_178 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
};
struct __anonstruct__timer_179 {
   __kernel_timer_t _tid ;
   int _overrun ;
   char _pad[0U] ;
   sigval_t _sigval ;
   int _sys_private ;
};
struct __anonstruct__rt_180 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
   sigval_t _sigval ;
};
struct __anonstruct__sigchld_181 {
   __kernel_pid_t _pid ;
   __kernel_uid32_t _uid ;
   int _status ;
   __kernel_clock_t _utime ;
   __kernel_clock_t _stime ;
};
struct __anonstruct__sigfault_182 {
   void *_addr ;
   short _addr_lsb ;
};
struct __anonstruct__sigpoll_183 {
   long _band ;
   int _fd ;
};
struct __anonstruct__sigsys_184 {
   void *_call_addr ;
   int _syscall ;
   unsigned int _arch ;
};
union __anonunion__sifields_177 {
   int _pad[28U] ;
   struct __anonstruct__kill_178 _kill ;
   struct __anonstruct__timer_179 _timer ;
   struct __anonstruct__rt_180 _rt ;
   struct __anonstruct__sigchld_181 _sigchld ;
   struct __anonstruct__sigfault_182 _sigfault ;
   struct __anonstruct__sigpoll_183 _sigpoll ;
   struct __anonstruct__sigsys_184 _sigsys ;
};
struct siginfo {
   int si_signo ;
   int si_errno ;
   int si_code ;
   union __anonunion__sifields_177 _sifields ;
};
typedef struct siginfo siginfo_t;
struct sigpending {
   struct list_head list ;
   sigset_t signal ;
};
struct seccomp_filter;
struct seccomp {
   int mode ;
   struct seccomp_filter *filter ;
};
struct rt_mutex_waiter;
struct rlimit {
   unsigned long rlim_cur ;
   unsigned long rlim_max ;
};
struct timerqueue_node {
   struct rb_node node ;
   ktime_t expires ;
};
struct timerqueue_head {
   struct rb_root head ;
   struct timerqueue_node *next ;
};
struct hrtimer_clock_base;
struct hrtimer_cpu_base;
enum hrtimer_restart {
    HRTIMER_NORESTART = 0,
    HRTIMER_RESTART = 1
} ;
struct hrtimer {
   struct timerqueue_node node ;
   ktime_t _softexpires ;
   enum hrtimer_restart (*function)(struct hrtimer * ) ;
   struct hrtimer_clock_base *base ;
   unsigned long state ;
   int start_pid ;
   void *start_site ;
   char start_comm[16U] ;
};
struct hrtimer_clock_base {
   struct hrtimer_cpu_base *cpu_base ;
   int index ;
   clockid_t clockid ;
   struct timerqueue_head active ;
   ktime_t resolution ;
   ktime_t (*get_time)(void) ;
   ktime_t softirq_time ;
   ktime_t offset ;
};
struct hrtimer_cpu_base {
   raw_spinlock_t lock ;
   unsigned int active_bases ;
   unsigned int clock_was_set ;
   ktime_t expires_next ;
   int hres_active ;
   int hang_detected ;
   unsigned long nr_events ;
   unsigned long nr_retries ;
   unsigned long nr_hangs ;
   ktime_t max_hang_time ;
   struct hrtimer_clock_base clock_base[3U] ;
};
struct task_io_accounting {
   u64 rchar ;
   u64 wchar ;
   u64 syscr ;
   u64 syscw ;
   u64 read_bytes ;
   u64 write_bytes ;
   u64 cancelled_write_bytes ;
};
struct latency_record {
   unsigned long backtrace[12U] ;
   unsigned int count ;
   unsigned long time ;
   unsigned long max ;
};
typedef int32_t key_serial_t;
typedef uint32_t key_perm_t;
struct key;
struct signal_struct;
struct key_type;
struct keyring_list;
union __anonunion_ldv_26520_187 {
   struct list_head graveyard_link ;
   struct rb_node serial_node ;
};
struct key_user;
union __anonunion_ldv_26529_188 {
   time_t expiry ;
   time_t revoked_at ;
};
union __anonunion_type_data_189 {
   struct list_head link ;
   unsigned long x[2U] ;
   void *p[2U] ;
   int reject_error ;
};
union __anonunion_payload_190 {
   unsigned long value ;
   void *rcudata ;
   void *data ;
   struct keyring_list *subscriptions ;
};
struct key {
   atomic_t usage ;
   key_serial_t serial ;
   union __anonunion_ldv_26520_187 ldv_26520 ;
   struct key_type *type ;
   struct rw_semaphore sem ;
   struct key_user *user ;
   void *security ;
   union __anonunion_ldv_26529_188 ldv_26529 ;
   time_t last_used_at ;
   kuid_t uid ;
   kgid_t gid ;
   key_perm_t perm ;
   unsigned short quotalen ;
   unsigned short datalen ;
   unsigned long flags ;
   char *description ;
   union __anonunion_type_data_189 type_data ;
   union __anonunion_payload_190 payload ;
};
struct audit_context;
struct group_info {
   atomic_t usage ;
   int ngroups ;
   int nblocks ;
   kgid_t small_block[32U] ;
   kgid_t *blocks[0U] ;
};
struct thread_group_cred;
struct cred {
   atomic_t usage ;
   atomic_t subscribers ;
   void *put_addr ;
   unsigned int magic ;
   kuid_t uid ;
   kgid_t gid ;
   kuid_t suid ;
   kgid_t sgid ;
   kuid_t euid ;
   kgid_t egid ;
   kuid_t fsuid ;
   kgid_t fsgid ;
   unsigned int securebits ;
   kernel_cap_t cap_inheritable ;
   kernel_cap_t cap_permitted ;
   kernel_cap_t cap_effective ;
   kernel_cap_t cap_bset ;
   unsigned char jit_keyring ;
   struct key *session_keyring ;
   struct key *process_keyring ;
   struct key *thread_keyring ;
   struct key *request_key_auth ;
   struct thread_group_cred *tgcred ;
   void *security ;
   struct user_struct *user ;
   struct user_namespace *user_ns ;
   struct group_info *group_info ;
   struct callback_head rcu ;
};
struct llist_node;
struct llist_node {
   struct llist_node *next ;
};
struct futex_pi_state;
struct robust_list_head;
struct bio_list;
struct fs_struct;
struct perf_event_context;
struct blk_plug;
struct cfs_rq;
struct task_group;
struct kioctx;
union __anonunion_ki_obj_191 {
   void *user ;
   struct task_struct *tsk ;
};
struct eventfd_ctx;
struct kiocb {
   struct list_head ki_run_list ;
   unsigned long ki_flags ;
   int ki_users ;
   unsigned int ki_key ;
   struct file *ki_filp ;
   struct kioctx *ki_ctx ;
   int (*ki_cancel)(struct kiocb * , struct io_event * ) ;
   ssize_t (*ki_retry)(struct kiocb * ) ;
   void (*ki_dtor)(struct kiocb * ) ;
   union __anonunion_ki_obj_191 ki_obj ;
   __u64 ki_user_data ;
   loff_t ki_pos ;
   void *private ;
   unsigned short ki_opcode ;
   size_t ki_nbytes ;
   char *ki_buf ;
   size_t ki_left ;
   struct iovec ki_inline_vec ;
   struct iovec *ki_iovec ;
   unsigned long ki_nr_segs ;
   unsigned long ki_cur_seg ;
   struct list_head ki_list ;
   struct list_head ki_batch ;
   struct eventfd_ctx *ki_eventfd ;
};
struct aio_ring_info {
   unsigned long mmap_base ;
   unsigned long mmap_size ;
   struct page **ring_pages ;
   spinlock_t ring_lock ;
   long nr_pages ;
   unsigned int nr ;
   unsigned int tail ;
   struct page *internal_pages[8U] ;
};
struct kioctx {
   atomic_t users ;
   int dead ;
   struct mm_struct *mm ;
   unsigned long user_id ;
   struct hlist_node list ;
   wait_queue_head_t wait ;
   spinlock_t ctx_lock ;
   int reqs_active ;
   struct list_head active_reqs ;
   struct list_head run_list ;
   unsigned int max_reqs ;
   struct aio_ring_info ring_info ;
   struct delayed_work wq ;
   struct callback_head callback_head ;
};
struct sighand_struct {
   atomic_t count ;
   struct k_sigaction action[64U] ;
   spinlock_t siglock ;
   wait_queue_head_t signalfd_wqh ;
};
struct pacct_struct {
   int ac_flag ;
   long ac_exitcode ;
   unsigned long ac_mem ;
   cputime_t ac_utime ;
   cputime_t ac_stime ;
   unsigned long ac_minflt ;
   unsigned long ac_majflt ;
};
struct cpu_itimer {
   cputime_t expires ;
   cputime_t incr ;
   u32 error ;
   u32 incr_error ;
};
struct cputime {
   cputime_t utime ;
   cputime_t stime ;
};
struct task_cputime {
   cputime_t utime ;
   cputime_t stime ;
   unsigned long long sum_exec_runtime ;
};
struct thread_group_cputimer {
   struct task_cputime cputime ;
   int running ;
   raw_spinlock_t lock ;
};
struct autogroup;
struct tty_struct;
struct taskstats;
struct tty_audit_buf;
struct signal_struct {
   atomic_t sigcnt ;
   atomic_t live ;
   int nr_threads ;
   wait_queue_head_t wait_chldexit ;
   struct task_struct *curr_target ;
   struct sigpending shared_pending ;
   int group_exit_code ;
   int notify_count ;
   struct task_struct *group_exit_task ;
   int group_stop_count ;
   unsigned int flags ;
   unsigned char is_child_subreaper : 1 ;
   unsigned char has_child_subreaper : 1 ;
   struct list_head posix_timers ;
   struct hrtimer real_timer ;
   struct pid *leader_pid ;
   ktime_t it_real_incr ;
   struct cpu_itimer it[2U] ;
   struct thread_group_cputimer cputimer ;
   struct task_cputime cputime_expires ;
   struct list_head cpu_timers[3U] ;
   struct pid *tty_old_pgrp ;
   int leader ;
   struct tty_struct *tty ;
   struct autogroup *autogroup ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t cutime ;
   cputime_t cstime ;
   cputime_t gtime ;
   cputime_t cgtime ;
   struct cputime prev_cputime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   unsigned long cnvcsw ;
   unsigned long cnivcsw ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   unsigned long cmin_flt ;
   unsigned long cmaj_flt ;
   unsigned long inblock ;
   unsigned long oublock ;
   unsigned long cinblock ;
   unsigned long coublock ;
   unsigned long maxrss ;
   unsigned long cmaxrss ;
   struct task_io_accounting ioac ;
   unsigned long long sum_sched_runtime ;
   struct rlimit rlim[16U] ;
   struct pacct_struct pacct ;
   struct taskstats *stats ;
   unsigned int audit_tty ;
   struct tty_audit_buf *tty_audit_buf ;
   struct rw_semaphore group_rwsem ;
   oom_flags_t oom_flags ;
   short oom_score_adj ;
   short oom_score_adj_min ;
   struct mutex cred_guard_mutex ;
};
struct user_struct {
   atomic_t __count ;
   atomic_t processes ;
   atomic_t files ;
   atomic_t sigpending ;
   atomic_t inotify_watches ;
   atomic_t inotify_devs ;
   atomic_t fanotify_listeners ;
   atomic_long_t epoll_watches ;
   unsigned long mq_bytes ;
   unsigned long locked_shm ;
   struct key *uid_keyring ;
   struct key *session_keyring ;
   struct hlist_node uidhash_node ;
   kuid_t uid ;
   atomic_long_t locked_vm ;
};
struct reclaim_state;
struct sched_info {
   unsigned long pcount ;
   unsigned long long run_delay ;
   unsigned long long last_arrival ;
   unsigned long long last_queued ;
};
struct task_delay_info {
   spinlock_t lock ;
   unsigned int flags ;
   struct timespec blkio_start ;
   struct timespec blkio_end ;
   u64 blkio_delay ;
   u64 swapin_delay ;
   u32 blkio_count ;
   u32 swapin_count ;
   struct timespec freepages_start ;
   struct timespec freepages_end ;
   u64 freepages_delay ;
   u32 freepages_count ;
};
struct uts_namespace;
struct rq;
struct sched_class {
   struct sched_class const *next ;
   void (*enqueue_task)(struct rq * , struct task_struct * , int ) ;
   void (*dequeue_task)(struct rq * , struct task_struct * , int ) ;
   void (*yield_task)(struct rq * ) ;
   bool (*yield_to_task)(struct rq * , struct task_struct * , bool ) ;
   void (*check_preempt_curr)(struct rq * , struct task_struct * , int ) ;
   struct task_struct *(*pick_next_task)(struct rq * ) ;
   void (*put_prev_task)(struct rq * , struct task_struct * ) ;
   int (*select_task_rq)(struct task_struct * , int , int ) ;
   void (*migrate_task_rq)(struct task_struct * , int ) ;
   void (*pre_schedule)(struct rq * , struct task_struct * ) ;
   void (*post_schedule)(struct rq * ) ;
   void (*task_waking)(struct task_struct * ) ;
   void (*task_woken)(struct rq * , struct task_struct * ) ;
   void (*set_cpus_allowed)(struct task_struct * , struct cpumask const * ) ;
   void (*rq_online)(struct rq * ) ;
   void (*rq_offline)(struct rq * ) ;
   void (*set_curr_task)(struct rq * ) ;
   void (*task_tick)(struct rq * , struct task_struct * , int ) ;
   void (*task_fork)(struct task_struct * ) ;
   void (*switched_from)(struct rq * , struct task_struct * ) ;
   void (*switched_to)(struct rq * , struct task_struct * ) ;
   void (*prio_changed)(struct rq * , struct task_struct * , int ) ;
   unsigned int (*get_rr_interval)(struct rq * , struct task_struct * ) ;
   void (*task_move_group)(struct task_struct * , int ) ;
};
struct load_weight {
   unsigned long weight ;
   unsigned long inv_weight ;
};
struct sched_avg {
   u32 runnable_avg_sum ;
   u32 runnable_avg_period ;
   u64 last_runnable_update ;
   s64 decay_count ;
   unsigned long load_avg_contrib ;
};
struct sched_statistics {
   u64 wait_start ;
   u64 wait_max ;
   u64 wait_count ;
   u64 wait_sum ;
   u64 iowait_count ;
   u64 iowait_sum ;
   u64 sleep_start ;
   u64 sleep_max ;
   s64 sum_sleep_runtime ;
   u64 block_start ;
   u64 block_max ;
   u64 exec_max ;
   u64 slice_max ;
   u64 nr_migrations_cold ;
   u64 nr_failed_migrations_affine ;
   u64 nr_failed_migrations_running ;
   u64 nr_failed_migrations_hot ;
   u64 nr_forced_migrations ;
   u64 nr_wakeups ;
   u64 nr_wakeups_sync ;
   u64 nr_wakeups_migrate ;
   u64 nr_wakeups_local ;
   u64 nr_wakeups_remote ;
   u64 nr_wakeups_affine ;
   u64 nr_wakeups_affine_attempts ;
   u64 nr_wakeups_passive ;
   u64 nr_wakeups_idle ;
};
struct sched_entity {
   struct load_weight load ;
   struct rb_node run_node ;
   struct list_head group_node ;
   unsigned int on_rq ;
   u64 exec_start ;
   u64 sum_exec_runtime ;
   u64 vruntime ;
   u64 prev_sum_exec_runtime ;
   u64 nr_migrations ;
   struct sched_statistics statistics ;
   struct sched_entity *parent ;
   struct cfs_rq *cfs_rq ;
   struct cfs_rq *my_q ;
   struct sched_avg avg ;
};
struct rt_rq;
struct sched_rt_entity {
   struct list_head run_list ;
   unsigned long timeout ;
   unsigned int time_slice ;
   struct sched_rt_entity *back ;
   struct sched_rt_entity *parent ;
   struct rt_rq *rt_rq ;
   struct rt_rq *my_q ;
};
struct memcg_batch_info {
   int do_batch ;
   struct mem_cgroup *memcg ;
   unsigned long nr_pages ;
   unsigned long memsw_nr_pages ;
};
struct css_set;
struct compat_robust_list_head;
struct task_struct {
   long volatile state ;
   void *stack ;
   atomic_t usage ;
   unsigned int flags ;
   unsigned int ptrace ;
   struct llist_node wake_entry ;
   int on_cpu ;
   int on_rq ;
   int prio ;
   int static_prio ;
   int normal_prio ;
   unsigned int rt_priority ;
   struct sched_class const *sched_class ;
   struct sched_entity se ;
   struct sched_rt_entity rt ;
   struct task_group *sched_task_group ;
   struct hlist_head preempt_notifiers ;
   unsigned char fpu_counter ;
   unsigned int policy ;
   int nr_cpus_allowed ;
   cpumask_t cpus_allowed ;
   struct sched_info sched_info ;
   struct list_head tasks ;
   struct plist_node pushable_tasks ;
   struct mm_struct *mm ;
   struct mm_struct *active_mm ;
   unsigned char brk_randomized : 1 ;
   int exit_state ;
   int exit_code ;
   int exit_signal ;
   int pdeath_signal ;
   unsigned int jobctl ;
   unsigned int personality ;
   unsigned char did_exec : 1 ;
   unsigned char in_execve : 1 ;
   unsigned char in_iowait : 1 ;
   unsigned char no_new_privs : 1 ;
   unsigned char sched_reset_on_fork : 1 ;
   unsigned char sched_contributes_to_load : 1 ;
   pid_t pid ;
   pid_t tgid ;
   unsigned long stack_canary ;
   struct task_struct *real_parent ;
   struct task_struct *parent ;
   struct list_head children ;
   struct list_head sibling ;
   struct task_struct *group_leader ;
   struct list_head ptraced ;
   struct list_head ptrace_entry ;
   struct pid_link pids[3U] ;
   struct list_head thread_group ;
   struct completion *vfork_done ;
   int *set_child_tid ;
   int *clear_child_tid ;
   cputime_t utime ;
   cputime_t stime ;
   cputime_t utimescaled ;
   cputime_t stimescaled ;
   cputime_t gtime ;
   struct cputime prev_cputime ;
   unsigned long nvcsw ;
   unsigned long nivcsw ;
   struct timespec start_time ;
   struct timespec real_start_time ;
   unsigned long min_flt ;
   unsigned long maj_flt ;
   struct task_cputime cputime_expires ;
   struct list_head cpu_timers[3U] ;
   struct cred const *real_cred ;
   struct cred const *cred ;
   char comm[16U] ;
   int link_count ;
   int total_link_count ;
   struct sysv_sem sysvsem ;
   unsigned long last_switch_count ;
   struct thread_struct thread ;
   struct fs_struct *fs ;
   struct files_struct *files ;
   struct nsproxy *nsproxy ;
   struct signal_struct *signal ;
   struct sighand_struct *sighand ;
   sigset_t blocked ;
   sigset_t real_blocked ;
   sigset_t saved_sigmask ;
   struct sigpending pending ;
   unsigned long sas_ss_sp ;
   size_t sas_ss_size ;
   int (*notifier)(void * ) ;
   void *notifier_data ;
   sigset_t *notifier_mask ;
   struct callback_head *task_works ;
   struct audit_context *audit_context ;
   kuid_t loginuid ;
   unsigned int sessionid ;
   struct seccomp seccomp ;
   u32 parent_exec_id ;
   u32 self_exec_id ;
   spinlock_t alloc_lock ;
   raw_spinlock_t pi_lock ;
   struct plist_head pi_waiters ;
   struct rt_mutex_waiter *pi_blocked_on ;
   struct mutex_waiter *blocked_on ;
   unsigned int irq_events ;
   unsigned long hardirq_enable_ip ;
   unsigned long hardirq_disable_ip ;
   unsigned int hardirq_enable_event ;
   unsigned int hardirq_disable_event ;
   int hardirqs_enabled ;
   int hardirq_context ;
   unsigned long softirq_disable_ip ;
   unsigned long softirq_enable_ip ;
   unsigned int softirq_disable_event ;
   unsigned int softirq_enable_event ;
   int softirqs_enabled ;
   int softirq_context ;
   u64 curr_chain_key ;
   int lockdep_depth ;
   unsigned int lockdep_recursion ;
   struct held_lock held_locks[48U] ;
   gfp_t lockdep_reclaim_gfp ;
   void *journal_info ;
   struct bio_list *bio_list ;
   struct blk_plug *plug ;
   struct reclaim_state *reclaim_state ;
   struct backing_dev_info *backing_dev_info ;
   struct io_context *io_context ;
   unsigned long ptrace_message ;
   siginfo_t *last_siginfo ;
   struct task_io_accounting ioac ;
   u64 acct_rss_mem1 ;
   u64 acct_vm_mem1 ;
   cputime_t acct_timexpd ;
   nodemask_t mems_allowed ;
   seqcount_t mems_allowed_seq ;
   int cpuset_mem_spread_rotor ;
   int cpuset_slab_spread_rotor ;
   struct css_set *cgroups ;
   struct list_head cg_list ;
   struct robust_list_head *robust_list ;
   struct compat_robust_list_head *compat_robust_list ;
   struct list_head pi_state_list ;
   struct futex_pi_state *pi_state_cache ;
   struct perf_event_context *perf_event_ctxp[2U] ;
   struct mutex perf_event_mutex ;
   struct list_head perf_event_list ;
   struct mempolicy *mempolicy ;
   short il_next ;
   short pref_node_fork ;
   int numa_scan_seq ;
   int numa_migrate_seq ;
   unsigned int numa_scan_period ;
   u64 node_stamp ;
   struct callback_head numa_work ;
   struct callback_head rcu ;
   struct pipe_inode_info *splice_pipe ;
   struct page_frag task_frag ;
   struct task_delay_info *delays ;
   int make_it_fail ;
   int nr_dirtied ;
   int nr_dirtied_pause ;
   unsigned long dirty_paused_when ;
   int latency_record_count ;
   struct latency_record latency_record[32U] ;
   unsigned long timer_slack_ns ;
   unsigned long default_timer_slack_ns ;
   unsigned long trace ;
   unsigned long trace_recursion ;
   struct memcg_batch_info memcg_batch ;
   unsigned int memcg_kmem_skip_account ;
   atomic_t ptrace_bp_refcnt ;
   struct uprobe_task *utask ;
};
typedef s32 compat_long_t;
typedef u32 compat_uptr_t;
struct compat_robust_list {
   compat_uptr_t next ;
};
struct compat_robust_list_head {
   struct compat_robust_list list ;
   compat_long_t futex_offset ;
   compat_uptr_t list_op_pending ;
};
enum ldv_22357 {
    SS_FREE = 0,
    SS_UNCONNECTED = 1,
    SS_CONNECTING = 2,
    SS_CONNECTED = 3,
    SS_DISCONNECTING = 4
} ;
typedef enum ldv_22357 socket_state;
struct socket_wq {
   wait_queue_head_t wait ;
   struct fasync_struct *fasync_list ;
   struct callback_head rcu ;
};
struct proto_ops;
struct socket {
   socket_state state ;
   short type ;
   unsigned long flags ;
   struct socket_wq *wq ;
   struct file *file ;
   struct sock *sk ;
   struct proto_ops const *ops ;
};
struct proto_ops {
   int family ;
   struct module *owner ;
   int (*release)(struct socket * ) ;
   int (*bind)(struct socket * , struct sockaddr * , int ) ;
   int (*connect)(struct socket * , struct sockaddr * , int , int ) ;
   int (*socketpair)(struct socket * , struct socket * ) ;
   int (*accept)(struct socket * , struct socket * , int ) ;
   int (*getname)(struct socket * , struct sockaddr * , int * , int ) ;
   unsigned int (*poll)(struct file * , struct socket * , struct poll_table_struct * ) ;
   int (*ioctl)(struct socket * , unsigned int , unsigned long ) ;
   int (*compat_ioctl)(struct socket * , unsigned int , unsigned long ) ;
   int (*listen)(struct socket * , int ) ;
   int (*shutdown)(struct socket * , int ) ;
   int (*setsockopt)(struct socket * , int , int , char * , unsigned int ) ;
   int (*getsockopt)(struct socket * , int , int , char * , int * ) ;
   int (*compat_setsockopt)(struct socket * , int , int , char * , unsigned int ) ;
   int (*compat_getsockopt)(struct socket * , int , int , char * , int * ) ;
   int (*sendmsg)(struct kiocb * , struct socket * , struct msghdr * , size_t ) ;
   int (*recvmsg)(struct kiocb * , struct socket * , struct msghdr * , size_t , int ) ;
   int (*mmap)(struct file * , struct socket * , struct vm_area_struct * ) ;
   ssize_t (*sendpage)(struct socket * , struct page * , int , size_t , int ) ;
   ssize_t (*splice_read)(struct socket * , loff_t * , struct pipe_inode_info * ,
                          size_t , unsigned int ) ;
   void (*set_peek_off)(struct sock * , int ) ;
};
struct exception_table_entry {
   int insn ;
   int fixup ;
};
struct sk_buff;
typedef u64 netdev_features_t;
struct nf_conntrack {
   atomic_t use ;
};
struct nf_bridge_info {
   atomic_t use ;
   unsigned int mask ;
   struct net_device *physindev ;
   struct net_device *physoutdev ;
   unsigned long data[4U] ;
};
struct sk_buff_head {
   struct sk_buff *next ;
   struct sk_buff *prev ;
   __u32 qlen ;
   spinlock_t lock ;
};
typedef unsigned int sk_buff_data_t;
struct sec_path;
struct __anonstruct_ldv_30199_209 {
   __u16 csum_start ;
   __u16 csum_offset ;
};
union __anonunion_ldv_30200_208 {
   __wsum csum ;
   struct __anonstruct_ldv_30199_209 ldv_30199 ;
};
union __anonunion_ldv_30239_210 {
   __u32 mark ;
   __u32 dropcount ;
   __u32 avail_size ;
};
struct sk_buff {
   struct sk_buff *next ;
   struct sk_buff *prev ;
   ktime_t tstamp ;
   struct sock *sk ;
   struct net_device *dev ;
   char cb[48U] ;
   unsigned long _skb_refdst ;
   struct sec_path *sp ;
   unsigned int len ;
   unsigned int data_len ;
   __u16 mac_len ;
   __u16 hdr_len ;
   union __anonunion_ldv_30200_208 ldv_30200 ;
   __u32 priority ;
   unsigned char local_df : 1 ;
   unsigned char cloned : 1 ;
   unsigned char ip_summed : 2 ;
   unsigned char nohdr : 1 ;
   unsigned char nfctinfo : 3 ;
   unsigned char pkt_type : 3 ;
   unsigned char fclone : 2 ;
   unsigned char ipvs_property : 1 ;
   unsigned char peeked : 1 ;
   unsigned char nf_trace : 1 ;
   __be16 protocol ;
   void (*destructor)(struct sk_buff * ) ;
   struct nf_conntrack *nfct ;
   struct sk_buff *nfct_reasm ;
   struct nf_bridge_info *nf_bridge ;
   int skb_iif ;
   __u32 rxhash ;
   __u16 vlan_tci ;
   __u16 tc_index ;
   __u16 tc_verd ;
   __u16 queue_mapping ;
   unsigned char ndisc_nodetype : 2 ;
   unsigned char pfmemalloc : 1 ;
   unsigned char ooo_okay : 1 ;
   unsigned char l4_rxhash : 1 ;
   unsigned char wifi_acked_valid : 1 ;
   unsigned char wifi_acked : 1 ;
   unsigned char no_fcs : 1 ;
   unsigned char head_frag : 1 ;
   unsigned char encapsulation : 1 ;
   dma_cookie_t dma_cookie ;
   __u32 secmark ;
   union __anonunion_ldv_30239_210 ldv_30239 ;
   sk_buff_data_t inner_transport_header ;
   sk_buff_data_t inner_network_header ;
   sk_buff_data_t transport_header ;
   sk_buff_data_t network_header ;
   sk_buff_data_t mac_header ;
   sk_buff_data_t tail ;
   sk_buff_data_t end ;
   unsigned char *head ;
   unsigned char *data ;
   unsigned int truesize ;
   atomic_t users ;
};
struct dst_entry;
struct rtable;
struct ethhdr {
   unsigned char h_dest[6U] ;
   unsigned char h_source[6U] ;
   __be16 h_proto ;
};
struct ethtool_cmd {
   __u32 cmd ;
   __u32 supported ;
   __u32 advertising ;
   __u16 speed ;
   __u8 duplex ;
   __u8 port ;
   __u8 phy_address ;
   __u8 transceiver ;
   __u8 autoneg ;
   __u8 mdio_support ;
   __u32 maxtxpkt ;
   __u32 maxrxpkt ;
   __u16 speed_hi ;
   __u8 eth_tp_mdix ;
   __u8 eth_tp_mdix_ctrl ;
   __u32 lp_advertising ;
   __u32 reserved[2U] ;
};
struct ethtool_drvinfo {
   __u32 cmd ;
   char driver[32U] ;
   char version[32U] ;
   char fw_version[32U] ;
   char bus_info[32U] ;
   char reserved1[32U] ;
   char reserved2[12U] ;
   __u32 n_priv_flags ;
   __u32 n_stats ;
   __u32 testinfo_len ;
   __u32 eedump_len ;
   __u32 regdump_len ;
};
struct ethtool_wolinfo {
   __u32 cmd ;
   __u32 supported ;
   __u32 wolopts ;
   __u8 sopass[6U] ;
};
struct ethtool_regs {
   __u32 cmd ;
   __u32 version ;
   __u32 len ;
   __u8 data[0U] ;
};
struct ethtool_eeprom {
   __u32 cmd ;
   __u32 magic ;
   __u32 offset ;
   __u32 len ;
   __u8 data[0U] ;
};
struct ethtool_eee {
   __u32 cmd ;
   __u32 supported ;
   __u32 advertised ;
   __u32 lp_advertised ;
   __u32 eee_active ;
   __u32 eee_enabled ;
   __u32 tx_lpi_enabled ;
   __u32 tx_lpi_timer ;
   __u32 reserved[2U] ;
};
struct ethtool_modinfo {
   __u32 cmd ;
   __u32 type ;
   __u32 eeprom_len ;
   __u32 reserved[8U] ;
};
struct ethtool_coalesce {
   __u32 cmd ;
   __u32 rx_coalesce_usecs ;
   __u32 rx_max_coalesced_frames ;
   __u32 rx_coalesce_usecs_irq ;
   __u32 rx_max_coalesced_frames_irq ;
   __u32 tx_coalesce_usecs ;
   __u32 tx_max_coalesced_frames ;
   __u32 tx_coalesce_usecs_irq ;
   __u32 tx_max_coalesced_frames_irq ;
   __u32 stats_block_coalesce_usecs ;
   __u32 use_adaptive_rx_coalesce ;
   __u32 use_adaptive_tx_coalesce ;
   __u32 pkt_rate_low ;
   __u32 rx_coalesce_usecs_low ;
   __u32 rx_max_coalesced_frames_low ;
   __u32 tx_coalesce_usecs_low ;
   __u32 tx_max_coalesced_frames_low ;
   __u32 pkt_rate_high ;
   __u32 rx_coalesce_usecs_high ;
   __u32 rx_max_coalesced_frames_high ;
   __u32 tx_coalesce_usecs_high ;
   __u32 tx_max_coalesced_frames_high ;
   __u32 rate_sample_interval ;
};
struct ethtool_ringparam {
   __u32 cmd ;
   __u32 rx_max_pending ;
   __u32 rx_mini_max_pending ;
   __u32 rx_jumbo_max_pending ;
   __u32 tx_max_pending ;
   __u32 rx_pending ;
   __u32 rx_mini_pending ;
   __u32 rx_jumbo_pending ;
   __u32 tx_pending ;
};
struct ethtool_channels {
   __u32 cmd ;
   __u32 max_rx ;
   __u32 max_tx ;
   __u32 max_other ;
   __u32 max_combined ;
   __u32 rx_count ;
   __u32 tx_count ;
   __u32 other_count ;
   __u32 combined_count ;
};
struct ethtool_pauseparam {
   __u32 cmd ;
   __u32 autoneg ;
   __u32 rx_pause ;
   __u32 tx_pause ;
};
struct ethtool_test {
   __u32 cmd ;
   __u32 flags ;
   __u32 reserved ;
   __u32 len ;
   __u64 data[0U] ;
};
struct ethtool_stats {
   __u32 cmd ;
   __u32 n_stats ;
   __u64 data[0U] ;
};
struct ethtool_tcpip4_spec {
   __be32 ip4src ;
   __be32 ip4dst ;
   __be16 psrc ;
   __be16 pdst ;
   __u8 tos ;
};
struct ethtool_ah_espip4_spec {
   __be32 ip4src ;
   __be32 ip4dst ;
   __be32 spi ;
   __u8 tos ;
};
struct ethtool_usrip4_spec {
   __be32 ip4src ;
   __be32 ip4dst ;
   __be32 l4_4_bytes ;
   __u8 tos ;
   __u8 ip_ver ;
   __u8 proto ;
};
union ethtool_flow_union {
   struct ethtool_tcpip4_spec tcp_ip4_spec ;
   struct ethtool_tcpip4_spec udp_ip4_spec ;
   struct ethtool_tcpip4_spec sctp_ip4_spec ;
   struct ethtool_ah_espip4_spec ah_ip4_spec ;
   struct ethtool_ah_espip4_spec esp_ip4_spec ;
   struct ethtool_usrip4_spec usr_ip4_spec ;
   struct ethhdr ether_spec ;
   __u8 hdata[52U] ;
};
struct ethtool_flow_ext {
   __u8 padding[2U] ;
   unsigned char h_dest[6U] ;
   __be16 vlan_etype ;
   __be16 vlan_tci ;
   __be32 data[2U] ;
};
struct ethtool_rx_flow_spec {
   __u32 flow_type ;
   union ethtool_flow_union h_u ;
   struct ethtool_flow_ext h_ext ;
   union ethtool_flow_union m_u ;
   struct ethtool_flow_ext m_ext ;
   __u64 ring_cookie ;
   __u32 location ;
};
struct ethtool_rxnfc {
   __u32 cmd ;
   __u32 flow_type ;
   __u64 data ;
   struct ethtool_rx_flow_spec fs ;
   __u32 rule_cnt ;
   __u32 rule_locs[0U] ;
};
struct ethtool_flash {
   __u32 cmd ;
   __u32 region ;
   char data[128U] ;
};
struct ethtool_dump {
   __u32 cmd ;
   __u32 version ;
   __u32 flag ;
   __u32 len ;
   __u8 data[0U] ;
};
struct ethtool_ts_info {
   __u32 cmd ;
   __u32 so_timestamping ;
   __s32 phc_index ;
   __u32 tx_types ;
   __u32 tx_reserved[3U] ;
   __u32 rx_filters ;
   __u32 rx_reserved[3U] ;
};
enum ethtool_phys_id_state {
    ETHTOOL_ID_INACTIVE = 0,
    ETHTOOL_ID_ACTIVE = 1,
    ETHTOOL_ID_ON = 2,
    ETHTOOL_ID_OFF = 3
} ;
struct ethtool_ops {
   int (*get_settings)(struct net_device * , struct ethtool_cmd * ) ;
   int (*set_settings)(struct net_device * , struct ethtool_cmd * ) ;
   void (*get_drvinfo)(struct net_device * , struct ethtool_drvinfo * ) ;
   int (*get_regs_len)(struct net_device * ) ;
   void (*get_regs)(struct net_device * , struct ethtool_regs * , void * ) ;
   void (*get_wol)(struct net_device * , struct ethtool_wolinfo * ) ;
   int (*set_wol)(struct net_device * , struct ethtool_wolinfo * ) ;
   u32 (*get_msglevel)(struct net_device * ) ;
   void (*set_msglevel)(struct net_device * , u32 ) ;
   int (*nway_reset)(struct net_device * ) ;
   u32 (*get_link)(struct net_device * ) ;
   int (*get_eeprom_len)(struct net_device * ) ;
   int (*get_eeprom)(struct net_device * , struct ethtool_eeprom * , u8 * ) ;
   int (*set_eeprom)(struct net_device * , struct ethtool_eeprom * , u8 * ) ;
   int (*get_coalesce)(struct net_device * , struct ethtool_coalesce * ) ;
   int (*set_coalesce)(struct net_device * , struct ethtool_coalesce * ) ;
   void (*get_ringparam)(struct net_device * , struct ethtool_ringparam * ) ;
   int (*set_ringparam)(struct net_device * , struct ethtool_ringparam * ) ;
   void (*get_pauseparam)(struct net_device * , struct ethtool_pauseparam * ) ;
   int (*set_pauseparam)(struct net_device * , struct ethtool_pauseparam * ) ;
   void (*self_test)(struct net_device * , struct ethtool_test * , u64 * ) ;
   void (*get_strings)(struct net_device * , u32 , u8 * ) ;
   int (*set_phys_id)(struct net_device * , enum ethtool_phys_id_state ) ;
   void (*get_ethtool_stats)(struct net_device * , struct ethtool_stats * , u64 * ) ;
   int (*begin)(struct net_device * ) ;
   void (*complete)(struct net_device * ) ;
   u32 (*get_priv_flags)(struct net_device * ) ;
   int (*set_priv_flags)(struct net_device * , u32 ) ;
   int (*get_sset_count)(struct net_device * , int ) ;
   int (*get_rxnfc)(struct net_device * , struct ethtool_rxnfc * , u32 * ) ;
   int (*set_rxnfc)(struct net_device * , struct ethtool_rxnfc * ) ;
   int (*flash_device)(struct net_device * , struct ethtool_flash * ) ;
   int (*reset)(struct net_device * , u32 * ) ;
   u32 (*get_rxfh_indir_size)(struct net_device * ) ;
   int (*get_rxfh_indir)(struct net_device * , u32 * ) ;
   int (*set_rxfh_indir)(struct net_device * , u32 const * ) ;
   void (*get_channels)(struct net_device * , struct ethtool_channels * ) ;
   int (*set_channels)(struct net_device * , struct ethtool_channels * ) ;
   int (*get_dump_flag)(struct net_device * , struct ethtool_dump * ) ;
   int (*get_dump_data)(struct net_device * , struct ethtool_dump * , void * ) ;
   int (*set_dump)(struct net_device * , struct ethtool_dump * ) ;
   int (*get_ts_info)(struct net_device * , struct ethtool_ts_info * ) ;
   int (*get_module_info)(struct net_device * , struct ethtool_modinfo * ) ;
   int (*get_module_eeprom)(struct net_device * , struct ethtool_eeprom * , u8 * ) ;
   int (*get_eee)(struct net_device * , struct ethtool_eee * ) ;
   int (*set_eee)(struct net_device * , struct ethtool_eee * ) ;
};
struct prot_inuse;
struct netns_core {
   struct ctl_table_header *sysctl_hdr ;
   int sysctl_somaxconn ;
   struct prot_inuse *inuse ;
};
struct u64_stats_sync {
};
struct ipstats_mib {
   u64 mibs[31U] ;
   struct u64_stats_sync syncp ;
};
struct icmp_mib {
   unsigned long mibs[27U] ;
};
struct icmpmsg_mib {
   atomic_long_t mibs[512U] ;
};
struct icmpv6_mib {
   unsigned long mibs[5U] ;
};
struct icmpv6msg_mib {
   atomic_long_t mibs[512U] ;
};
struct tcp_mib {
   unsigned long mibs[15U] ;
};
struct udp_mib {
   unsigned long mibs[7U] ;
};
struct linux_mib {
   unsigned long mibs[92U] ;
};
struct linux_xfrm_mib {
   unsigned long mibs[27U] ;
};
struct netns_mib {
   struct tcp_mib *tcp_statistics[1U] ;
   struct ipstats_mib *ip_statistics[1U] ;
   struct linux_mib *net_statistics[1U] ;
   struct udp_mib *udp_statistics[1U] ;
   struct udp_mib *udplite_statistics[1U] ;
   struct icmp_mib *icmp_statistics[1U] ;
   struct icmpmsg_mib *icmpmsg_statistics ;
   struct proc_dir_entry *proc_net_devsnmp6 ;
   struct udp_mib *udp_stats_in6[1U] ;
   struct udp_mib *udplite_stats_in6[1U] ;
   struct ipstats_mib *ipv6_statistics[1U] ;
   struct icmpv6_mib *icmpv6_statistics[1U] ;
   struct icmpv6msg_mib *icmpv6msg_statistics ;
   struct linux_xfrm_mib *xfrm_statistics[1U] ;
};
struct netns_unix {
   int sysctl_max_dgram_qlen ;
   struct ctl_table_header *ctl ;
};
struct netns_packet {
   struct mutex sklist_lock ;
   struct hlist_head sklist ;
};
struct netns_frags {
   int nqueues ;
   atomic_t mem ;
   struct list_head lru_list ;
   int timeout ;
   int high_thresh ;
   int low_thresh ;
};
struct tcpm_hash_bucket;
struct ipv4_devconf;
struct fib_rules_ops;
struct fib_table;
struct inet_peer_base;
struct xt_table;
struct netns_ipv4 {
   struct ctl_table_header *forw_hdr ;
   struct ctl_table_header *frags_hdr ;
   struct ctl_table_header *ipv4_hdr ;
   struct ctl_table_header *route_hdr ;
   struct ipv4_devconf *devconf_all ;
   struct ipv4_devconf *devconf_dflt ;
   struct fib_rules_ops *rules_ops ;
   bool fib_has_custom_rules ;
   struct fib_table *fib_local ;
   struct fib_table *fib_main ;
   struct fib_table *fib_default ;
   int fib_num_tclassid_users ;
   struct hlist_head *fib_table_hash ;
   struct sock *fibnl ;
   struct sock **icmp_sk ;
   struct inet_peer_base *peers ;
   struct tcpm_hash_bucket *tcp_metrics_hash ;
   unsigned int tcp_metrics_hash_log ;
   struct netns_frags frags ;
   struct xt_table *iptable_filter ;
   struct xt_table *iptable_mangle ;
   struct xt_table *iptable_raw ;
   struct xt_table *arptable_filter ;
   struct xt_table *iptable_security ;
   struct xt_table *nat_table ;
   int sysctl_icmp_echo_ignore_all ;
   int sysctl_icmp_echo_ignore_broadcasts ;
   int sysctl_icmp_ignore_bogus_error_responses ;
   int sysctl_icmp_ratelimit ;
   int sysctl_icmp_ratemask ;
   int sysctl_icmp_errors_use_inbound_ifaddr ;
   kgid_t sysctl_ping_group_range[2U] ;
   long sysctl_tcp_mem[3U] ;
   atomic_t dev_addr_genid ;
   struct list_head mr_tables ;
   struct fib_rules_ops *mr_rules_ops ;
};
struct neighbour;
struct dst_ops {
   unsigned short family ;
   __be16 protocol ;
   unsigned int gc_thresh ;
   int (*gc)(struct dst_ops * ) ;
   struct dst_entry *(*check)(struct dst_entry * , __u32 ) ;
   unsigned int (*default_advmss)(struct dst_entry const * ) ;
   unsigned int (*mtu)(struct dst_entry const * ) ;
   u32 *(*cow_metrics)(struct dst_entry * , unsigned long ) ;
   void (*destroy)(struct dst_entry * ) ;
   void (*ifdown)(struct dst_entry * , struct net_device * , int ) ;
   struct dst_entry *(*negative_advice)(struct dst_entry * ) ;
   void (*link_failure)(struct sk_buff * ) ;
   void (*update_pmtu)(struct dst_entry * , struct sock * , struct sk_buff * , u32 ) ;
   void (*redirect)(struct dst_entry * , struct sock * , struct sk_buff * ) ;
   int (*local_out)(struct sk_buff * ) ;
   struct neighbour *(*neigh_lookup)(struct dst_entry const * , struct sk_buff * ,
                                     void const * ) ;
   struct kmem_cache *kmem_cachep ;
   struct percpu_counter pcpuc_entries ;
};
struct netns_sysctl_ipv6 {
   struct ctl_table_header *hdr ;
   struct ctl_table_header *route_hdr ;
   struct ctl_table_header *icmp_hdr ;
   struct ctl_table_header *frags_hdr ;
   int bindv6only ;
   int flush_delay ;
   int ip6_rt_max_size ;
   int ip6_rt_gc_min_interval ;
   int ip6_rt_gc_timeout ;
   int ip6_rt_gc_interval ;
   int ip6_rt_gc_elasticity ;
   int ip6_rt_mtu_expires ;
   int ip6_rt_min_advmss ;
   int icmpv6_time ;
};
struct ipv6_devconf;
struct rt6_info;
struct rt6_statistics;
struct fib6_table;
struct netns_ipv6 {
   struct netns_sysctl_ipv6 sysctl ;
   struct ipv6_devconf *devconf_all ;
   struct ipv6_devconf *devconf_dflt ;
   struct inet_peer_base *peers ;
   struct netns_frags frags ;
   struct xt_table *ip6table_filter ;
   struct xt_table *ip6table_mangle ;
   struct xt_table *ip6table_raw ;
   struct xt_table *ip6table_security ;
   struct xt_table *ip6table_nat ;
   struct rt6_info *ip6_null_entry ;
   struct rt6_statistics *rt6_stats ;
   struct timer_list ip6_fib_timer ;
   struct hlist_head *fib_table_hash ;
   struct fib6_table *fib6_main_tbl ;
   struct dst_ops ip6_dst_ops ;
   unsigned int ip6_rt_gc_expire ;
   unsigned long ip6_rt_last_gc ;
   struct rt6_info *ip6_prohibit_entry ;
   struct rt6_info *ip6_blk_hole_entry ;
   struct fib6_table *fib6_local_tbl ;
   struct fib_rules_ops *fib6_rules_ops ;
   struct sock **icmp_sk ;
   struct sock *ndisc_sk ;
   struct sock *tcp_sk ;
   struct sock *igmp_sk ;
   struct list_head mr6_tables ;
   struct fib_rules_ops *mr6_rules_ops ;
};
struct netns_nf_frag {
   struct netns_sysctl_ipv6 sysctl ;
   struct netns_frags frags ;
};
struct sctp_mib;
struct netns_sctp {
   struct sctp_mib *sctp_statistics[1U] ;
   struct proc_dir_entry *proc_net_sctp ;
   struct ctl_table_header *sysctl_header ;
   struct sock *ctl_sock ;
   struct list_head local_addr_list ;
   struct list_head addr_waitq ;
   struct timer_list addr_wq_timer ;
   struct list_head auto_asconf_splist ;
   spinlock_t addr_wq_lock ;
   spinlock_t local_addr_lock ;
   unsigned int rto_initial ;
   unsigned int rto_min ;
   unsigned int rto_max ;
   int rto_alpha ;
   int rto_beta ;
   int max_burst ;
   int cookie_preserve_enable ;
   char *sctp_hmac_alg ;
   unsigned int valid_cookie_life ;
   unsigned int sack_timeout ;
   unsigned int hb_interval ;
   int max_retrans_association ;
   int max_retrans_path ;
   int max_retrans_init ;
   int pf_retrans ;
   int sndbuf_policy ;
   int rcvbuf_policy ;
   int default_auto_asconf ;
   int addip_enable ;
   int addip_noauth ;
   int prsctp_enable ;
   int auth_enable ;
   int scope_policy ;
   int rwnd_upd_shift ;
   unsigned long max_autoclose ;
};
struct netns_dccp {
   struct sock *v4_ctl_sk ;
   struct sock *v6_ctl_sk ;
};
typedef int read_proc_t(char * , char ** , off_t , int , int * , void * );
typedef int write_proc_t(struct file * , char const * , unsigned long , void * );
struct proc_dir_entry {
   unsigned int low_ino ;
   umode_t mode ;
   nlink_t nlink ;
   kuid_t uid ;
   kgid_t gid ;
   loff_t size ;
   struct inode_operations const *proc_iops ;
   struct file_operations const *proc_fops ;
   struct proc_dir_entry *next ;
   struct proc_dir_entry *parent ;
   struct proc_dir_entry *subdir ;
   void *data ;
   read_proc_t *read_proc ;
   write_proc_t *write_proc ;
   atomic_t count ;
   int pde_users ;
   struct completion *pde_unload_completion ;
   struct list_head pde_openers ;
   spinlock_t pde_unload_lock ;
   u8 namelen ;
   char name[] ;
};
struct nlattr;
struct ebt_table;
struct netns_xt {
   struct list_head tables[13U] ;
   struct ebt_table *broute_table ;
   struct ebt_table *frame_filter ;
   struct ebt_table *frame_nat ;
};
struct hlist_nulls_node;
struct hlist_nulls_head {
   struct hlist_nulls_node *first ;
};
struct hlist_nulls_node {
   struct hlist_nulls_node *next ;
   struct hlist_nulls_node **pprev ;
};
struct nf_proto_net {
   struct ctl_table_header *ctl_table_header ;
   struct ctl_table *ctl_table ;
   struct ctl_table_header *ctl_compat_header ;
   struct ctl_table *ctl_compat_table ;
   unsigned int users ;
};
struct nf_generic_net {
   struct nf_proto_net pn ;
   unsigned int timeout ;
};
struct nf_tcp_net {
   struct nf_proto_net pn ;
   unsigned int timeouts[14U] ;
   unsigned int tcp_loose ;
   unsigned int tcp_be_liberal ;
   unsigned int tcp_max_retrans ;
};
struct nf_udp_net {
   struct nf_proto_net pn ;
   unsigned int timeouts[2U] ;
};
struct nf_icmp_net {
   struct nf_proto_net pn ;
   unsigned int timeout ;
};
struct nf_ip_net {
   struct nf_generic_net generic ;
   struct nf_tcp_net tcp ;
   struct nf_udp_net udp ;
   struct nf_icmp_net icmp ;
   struct nf_icmp_net icmpv6 ;
   struct ctl_table_header *ctl_table_header ;
   struct ctl_table *ctl_table ;
};
struct ip_conntrack_stat;
struct nf_ct_event_notifier;
struct nf_exp_event_notifier;
struct netns_ct {
   atomic_t count ;
   unsigned int expect_count ;
   unsigned int htable_size ;
   struct kmem_cache *nf_conntrack_cachep ;
   struct hlist_nulls_head *hash ;
   struct hlist_head *expect_hash ;
   struct hlist_nulls_head unconfirmed ;
   struct hlist_nulls_head dying ;
   struct ip_conntrack_stat *stat ;
   struct nf_ct_event_notifier *nf_conntrack_event_cb ;
   struct nf_exp_event_notifier *nf_expect_event_cb ;
   int sysctl_events ;
   unsigned int sysctl_events_retry_timeout ;
   int sysctl_acct ;
   int sysctl_tstamp ;
   int sysctl_checksum ;
   unsigned int sysctl_log_invalid ;
   int sysctl_auto_assign_helper ;
   bool auto_assign_helper_warned ;
   struct nf_ip_net nf_ct_proto ;
   struct hlist_head *nat_bysource ;
   unsigned int nat_htable_size ;
   struct ctl_table_header *sysctl_header ;
   struct ctl_table_header *acct_sysctl_header ;
   struct ctl_table_header *tstamp_sysctl_header ;
   struct ctl_table_header *event_sysctl_header ;
   struct ctl_table_header *helper_sysctl_header ;
   char *slabname ;
};
struct xfrm_policy_hash {
   struct hlist_head *table ;
   unsigned int hmask ;
};
struct netns_xfrm {
   struct list_head state_all ;
   struct hlist_head *state_bydst ;
   struct hlist_head *state_bysrc ;
   struct hlist_head *state_byspi ;
   unsigned int state_hmask ;
   unsigned int state_num ;
   struct work_struct state_hash_work ;
   struct hlist_head state_gc_list ;
   struct work_struct state_gc_work ;
   wait_queue_head_t km_waitq ;
   struct list_head policy_all ;
   struct hlist_head *policy_byidx ;
   unsigned int policy_idx_hmask ;
   struct hlist_head policy_inexact[6U] ;
   struct xfrm_policy_hash policy_bydst[6U] ;
   unsigned int policy_count[6U] ;
   struct work_struct policy_hash_work ;
   struct sock *nlsk ;
   struct sock *nlsk_stash ;
   u32 sysctl_aevent_etime ;
   u32 sysctl_aevent_rseqth ;
   int sysctl_larval_drop ;
   u32 sysctl_acq_expires ;
   struct ctl_table_header *sysctl_hdr ;
   struct dst_ops xfrm4_dst_ops ;
   struct dst_ops xfrm6_dst_ops ;
};
struct net_generic;
struct netns_ipvs;
struct net {
   atomic_t passive ;
   atomic_t count ;
   spinlock_t rules_mod_lock ;
   struct list_head list ;
   struct list_head cleanup_list ;
   struct list_head exit_list ;
   struct user_namespace *user_ns ;
   unsigned int proc_inum ;
   struct proc_dir_entry *proc_net ;
   struct proc_dir_entry *proc_net_stat ;
   struct ctl_table_set sysctls ;
   struct sock *rtnl ;
   struct sock *genl_sock ;
   struct list_head dev_base_head ;
   struct hlist_head *dev_name_head ;
   struct hlist_head *dev_index_head ;
   unsigned int dev_base_seq ;
   int ifindex ;
   struct list_head rules_ops ;
   struct net_device *loopback_dev ;
   struct netns_core core ;
   struct netns_mib mib ;
   struct netns_packet packet ;
   struct netns_unix unx ;
   struct netns_ipv4 ipv4 ;
   struct netns_ipv6 ipv6 ;
   struct netns_sctp sctp ;
   struct netns_dccp dccp ;
   struct netns_xt xt ;
   struct netns_ct ct ;
   struct netns_nf_frag nf_frag ;
   struct sock *nfnl ;
   struct sock *nfnl_stash ;
   struct sk_buff_head wext_nlevents ;
   struct net_generic *gen ;
   struct netns_xfrm xfrm ;
   struct netns_ipvs *ipvs ;
   struct sock *diag_nlsk ;
   atomic_t rt_genid ;
};
struct seq_file {
   char *buf ;
   size_t size ;
   size_t from ;
   size_t count ;
   loff_t index ;
   loff_t read_pos ;
   u64 version ;
   struct mutex lock ;
   struct seq_operations const *op ;
   int poll_event ;
   void *private ;
};
struct seq_operations {
   void *(*start)(struct seq_file * , loff_t * ) ;
   void (*stop)(struct seq_file * , void * ) ;
   void *(*next)(struct seq_file * , void * , loff_t * ) ;
   int (*show)(struct seq_file * , void * ) ;
};
struct dsa_chip_data {
   struct device *mii_bus ;
   int sw_addr ;
   char *port_names[12U] ;
   s8 *rtable ;
};
struct dsa_platform_data {
   struct device *netdev ;
   int nr_chips ;
   struct dsa_chip_data *chip ;
};
struct dsa_switch;
struct dsa_switch_tree {
   struct dsa_platform_data *pd ;
   struct net_device *master_netdev ;
   __be16 tag_protocol ;
   s8 cpu_switch ;
   s8 cpu_port ;
   int link_poll_needed ;
   struct work_struct link_poll_work ;
   struct timer_list link_poll_timer ;
   struct dsa_switch *ds[4U] ;
};
struct dsa_switch_driver;
struct mii_bus;
struct dsa_switch {
   struct dsa_switch_tree *dst ;
   int index ;
   struct dsa_chip_data *pd ;
   struct dsa_switch_driver *drv ;
   struct mii_bus *master_mii_bus ;
   u32 dsa_port_mask ;
   u32 phys_port_mask ;
   struct mii_bus *slave_mii_bus ;
   struct net_device *ports[12U] ;
};
struct dsa_switch_driver {
   struct list_head list ;
   __be16 tag_protocol ;
   int priv_size ;
   char *(*probe)(struct mii_bus * , int ) ;
   int (*setup)(struct dsa_switch * ) ;
   int (*set_addr)(struct dsa_switch * , u8 * ) ;
   int (*phy_read)(struct dsa_switch * , int , int ) ;
   int (*phy_write)(struct dsa_switch * , int , int , u16 ) ;
   void (*poll_link)(struct dsa_switch * ) ;
   void (*get_strings)(struct dsa_switch * , int , uint8_t * ) ;
   void (*get_ethtool_stats)(struct dsa_switch * , int , uint64_t * ) ;
   int (*get_sset_count)(struct dsa_switch * ) ;
};
struct ieee_ets {
   __u8 willing ;
   __u8 ets_cap ;
   __u8 cbs ;
   __u8 tc_tx_bw[8U] ;
   __u8 tc_rx_bw[8U] ;
   __u8 tc_tsa[8U] ;
   __u8 prio_tc[8U] ;
   __u8 tc_reco_bw[8U] ;
   __u8 tc_reco_tsa[8U] ;
   __u8 reco_prio_tc[8U] ;
};
struct ieee_maxrate {
   __u64 tc_maxrate[8U] ;
};
struct ieee_pfc {
   __u8 pfc_cap ;
   __u8 pfc_en ;
   __u8 mbc ;
   __u16 delay ;
   __u64 requests[8U] ;
   __u64 indications[8U] ;
};
struct cee_pg {
   __u8 willing ;
   __u8 error ;
   __u8 pg_en ;
   __u8 tcs_supported ;
   __u8 pg_bw[8U] ;
   __u8 prio_pg[8U] ;
};
struct cee_pfc {
   __u8 willing ;
   __u8 error ;
   __u8 pfc_en ;
   __u8 tcs_supported ;
};
struct dcb_app {
   __u8 selector ;
   __u8 priority ;
   __u16 protocol ;
};
struct dcb_peer_app_info {
   __u8 willing ;
   __u8 error ;
};
struct dcbnl_rtnl_ops {
   int (*ieee_getets)(struct net_device * , struct ieee_ets * ) ;
   int (*ieee_setets)(struct net_device * , struct ieee_ets * ) ;
   int (*ieee_getmaxrate)(struct net_device * , struct ieee_maxrate * ) ;
   int (*ieee_setmaxrate)(struct net_device * , struct ieee_maxrate * ) ;
   int (*ieee_getpfc)(struct net_device * , struct ieee_pfc * ) ;
   int (*ieee_setpfc)(struct net_device * , struct ieee_pfc * ) ;
   int (*ieee_getapp)(struct net_device * , struct dcb_app * ) ;
   int (*ieee_setapp)(struct net_device * , struct dcb_app * ) ;
   int (*ieee_delapp)(struct net_device * , struct dcb_app * ) ;
   int (*ieee_peer_getets)(struct net_device * , struct ieee_ets * ) ;
   int (*ieee_peer_getpfc)(struct net_device * , struct ieee_pfc * ) ;
   u8 (*getstate)(struct net_device * ) ;
   u8 (*setstate)(struct net_device * , u8 ) ;
   void (*getpermhwaddr)(struct net_device * , u8 * ) ;
   void (*setpgtccfgtx)(struct net_device * , int , u8 , u8 , u8 , u8 ) ;
   void (*setpgbwgcfgtx)(struct net_device * , int , u8 ) ;
   void (*setpgtccfgrx)(struct net_device * , int , u8 , u8 , u8 , u8 ) ;
   void (*setpgbwgcfgrx)(struct net_device * , int , u8 ) ;
   void (*getpgtccfgtx)(struct net_device * , int , u8 * , u8 * , u8 * , u8 * ) ;
   void (*getpgbwgcfgtx)(struct net_device * , int , u8 * ) ;
   void (*getpgtccfgrx)(struct net_device * , int , u8 * , u8 * , u8 * , u8 * ) ;
   void (*getpgbwgcfgrx)(struct net_device * , int , u8 * ) ;
   void (*setpfccfg)(struct net_device * , int , u8 ) ;
   void (*getpfccfg)(struct net_device * , int , u8 * ) ;
   u8 (*setall)(struct net_device * ) ;
   u8 (*getcap)(struct net_device * , int , u8 * ) ;
   int (*getnumtcs)(struct net_device * , int , u8 * ) ;
   int (*setnumtcs)(struct net_device * , int , u8 ) ;
   u8 (*getpfcstate)(struct net_device * ) ;
   void (*setpfcstate)(struct net_device * , u8 ) ;
   void (*getbcncfg)(struct net_device * , int , u32 * ) ;
   void (*setbcncfg)(struct net_device * , int , u32 ) ;
   void (*getbcnrp)(struct net_device * , int , u8 * ) ;
   void (*setbcnrp)(struct net_device * , int , u8 ) ;
   u8 (*setapp)(struct net_device * , u8 , u16 , u8 ) ;
   u8 (*getapp)(struct net_device * , u8 , u16 ) ;
   u8 (*getfeatcfg)(struct net_device * , int , u8 * ) ;
   u8 (*setfeatcfg)(struct net_device * , int , u8 ) ;
   u8 (*getdcbx)(struct net_device * ) ;
   u8 (*setdcbx)(struct net_device * , u8 ) ;
   int (*peer_getappinfo)(struct net_device * , struct dcb_peer_app_info * , u16 * ) ;
   int (*peer_getapptable)(struct net_device * , struct dcb_app * ) ;
   int (*cee_peer_getpg)(struct net_device * , struct cee_pg * ) ;
   int (*cee_peer_getpfc)(struct net_device * , struct cee_pfc * ) ;
};
struct taskstats {
   __u16 version ;
   __u32 ac_exitcode ;
   __u8 ac_flag ;
   __u8 ac_nice ;
   __u64 cpu_count ;
   __u64 cpu_delay_total ;
   __u64 blkio_count ;
   __u64 blkio_delay_total ;
   __u64 swapin_count ;
   __u64 swapin_delay_total ;
   __u64 cpu_run_real_total ;
   __u64 cpu_run_virtual_total ;
   char ac_comm[32U] ;
   __u8 ac_sched ;
   __u8 ac_pad[3U] ;
   __u32 ac_uid ;
   __u32 ac_gid ;
   __u32 ac_pid ;
   __u32 ac_ppid ;
   __u32 ac_btime ;
   __u64 ac_etime ;
   __u64 ac_utime ;
   __u64 ac_stime ;
   __u64 ac_minflt ;
   __u64 ac_majflt ;
   __u64 coremem ;
   __u64 virtmem ;
   __u64 hiwater_rss ;
   __u64 hiwater_vm ;
   __u64 read_char ;
   __u64 write_char ;
   __u64 read_syscalls ;
   __u64 write_syscalls ;
   __u64 read_bytes ;
   __u64 write_bytes ;
   __u64 cancelled_write_bytes ;
   __u64 nvcsw ;
   __u64 nivcsw ;
   __u64 ac_utimescaled ;
   __u64 ac_stimescaled ;
   __u64 cpu_scaled_run_real_total ;
   __u64 freepages_count ;
   __u64 freepages_delay_total ;
};
struct idr_layer {
   unsigned long bitmap ;
   struct idr_layer *ary[64U] ;
   int count ;
   int layer ;
   struct callback_head callback_head ;
};
struct idr {
   struct idr_layer *top ;
   struct idr_layer *id_free ;
   int layers ;
   int id_free_cnt ;
   spinlock_t lock ;
};
struct xattr_handler {
   char const *prefix ;
   int flags ;
   size_t (*list)(struct dentry * , char * , size_t , char const * , size_t ,
                  int ) ;
   int (*get)(struct dentry * , char const * , void * , size_t , int ) ;
   int (*set)(struct dentry * , char const * , void const * , size_t , int ,
              int ) ;
};
struct simple_xattrs {
   struct list_head head ;
   spinlock_t lock ;
};
struct cgroupfs_root;
struct cgroup_subsys;
struct cgroup;
struct css_id;
struct cgroup_subsys_state {
   struct cgroup *cgroup ;
   atomic_t refcnt ;
   unsigned long flags ;
   struct css_id *id ;
   struct work_struct dput_work ;
};
struct cgroup {
   unsigned long flags ;
   atomic_t count ;
   int id ;
   struct list_head sibling ;
   struct list_head children ;
   struct list_head files ;
   struct cgroup *parent ;
   struct dentry *dentry ;
   struct cgroup_subsys_state *subsys[12U] ;
   struct cgroupfs_root *root ;
   struct cgroup *top_cgroup ;
   struct list_head css_sets ;
   struct list_head allcg_node ;
   struct list_head cft_q_node ;
   struct list_head release_list ;
   struct list_head pidlists ;
   struct mutex pidlist_mutex ;
   struct callback_head callback_head ;
   struct list_head event_list ;
   spinlock_t event_list_lock ;
   struct simple_xattrs xattrs ;
};
struct css_set {
   atomic_t refcount ;
   struct hlist_node hlist ;
   struct list_head tasks ;
   struct list_head cg_links ;
   struct cgroup_subsys_state *subsys[12U] ;
   struct callback_head callback_head ;
};
struct cgroup_map_cb {
   int (*fill)(struct cgroup_map_cb * , char const * , u64 ) ;
   void *state ;
};
struct cftype {
   char name[64U] ;
   int private ;
   umode_t mode ;
   size_t max_write_len ;
   unsigned int flags ;
   struct simple_xattrs xattrs ;
   int (*open)(struct inode * , struct file * ) ;
   ssize_t (*read)(struct cgroup * , struct cftype * , struct file * , char * , size_t ,
                   loff_t * ) ;
   u64 (*read_u64)(struct cgroup * , struct cftype * ) ;
   s64 (*read_s64)(struct cgroup * , struct cftype * ) ;
   int (*read_map)(struct cgroup * , struct cftype * , struct cgroup_map_cb * ) ;
   int (*read_seq_string)(struct cgroup * , struct cftype * , struct seq_file * ) ;
   ssize_t (*write)(struct cgroup * , struct cftype * , struct file * , char const * ,
                    size_t , loff_t * ) ;
   int (*write_u64)(struct cgroup * , struct cftype * , u64 ) ;
   int (*write_s64)(struct cgroup * , struct cftype * , s64 ) ;
   int (*write_string)(struct cgroup * , struct cftype * , char const * ) ;
   int (*trigger)(struct cgroup * , unsigned int ) ;
   int (*release)(struct inode * , struct file * ) ;
   int (*register_event)(struct cgroup * , struct cftype * , struct eventfd_ctx * ,
                         char const * ) ;
   void (*unregister_event)(struct cgroup * , struct cftype * , struct eventfd_ctx * ) ;
};
struct cftype_set {
   struct list_head node ;
   struct cftype *cfts ;
};
struct cgroup_taskset;
struct cgroup_subsys {
   struct cgroup_subsys_state *(*css_alloc)(struct cgroup * ) ;
   int (*css_online)(struct cgroup * ) ;
   void (*css_offline)(struct cgroup * ) ;
   void (*css_free)(struct cgroup * ) ;
   int (*can_attach)(struct cgroup * , struct cgroup_taskset * ) ;
   void (*cancel_attach)(struct cgroup * , struct cgroup_taskset * ) ;
   void (*attach)(struct cgroup * , struct cgroup_taskset * ) ;
   void (*fork)(struct task_struct * ) ;
   void (*exit)(struct cgroup * , struct cgroup * , struct task_struct * ) ;
   void (*bind)(struct cgroup * ) ;
   int subsys_id ;
   int active ;
   int disabled ;
   int early_init ;
   bool use_id ;
   bool broken_hierarchy ;
   bool warned_broken_hierarchy ;
   char const *name ;
   struct cgroupfs_root *root ;
   struct list_head sibling ;
   struct idr idr ;
   spinlock_t id_lock ;
   struct list_head cftsets ;
   struct cftype *base_cftypes ;
   struct cftype_set base_cftset ;
   struct module *module ;
};
struct netprio_map {
   struct callback_head rcu ;
   u32 priomap_len ;
   u32 priomap[] ;
};
struct xfrm_policy;
struct xfrm_state;
struct request_sock;
struct mnt_namespace;
struct ipc_namespace;
struct nsproxy {
   atomic_t count ;
   struct uts_namespace *uts_ns ;
   struct ipc_namespace *ipc_ns ;
   struct mnt_namespace *mnt_ns ;
   struct pid_namespace *pid_ns ;
   struct net *net_ns ;
};
struct nlmsghdr {
   __u32 nlmsg_len ;
   __u16 nlmsg_type ;
   __u16 nlmsg_flags ;
   __u32 nlmsg_seq ;
   __u32 nlmsg_pid ;
};
struct nlattr {
   __u16 nla_len ;
   __u16 nla_type ;
};
struct netlink_callback {
   struct sk_buff *skb ;
   struct nlmsghdr const *nlh ;
   int (*dump)(struct sk_buff * , struct netlink_callback * ) ;
   int (*done)(struct netlink_callback * ) ;
   void *data ;
   struct module *module ;
   u16 family ;
   u16 min_dump_alloc ;
   unsigned int prev_seq ;
   unsigned int seq ;
   long args[6U] ;
};
struct ndmsg {
   __u8 ndm_family ;
   __u8 ndm_pad1 ;
   __u16 ndm_pad2 ;
   __s32 ndm_ifindex ;
   __u16 ndm_state ;
   __u8 ndm_flags ;
   __u8 ndm_type ;
};
struct rtnl_link_stats64 {
   __u64 rx_packets ;
   __u64 tx_packets ;
   __u64 rx_bytes ;
   __u64 tx_bytes ;
   __u64 rx_errors ;
   __u64 tx_errors ;
   __u64 rx_dropped ;
   __u64 tx_dropped ;
   __u64 multicast ;
   __u64 collisions ;
   __u64 rx_length_errors ;
   __u64 rx_over_errors ;
   __u64 rx_crc_errors ;
   __u64 rx_frame_errors ;
   __u64 rx_fifo_errors ;
   __u64 rx_missed_errors ;
   __u64 tx_aborted_errors ;
   __u64 tx_carrier_errors ;
   __u64 tx_fifo_errors ;
   __u64 tx_heartbeat_errors ;
   __u64 tx_window_errors ;
   __u64 rx_compressed ;
   __u64 tx_compressed ;
};
struct ifla_vf_info {
   __u32 vf ;
   __u8 mac[32U] ;
   __u32 vlan ;
   __u32 qos ;
   __u32 tx_rate ;
   __u32 spoofchk ;
};
struct netpoll_info;
struct phy_device;
struct wireless_dev;
enum netdev_tx {
    __NETDEV_TX_MIN = (-0x7FFFFFFF-1),
    NETDEV_TX_OK = 0,
    NETDEV_TX_BUSY = 16,
    NETDEV_TX_LOCKED = 32
} ;
typedef enum netdev_tx netdev_tx_t;
struct net_device_stats {
   unsigned long rx_packets ;
   unsigned long tx_packets ;
   unsigned long rx_bytes ;
   unsigned long tx_bytes ;
   unsigned long rx_errors ;
   unsigned long tx_errors ;
   unsigned long rx_dropped ;
   unsigned long tx_dropped ;
   unsigned long multicast ;
   unsigned long collisions ;
   unsigned long rx_length_errors ;
   unsigned long rx_over_errors ;
   unsigned long rx_crc_errors ;
   unsigned long rx_frame_errors ;
   unsigned long rx_fifo_errors ;
   unsigned long rx_missed_errors ;
   unsigned long tx_aborted_errors ;
   unsigned long tx_carrier_errors ;
   unsigned long tx_fifo_errors ;
   unsigned long tx_heartbeat_errors ;
   unsigned long tx_window_errors ;
   unsigned long rx_compressed ;
   unsigned long tx_compressed ;
};
struct neigh_parms;
struct netdev_hw_addr {
   struct list_head list ;
   unsigned char addr[32U] ;
   unsigned char type ;
   bool synced ;
   bool global_use ;
   int refcount ;
   struct callback_head callback_head ;
};
struct netdev_hw_addr_list {
   struct list_head list ;
   int count ;
};
struct hh_cache {
   u16 hh_len ;
   u16 __pad ;
   seqlock_t hh_lock ;
   unsigned long hh_data[16U] ;
};
struct header_ops {
   int (*create)(struct sk_buff * , struct net_device * , unsigned short , void const * ,
                 void const * , unsigned int ) ;
   int (*parse)(struct sk_buff const * , unsigned char * ) ;
   int (*rebuild)(struct sk_buff * ) ;
   int (*cache)(struct neighbour const * , struct hh_cache * , __be16 ) ;
   void (*cache_update)(struct hh_cache * , struct net_device const * , unsigned char const * ) ;
};
struct napi_struct {
   struct list_head poll_list ;
   unsigned long state ;
   int weight ;
   unsigned int gro_count ;
   int (*poll)(struct napi_struct * , int ) ;
   spinlock_t poll_lock ;
   int poll_owner ;
   struct net_device *dev ;
   struct sk_buff *gro_list ;
   struct sk_buff *skb ;
   struct list_head dev_list ;
};
enum rx_handler_result {
    RX_HANDLER_CONSUMED = 0,
    RX_HANDLER_ANOTHER = 1,
    RX_HANDLER_EXACT = 2,
    RX_HANDLER_PASS = 3
} ;
typedef enum rx_handler_result rx_handler_result_t;
typedef rx_handler_result_t rx_handler_func_t(struct sk_buff ** );
struct Qdisc;
struct netdev_queue {
   struct net_device *dev ;
   struct Qdisc *qdisc ;
   struct Qdisc *qdisc_sleeping ;
   struct kobject kobj ;
   int numa_node ;
   spinlock_t _xmit_lock ;
   int xmit_lock_owner ;
   unsigned long trans_start ;
   unsigned long trans_timeout ;
   unsigned long state ;
   struct dql dql ;
};
struct rps_map {
   unsigned int len ;
   struct callback_head rcu ;
   u16 cpus[0U] ;
};
struct rps_dev_flow {
   u16 cpu ;
   u16 filter ;
   unsigned int last_qtail ;
};
struct rps_dev_flow_table {
   unsigned int mask ;
   struct callback_head rcu ;
   struct work_struct free_work ;
   struct rps_dev_flow flows[0U] ;
};
struct netdev_rx_queue {
   struct rps_map *rps_map ;
   struct rps_dev_flow_table *rps_flow_table ;
   struct kobject kobj ;
   struct net_device *dev ;
};
struct xps_map {
   unsigned int len ;
   unsigned int alloc_len ;
   struct callback_head rcu ;
   u16 queues[0U] ;
};
struct xps_dev_maps {
   struct callback_head rcu ;
   struct xps_map *cpu_map[0U] ;
};
struct netdev_tc_txq {
   u16 count ;
   u16 offset ;
};
struct netdev_fcoe_hbainfo {
   char manufacturer[64U] ;
   char serial_number[64U] ;
   char hardware_version[64U] ;
   char driver_version[64U] ;
   char optionrom_version[64U] ;
   char firmware_version[64U] ;
   char model[256U] ;
   char model_description[256U] ;
};
struct net_device_ops {
   int (*ndo_init)(struct net_device * ) ;
   void (*ndo_uninit)(struct net_device * ) ;
   int (*ndo_open)(struct net_device * ) ;
   int (*ndo_stop)(struct net_device * ) ;
   netdev_tx_t (*ndo_start_xmit)(struct sk_buff * , struct net_device * ) ;
   u16 (*ndo_select_queue)(struct net_device * , struct sk_buff * ) ;
   void (*ndo_change_rx_flags)(struct net_device * , int ) ;
   void (*ndo_set_rx_mode)(struct net_device * ) ;
   int (*ndo_set_mac_address)(struct net_device * , void * ) ;
   int (*ndo_validate_addr)(struct net_device * ) ;
   int (*ndo_do_ioctl)(struct net_device * , struct ifreq * , int ) ;
   int (*ndo_set_config)(struct net_device * , struct ifmap * ) ;
   int (*ndo_change_mtu)(struct net_device * , int ) ;
   int (*ndo_neigh_setup)(struct net_device * , struct neigh_parms * ) ;
   void (*ndo_tx_timeout)(struct net_device * ) ;
   struct rtnl_link_stats64 *(*ndo_get_stats64)(struct net_device * , struct rtnl_link_stats64 * ) ;
   struct net_device_stats *(*ndo_get_stats)(struct net_device * ) ;
   int (*ndo_vlan_rx_add_vid)(struct net_device * , unsigned short ) ;
   int (*ndo_vlan_rx_kill_vid)(struct net_device * , unsigned short ) ;
   void (*ndo_poll_controller)(struct net_device * ) ;
   int (*ndo_netpoll_setup)(struct net_device * , struct netpoll_info * , gfp_t ) ;
   void (*ndo_netpoll_cleanup)(struct net_device * ) ;
   int (*ndo_set_vf_mac)(struct net_device * , int , u8 * ) ;
   int (*ndo_set_vf_vlan)(struct net_device * , int , u16 , u8 ) ;
   int (*ndo_set_vf_tx_rate)(struct net_device * , int , int ) ;
   int (*ndo_set_vf_spoofchk)(struct net_device * , int , bool ) ;
   int (*ndo_get_vf_config)(struct net_device * , int , struct ifla_vf_info * ) ;
   int (*ndo_set_vf_port)(struct net_device * , int , struct nlattr ** ) ;
   int (*ndo_get_vf_port)(struct net_device * , int , struct sk_buff * ) ;
   int (*ndo_setup_tc)(struct net_device * , u8 ) ;
   int (*ndo_fcoe_enable)(struct net_device * ) ;
   int (*ndo_fcoe_disable)(struct net_device * ) ;
   int (*ndo_fcoe_ddp_setup)(struct net_device * , u16 , struct scatterlist * , unsigned int ) ;
   int (*ndo_fcoe_ddp_done)(struct net_device * , u16 ) ;
   int (*ndo_fcoe_ddp_target)(struct net_device * , u16 , struct scatterlist * ,
                              unsigned int ) ;
   int (*ndo_fcoe_get_hbainfo)(struct net_device * , struct netdev_fcoe_hbainfo * ) ;
   int (*ndo_fcoe_get_wwn)(struct net_device * , u64 * , int ) ;
   int (*ndo_rx_flow_steer)(struct net_device * , struct sk_buff const * , u16 ,
                            u32 ) ;
   int (*ndo_add_slave)(struct net_device * , struct net_device * ) ;
   int (*ndo_del_slave)(struct net_device * , struct net_device * ) ;
   netdev_features_t (*ndo_fix_features)(struct net_device * , netdev_features_t ) ;
   int (*ndo_set_features)(struct net_device * , netdev_features_t ) ;
   int (*ndo_neigh_construct)(struct neighbour * ) ;
   void (*ndo_neigh_destroy)(struct neighbour * ) ;
   int (*ndo_fdb_add)(struct ndmsg * , struct nlattr ** , struct net_device * , unsigned char const * ,
                      u16 ) ;
   int (*ndo_fdb_del)(struct ndmsg * , struct net_device * , unsigned char const * ) ;
   int (*ndo_fdb_dump)(struct sk_buff * , struct netlink_callback * , struct net_device * ,
                       int ) ;
   int (*ndo_bridge_setlink)(struct net_device * , struct nlmsghdr * ) ;
   int (*ndo_bridge_getlink)(struct sk_buff * , u32 , u32 , struct net_device * ) ;
};
struct iw_handler_def;
struct iw_public_data;
struct vlan_info;
struct in_device;
struct dn_dev;
struct inet6_dev;
struct cpu_rmap;
struct pcpu_lstats;
struct pcpu_tstats;
struct pcpu_dstats;
union __anonunion_ldv_38489_223 {
   void *ml_priv ;
   struct pcpu_lstats *lstats ;
   struct pcpu_tstats *tstats ;
   struct pcpu_dstats *dstats ;
};
struct garp_port;
struct rtnl_link_ops;
struct net_device {
   char name[16U] ;
   struct hlist_node name_hlist ;
   char *ifalias ;
   unsigned long mem_end ;
   unsigned long mem_start ;
   unsigned long base_addr ;
   unsigned int irq ;
   unsigned long state ;
   struct list_head dev_list ;
   struct list_head napi_list ;
   struct list_head unreg_list ;
   netdev_features_t features ;
   netdev_features_t hw_features ;
   netdev_features_t wanted_features ;
   netdev_features_t vlan_features ;
   netdev_features_t hw_enc_features ;
   int ifindex ;
   int iflink ;
   struct net_device_stats stats ;
   atomic_long_t rx_dropped ;
   struct iw_handler_def const *wireless_handlers ;
   struct iw_public_data *wireless_data ;
   struct net_device_ops const *netdev_ops ;
   struct ethtool_ops const *ethtool_ops ;
   struct header_ops const *header_ops ;
   unsigned int flags ;
   unsigned int priv_flags ;
   unsigned short gflags ;
   unsigned short padded ;
   unsigned char operstate ;
   unsigned char link_mode ;
   unsigned char if_port ;
   unsigned char dma ;
   unsigned int mtu ;
   unsigned short type ;
   unsigned short hard_header_len ;
   unsigned short needed_headroom ;
   unsigned short needed_tailroom ;
   unsigned char perm_addr[32U] ;
   unsigned char addr_assign_type ;
   unsigned char addr_len ;
   unsigned char neigh_priv_len ;
   unsigned short dev_id ;
   spinlock_t addr_list_lock ;
   struct netdev_hw_addr_list uc ;
   struct netdev_hw_addr_list mc ;
   bool uc_promisc ;
   unsigned int promiscuity ;
   unsigned int allmulti ;
   struct vlan_info *vlan_info ;
   struct dsa_switch_tree *dsa_ptr ;
   void *atalk_ptr ;
   struct in_device *ip_ptr ;
   struct dn_dev *dn_ptr ;
   struct inet6_dev *ip6_ptr ;
   void *ax25_ptr ;
   struct wireless_dev *ieee80211_ptr ;
   unsigned long last_rx ;
   struct net_device *master ;
   unsigned char *dev_addr ;
   struct netdev_hw_addr_list dev_addrs ;
   unsigned char broadcast[32U] ;
   struct kset *queues_kset ;
   struct netdev_rx_queue *_rx ;
   unsigned int num_rx_queues ;
   unsigned int real_num_rx_queues ;
   struct cpu_rmap *rx_cpu_rmap ;
   rx_handler_func_t *rx_handler ;
   void *rx_handler_data ;
   struct netdev_queue *ingress_queue ;
   struct netdev_queue *_tx ;
   unsigned int num_tx_queues ;
   unsigned int real_num_tx_queues ;
   struct Qdisc *qdisc ;
   unsigned long tx_queue_len ;
   spinlock_t tx_global_lock ;
   struct xps_dev_maps *xps_maps ;
   unsigned long trans_start ;
   int watchdog_timeo ;
   struct timer_list watchdog_timer ;
   int *pcpu_refcnt ;
   struct list_head todo_list ;
   struct hlist_node index_hlist ;
   struct list_head link_watch_list ;
   unsigned char reg_state ;
   bool dismantle ;
   unsigned short rtnl_link_state ;
   void (*destructor)(struct net_device * ) ;
   struct netpoll_info *npinfo ;
   struct net *nd_net ;
   union __anonunion_ldv_38489_223 ldv_38489 ;
   struct garp_port *garp_port ;
   struct device dev ;
   struct attribute_group const *sysfs_groups[4U] ;
   struct rtnl_link_ops const *rtnl_link_ops ;
   unsigned int gso_max_size ;
   u16 gso_max_segs ;
   struct dcbnl_rtnl_ops const *dcbnl_ops ;
   u8 num_tc ;
   struct netdev_tc_txq tc_to_txq[16U] ;
   u8 prio_tc_map[16U] ;
   unsigned int fcoe_ddp_xid ;
   struct netprio_map *priomap ;
   struct phy_device *phydev ;
   struct lock_class_key *qdisc_tx_busylock ;
   int group ;
   struct pm_qos_request pm_qos_req ;
};
struct res_counter {
   unsigned long long usage ;
   unsigned long long max_usage ;
   unsigned long long limit ;
   unsigned long long soft_limit ;
   unsigned long long failcnt ;
   spinlock_t lock ;
   struct res_counter *parent ;
};
struct sock_filter {
   __u16 code ;
   __u8 jt ;
   __u8 jf ;
   __u32 k ;
};
struct sk_filter {
   atomic_t refcnt ;
   unsigned int len ;
   unsigned int (*bpf_func)(struct sk_buff const * , struct sock_filter const * ) ;
   struct callback_head rcu ;
   struct sock_filter insns[0U] ;
};
struct poll_table_struct {
   void (*_qproc)(struct file * , wait_queue_head_t * , struct poll_table_struct * ) ;
   unsigned long _key ;
};
struct nla_policy {
   u16 type ;
   u16 len ;
};
struct rtnl_link_ops {
   struct list_head list ;
   char const *kind ;
   size_t priv_size ;
   void (*setup)(struct net_device * ) ;
   int maxtype ;
   struct nla_policy const *policy ;
   int (*validate)(struct nlattr ** , struct nlattr ** ) ;
   int (*newlink)(struct net * , struct net_device * , struct nlattr ** , struct nlattr ** ) ;
   int (*changelink)(struct net_device * , struct nlattr ** , struct nlattr ** ) ;
   void (*dellink)(struct net_device * , struct list_head * ) ;
   size_t (*get_size)(struct net_device const * ) ;
   int (*fill_info)(struct sk_buff * , struct net_device const * ) ;
   size_t (*get_xstats_size)(struct net_device const * ) ;
   int (*fill_xstats)(struct sk_buff * , struct net_device const * ) ;
   unsigned int (*get_num_tx_queues)(void) ;
   unsigned int (*get_num_rx_queues)(void) ;
};
struct neigh_table;
struct neigh_parms {
   struct net *net ;
   struct net_device *dev ;
   struct neigh_parms *next ;
   int (*neigh_setup)(struct neighbour * ) ;
   void (*neigh_cleanup)(struct neighbour * ) ;
   struct neigh_table *tbl ;
   void *sysctl_table ;
   int dead ;
   atomic_t refcnt ;
   struct callback_head callback_head ;
   int base_reachable_time ;
   int retrans_time ;
   int gc_staletime ;
   int reachable_time ;
   int delay_probe_time ;
   int queue_len_bytes ;
   int ucast_probes ;
   int app_probes ;
   int mcast_probes ;
   int anycast_delay ;
   int proxy_delay ;
   int proxy_qlen ;
   int locktime ;
};
struct neigh_statistics {
   unsigned long allocs ;
   unsigned long destroys ;
   unsigned long hash_grows ;
   unsigned long res_failed ;
   unsigned long lookups ;
   unsigned long hits ;
   unsigned long rcv_probes_mcast ;
   unsigned long rcv_probes_ucast ;
   unsigned long periodic_gc_runs ;
   unsigned long forced_gc_runs ;
   unsigned long unres_discards ;
};
struct neigh_ops;
struct neighbour {
   struct neighbour *next ;
   struct neigh_table *tbl ;
   struct neigh_parms *parms ;
   unsigned long confirmed ;
   unsigned long updated ;
   rwlock_t lock ;
   atomic_t refcnt ;
   struct sk_buff_head arp_queue ;
   unsigned int arp_queue_len_bytes ;
   struct timer_list timer ;
   unsigned long used ;
   atomic_t probes ;
   __u8 flags ;
   __u8 nud_state ;
   __u8 type ;
   __u8 dead ;
   seqlock_t ha_lock ;
   unsigned char ha[32U] ;
   struct hh_cache hh ;
   int (*output)(struct neighbour * , struct sk_buff * ) ;
   struct neigh_ops const *ops ;
   struct callback_head rcu ;
   struct net_device *dev ;
   u8 primary_key[0U] ;
};
struct neigh_ops {
   int family ;
   void (*solicit)(struct neighbour * , struct sk_buff * ) ;
   void (*error_report)(struct neighbour * , struct sk_buff * ) ;
   int (*output)(struct neighbour * , struct sk_buff * ) ;
   int (*connected_output)(struct neighbour * , struct sk_buff * ) ;
};
struct pneigh_entry {
   struct pneigh_entry *next ;
   struct net *net ;
   struct net_device *dev ;
   u8 flags ;
   u8 key[0U] ;
};
struct neigh_hash_table {
   struct neighbour **hash_buckets ;
   unsigned int hash_shift ;
   __u32 hash_rnd[4U] ;
   struct callback_head rcu ;
};
struct neigh_table {
   struct neigh_table *next ;
   int family ;
   int entry_size ;
   int key_len ;
   __u32 (*hash)(void const * , struct net_device const * , __u32 * ) ;
   int (*constructor)(struct neighbour * ) ;
   int (*pconstructor)(struct pneigh_entry * ) ;
   void (*pdestructor)(struct pneigh_entry * ) ;
   void (*proxy_redo)(struct sk_buff * ) ;
   char *id ;
   struct neigh_parms parms ;
   int gc_interval ;
   int gc_thresh1 ;
   int gc_thresh2 ;
   int gc_thresh3 ;
   unsigned long last_flush ;
   struct delayed_work gc_work ;
   struct timer_list proxy_timer ;
   struct sk_buff_head proxy_queue ;
   atomic_t entries ;
   rwlock_t lock ;
   unsigned long last_rand ;
   struct neigh_statistics *stats ;
   struct neigh_hash_table *nht ;
   struct pneigh_entry **phash_buckets ;
};
union __anonunion_ldv_41618_228 {
   unsigned long expires ;
   struct dst_entry *from ;
};
struct dn_route;
union __anonunion_ldv_41643_229 {
   struct dst_entry *next ;
   struct rtable *rt_next ;
   struct rt6_info *rt6_next ;
   struct dn_route *dn_next ;
};
struct dst_entry {
   struct callback_head callback_head ;
   struct dst_entry *child ;
   struct net_device *dev ;
   struct dst_ops *ops ;
   unsigned long _metrics ;
   union __anonunion_ldv_41618_228 ldv_41618 ;
   struct dst_entry *path ;
   void *__pad0 ;
   struct xfrm_state *xfrm ;
   int (*input)(struct sk_buff * ) ;
   int (*output)(struct sk_buff * ) ;
   unsigned short flags ;
   unsigned short pending_confirm ;
   short error ;
   short obsolete ;
   unsigned short header_len ;
   unsigned short trailer_len ;
   __u32 tclassid ;
   long __pad_to_align_refcnt[2U] ;
   atomic_t __refcnt ;
   int __use ;
   unsigned long lastuse ;
   union __anonunion_ldv_41643_229 ldv_41643 ;
};
struct __anonstruct_socket_lock_t_230 {
   spinlock_t slock ;
   int owned ;
   wait_queue_head_t wq ;
   struct lockdep_map dep_map ;
};
typedef struct __anonstruct_socket_lock_t_230 socket_lock_t;
struct proto;
typedef __u32 __portpair;
typedef __u64 __addrpair;
struct __anonstruct_ldv_41860_232 {
   __be32 skc_daddr ;
   __be32 skc_rcv_saddr ;
};
union __anonunion_ldv_41861_231 {
   __addrpair skc_addrpair ;
   struct __anonstruct_ldv_41860_232 ldv_41860 ;
};
union __anonunion_ldv_41865_233 {
   unsigned int skc_hash ;
   __u16 skc_u16hashes[2U] ;
};
struct __anonstruct_ldv_41871_235 {
   __be16 skc_dport ;
   __u16 skc_num ;
};
union __anonunion_ldv_41872_234 {
   __portpair skc_portpair ;
   struct __anonstruct_ldv_41871_235 ldv_41871 ;
};
union __anonunion_ldv_41880_236 {
   struct hlist_node skc_bind_node ;
   struct hlist_nulls_node skc_portaddr_node ;
};
union __anonunion_ldv_41887_237 {
   struct hlist_node skc_node ;
   struct hlist_nulls_node skc_nulls_node ;
};
struct sock_common {
   union __anonunion_ldv_41861_231 ldv_41861 ;
   union __anonunion_ldv_41865_233 ldv_41865 ;
   union __anonunion_ldv_41872_234 ldv_41872 ;
   unsigned short skc_family ;
   unsigned char volatile skc_state ;
   unsigned char skc_reuse ;
   int skc_bound_dev_if ;
   union __anonunion_ldv_41880_236 ldv_41880 ;
   struct proto *skc_prot ;
   struct net *skc_net ;
   int skc_dontcopy_begin[0U] ;
   union __anonunion_ldv_41887_237 ldv_41887 ;
   int skc_tx_queue_mapping ;
   atomic_t skc_refcnt ;
   int skc_dontcopy_end[0U] ;
};
struct cg_proto;
struct __anonstruct_sk_backlog_238 {
   atomic_t rmem_alloc ;
   int len ;
   struct sk_buff *head ;
   struct sk_buff *tail ;
};
struct sock {
   struct sock_common __sk_common ;
   socket_lock_t sk_lock ;
   struct sk_buff_head sk_receive_queue ;
   struct __anonstruct_sk_backlog_238 sk_backlog ;
   int sk_forward_alloc ;
   __u32 sk_rxhash ;
   atomic_t sk_drops ;
   int sk_rcvbuf ;
   struct sk_filter *sk_filter ;
   struct socket_wq *sk_wq ;
   struct sk_buff_head sk_async_wait_queue ;
   struct xfrm_policy *sk_policy[2U] ;
   unsigned long sk_flags ;
   struct dst_entry *sk_rx_dst ;
   struct dst_entry *sk_dst_cache ;
   spinlock_t sk_dst_lock ;
   atomic_t sk_wmem_alloc ;
   atomic_t sk_omem_alloc ;
   int sk_sndbuf ;
   struct sk_buff_head sk_write_queue ;
   unsigned char sk_shutdown : 2 ;
   unsigned char sk_no_check : 2 ;
   unsigned char sk_userlocks : 4 ;
   unsigned char sk_protocol ;
   unsigned short sk_type ;
   int sk_wmem_queued ;
   gfp_t sk_allocation ;
   netdev_features_t sk_route_caps ;
   netdev_features_t sk_route_nocaps ;
   int sk_gso_type ;
   unsigned int sk_gso_max_size ;
   u16 sk_gso_max_segs ;
   int sk_rcvlowat ;
   unsigned long sk_lingertime ;
   struct sk_buff_head sk_error_queue ;
   struct proto *sk_prot_creator ;
   rwlock_t sk_callback_lock ;
   int sk_err ;
   int sk_err_soft ;
   unsigned short sk_ack_backlog ;
   unsigned short sk_max_ack_backlog ;
   __u32 sk_priority ;
   __u32 sk_cgrp_prioidx ;
   struct pid *sk_peer_pid ;
   struct cred const *sk_peer_cred ;
   long sk_rcvtimeo ;
   long sk_sndtimeo ;
   void *sk_protinfo ;
   struct timer_list sk_timer ;
   ktime_t sk_stamp ;
   struct socket *sk_socket ;
   void *sk_user_data ;
   struct page_frag sk_frag ;
   struct sk_buff *sk_send_head ;
   __s32 sk_peek_off ;
   int sk_write_pending ;
   void *sk_security ;
   __u32 sk_mark ;
   u32 sk_classid ;
   struct cg_proto *sk_cgrp ;
   void (*sk_state_change)(struct sock * ) ;
   void (*sk_data_ready)(struct sock * , int ) ;
   void (*sk_write_space)(struct sock * ) ;
   void (*sk_error_report)(struct sock * ) ;
   int (*sk_backlog_rcv)(struct sock * , struct sk_buff * ) ;
   void (*sk_destruct)(struct sock * ) ;
};
struct request_sock_ops;
struct timewait_sock_ops;
struct inet_hashinfo;
struct raw_hashinfo;
struct udp_table;
union __anonunion_h_239 {
   struct inet_hashinfo *hashinfo ;
   struct udp_table *udp_table ;
   struct raw_hashinfo *raw_hash ;
};
struct proto {
   void (*close)(struct sock * , long ) ;
   int (*connect)(struct sock * , struct sockaddr * , int ) ;
   int (*disconnect)(struct sock * , int ) ;
   struct sock *(*accept)(struct sock * , int , int * ) ;
   int (*ioctl)(struct sock * , int , unsigned long ) ;
   int (*init)(struct sock * ) ;
   void (*destroy)(struct sock * ) ;
   void (*shutdown)(struct sock * , int ) ;
   int (*setsockopt)(struct sock * , int , int , char * , unsigned int ) ;
   int (*getsockopt)(struct sock * , int , int , char * , int * ) ;
   int (*compat_setsockopt)(struct sock * , int , int , char * , unsigned int ) ;
   int (*compat_getsockopt)(struct sock * , int , int , char * , int * ) ;
   int (*compat_ioctl)(struct sock * , unsigned int , unsigned long ) ;
   int (*sendmsg)(struct kiocb * , struct sock * , struct msghdr * , size_t ) ;
   int (*recvmsg)(struct kiocb * , struct sock * , struct msghdr * , size_t , int ,
                  int , int * ) ;
   int (*sendpage)(struct sock * , struct page * , int , size_t , int ) ;
   int (*bind)(struct sock * , struct sockaddr * , int ) ;
   int (*backlog_rcv)(struct sock * , struct sk_buff * ) ;
   void (*release_cb)(struct sock * ) ;
   void (*mtu_reduced)(struct sock * ) ;
   void (*hash)(struct sock * ) ;
   void (*unhash)(struct sock * ) ;
   void (*rehash)(struct sock * ) ;
   int (*get_port)(struct sock * , unsigned short ) ;
   void (*clear_sk)(struct sock * , int ) ;
   unsigned int inuse_idx ;
   void (*enter_memory_pressure)(struct sock * ) ;
   atomic_long_t *memory_allocated ;
   struct percpu_counter *sockets_allocated ;
   int *memory_pressure ;
   long *sysctl_mem ;
   int *sysctl_wmem ;
   int *sysctl_rmem ;
   int max_header ;
   bool no_autobind ;
   struct kmem_cache *slab ;
   unsigned int obj_size ;
   int slab_flags ;
   struct percpu_counter *orphan_count ;
   struct request_sock_ops *rsk_prot ;
   struct timewait_sock_ops *twsk_prot ;
   union __anonunion_h_239 h ;
   struct module *owner ;
   char name[32U] ;
   struct list_head node ;
   int (*init_cgroup)(struct mem_cgroup * , struct cgroup_subsys * ) ;
   void (*destroy_cgroup)(struct mem_cgroup * ) ;
   struct cg_proto *(*proto_cgroup)(struct mem_cgroup * ) ;
};
struct cg_proto {
   void (*enter_memory_pressure)(struct sock * ) ;
   struct res_counter *memory_allocated ;
   struct percpu_counter *sockets_allocated ;
   int *memory_pressure ;
   long *sysctl_mem ;
   unsigned long flags ;
   struct mem_cgroup *memcg ;
};
struct request_values {
};
struct request_sock_ops {
   int family ;
   int obj_size ;
   struct kmem_cache *slab ;
   char *slab_name ;
   int (*rtx_syn_ack)(struct sock * , struct request_sock * , struct request_values * ) ;
   void (*send_ack)(struct sock * , struct sk_buff * , struct request_sock * ) ;
   void (*send_reset)(struct sock * , struct sk_buff * ) ;
   void (*destructor)(struct request_sock * ) ;
   void (*syn_ack_timeout)(struct sock * , struct request_sock * ) ;
};
struct request_sock {
   struct request_sock *dl_next ;
   u16 mss ;
   u8 num_retrans ;
   unsigned char cookie_ts : 1 ;
   unsigned char num_timeout : 7 ;
   u32 window_clamp ;
   u32 rcv_wnd ;
   u32 ts_recent ;
   unsigned long expires ;
   struct request_sock_ops const *rsk_ops ;
   struct sock *sk ;
   u32 secid ;
   u32 peer_secid ;
};
struct timewait_sock_ops {
   struct kmem_cache *twsk_slab ;
   char *twsk_slab_name ;
   unsigned int twsk_obj_size ;
   int (*twsk_unique)(struct sock * , struct sock * , void * ) ;
   void (*twsk_destructor)(struct sock * ) ;
};
struct __anonstruct_near_246 {
   u16 index ;
   u16 dist ;
};
struct cpu_rmap {
   u16 size ;
   u16 used ;
   void **obj ;
   struct __anonstruct_near_246 near[0U] ;
};
struct mii_ioctl_data {
   __u16 phy_id ;
   __u16 reg_num ;
   __u16 val_in ;
   __u16 val_out ;
};
struct mdio_if_info {
   int prtad ;
   u32 mmds ;
   unsigned int mode_support ;
   struct net_device *dev ;
   int (*mdio_read)(struct net_device * , int , int , u16 ) ;
   int (*mdio_write)(struct net_device * , int , int , u16 , u16 ) ;
};
typedef u32 phandle;
struct property {
   char *name ;
   int length ;
   void *value ;
   struct property *next ;
   unsigned long _flags ;
   unsigned int unique_id ;
};
struct device_node {
   char const *name ;
   char const *type ;
   phandle phandle ;
   char const *full_name ;
   struct property *properties ;
   struct property *deadprops ;
   struct device_node *parent ;
   struct device_node *child ;
   struct device_node *sibling ;
   struct device_node *next ;
   struct device_node *allnext ;
   struct proc_dir_entry *pde ;
   struct kref kref ;
   unsigned long _flags ;
   void *data ;
};
enum efx_loopback_mode {
    LOOPBACK_NONE = 0,
    LOOPBACK_DATA = 1,
    LOOPBACK_GMAC = 2,
    LOOPBACK_XGMII = 3,
    LOOPBACK_XGXS = 4,
    LOOPBACK_XAUI = 5,
    LOOPBACK_GMII = 6,
    LOOPBACK_SGMII = 7,
    LOOPBACK_XGBR = 8,
    LOOPBACK_XFI = 9,
    LOOPBACK_XAUI_FAR = 10,
    LOOPBACK_GMII_FAR = 11,
    LOOPBACK_SGMII_FAR = 12,
    LOOPBACK_XFI_FAR = 13,
    LOOPBACK_GPHY = 14,
    LOOPBACK_PHYXS = 15,
    LOOPBACK_PCS = 16,
    LOOPBACK_PMAPMD = 17,
    LOOPBACK_XPORT = 18,
    LOOPBACK_XGMII_WS = 19,
    LOOPBACK_XAUI_WS = 20,
    LOOPBACK_XAUI_WS_FAR = 21,
    LOOPBACK_XAUI_WS_NEAR = 22,
    LOOPBACK_GMII_WS = 23,
    LOOPBACK_XFI_WS = 24,
    LOOPBACK_XFI_WS_FAR = 25,
    LOOPBACK_PHYXS_WS = 26,
    LOOPBACK_MAX = 27
} ;
enum reset_type {
    RESET_TYPE_INVISIBLE = 0,
    RESET_TYPE_ALL = 1,
    RESET_TYPE_WORLD = 2,
    RESET_TYPE_DISABLE = 3,
    RESET_TYPE_MAX_METHOD = 4,
    RESET_TYPE_TX_WATCHDOG = 5,
    RESET_TYPE_INT_ERROR = 6,
    RESET_TYPE_RX_RECOVERY = 7,
    RESET_TYPE_RX_DESC_FETCH = 8,
    RESET_TYPE_TX_DESC_FETCH = 9,
    RESET_TYPE_TX_SKIP = 10,
    RESET_TYPE_MC_FAILURE = 11,
    RESET_TYPE_MAX = 12
} ;
union efx_dword {
   __le32 u32[1U] ;
};
typedef union efx_dword efx_dword_t;
union efx_qword {
   __le64 u64[1U] ;
   __le32 u32[2U] ;
   efx_dword_t dword[2U] ;
};
typedef union efx_qword efx_qword_t;
union efx_oword {
   __le64 u64[2U] ;
   efx_qword_t qword[2U] ;
   __le32 u32[4U] ;
   efx_dword_t dword[4U] ;
};
typedef union efx_oword efx_oword_t;
struct efx_ptp_data;
struct efx_self_tests;
struct efx_special_buffer {
   void *addr ;
   dma_addr_t dma_addr ;
   unsigned int len ;
   unsigned int index ;
   unsigned int entries ;
};
union __anonunion_ldv_45115_248 {
   struct sk_buff const *skb ;
   void *heap_buf ;
};
struct efx_tx_buffer {
   union __anonunion_ldv_45115_248 ldv_45115 ;
   dma_addr_t dma_addr ;
   unsigned short flags ;
   unsigned short len ;
   unsigned short unmap_len ;
};
struct efx_nic;
struct efx_channel;
struct efx_buffer;
struct efx_tx_queue {
   struct efx_nic *efx ;
   unsigned int queue ;
   struct efx_channel *channel ;
   struct netdev_queue *core_txq ;
   struct efx_tx_buffer *buffer ;
   struct efx_buffer *tsoh_page ;
   struct efx_special_buffer txd ;
   unsigned int ptr_mask ;
   bool initialised ;
   unsigned int read_count ;
   unsigned int old_write_count ;
   unsigned int insert_count ;
   unsigned int write_count ;
   unsigned int old_read_count ;
   unsigned int tso_bursts ;
   unsigned int tso_long_headers ;
   unsigned int tso_packets ;
   unsigned int pushes ;
   unsigned int empty_read_count ;
   atomic_t flush_outstanding ;
};
union __anonunion_u_249 {
   struct sk_buff *skb ;
   struct page *page ;
};
struct efx_rx_buffer {
   dma_addr_t dma_addr ;
   union __anonunion_u_249 u ;
   unsigned int len ;
   u16 flags ;
};
struct efx_rx_queue {
   struct efx_nic *efx ;
   int core_index ;
   struct efx_rx_buffer *buffer ;
   struct efx_special_buffer rxd ;
   unsigned int ptr_mask ;
   bool enabled ;
   bool flush_pending ;
   int added_count ;
   int notified_count ;
   int removed_count ;
   unsigned int max_fill ;
   unsigned int fast_fill_trigger ;
   unsigned int min_fill ;
   unsigned int min_overfill ;
   unsigned int alloc_page_count ;
   unsigned int alloc_skb_count ;
   struct timer_list slow_fill ;
   unsigned int slow_fill_count ;
};
struct efx_buffer {
   void *addr ;
   dma_addr_t dma_addr ;
   unsigned int len ;
};
struct efx_channel_type;
struct efx_channel {
   struct efx_nic *efx ;
   int channel ;
   struct efx_channel_type const *type ;
   bool enabled ;
   int irq ;
   unsigned int irq_moderation ;
   struct net_device *napi_dev ;
   struct napi_struct napi_str ;
   bool work_pending ;
   struct efx_special_buffer eventq ;
   unsigned int eventq_mask ;
   unsigned int eventq_read_ptr ;
   int event_test_cpu ;
   unsigned int irq_count ;
   unsigned int irq_mod_score ;
   unsigned int rfs_filters_added ;
   int rx_alloc_level ;
   int rx_alloc_push_pages ;
   unsigned int n_rx_tobe_disc ;
   unsigned int n_rx_ip_hdr_chksum_err ;
   unsigned int n_rx_tcp_udp_chksum_err ;
   unsigned int n_rx_mcast_mismatch ;
   unsigned int n_rx_frm_trunc ;
   unsigned int n_rx_overlength ;
   unsigned int n_skbuff_leaks ;
   struct efx_rx_buffer *rx_pkt ;
   struct efx_rx_queue rx_queue ;
   struct efx_tx_queue tx_queue[4U] ;
};
struct efx_channel_type {
   void (*handle_no_channel)(struct efx_nic * ) ;
   int (*pre_probe)(struct efx_channel * ) ;
   void (*post_remove)(struct efx_channel * ) ;
   void (*get_name)(struct efx_channel * , char * , size_t ) ;
   struct efx_channel *(*copy)(struct efx_channel const * ) ;
   void (*receive_skb)(struct efx_channel * , struct sk_buff * ) ;
   bool keep_eventq ;
};
enum efx_led_mode {
    EFX_LED_OFF = 0,
    EFX_LED_ON = 1,
    EFX_LED_DEFAULT = 2
} ;
enum efx_int_mode {
    EFX_INT_MODE_MSIX = 0,
    EFX_INT_MODE_MSI = 1,
    EFX_INT_MODE_LEGACY = 2,
    EFX_INT_MODE_MAX = 3
} ;
enum nic_state {
    STATE_UNINIT = 0,
    STATE_READY = 1,
    STATE_DISABLED = 2
} ;
struct efx_link_state {
   bool up ;
   bool fd ;
   u8 fc ;
   unsigned int speed ;
};
struct efx_phy_operations {
   int (*probe)(struct efx_nic * ) ;
   int (*init)(struct efx_nic * ) ;
   void (*fini)(struct efx_nic * ) ;
   void (*remove)(struct efx_nic * ) ;
   int (*reconfigure)(struct efx_nic * ) ;
   bool (*poll)(struct efx_nic * ) ;
   void (*get_settings)(struct efx_nic * , struct ethtool_cmd * ) ;
   int (*set_settings)(struct efx_nic * , struct ethtool_cmd * ) ;
   void (*set_npage_adv)(struct efx_nic * , u32 ) ;
   int (*test_alive)(struct efx_nic * ) ;
   char const *(*test_name)(struct efx_nic * , unsigned int ) ;
   int (*run_tests)(struct efx_nic * , int * , unsigned int ) ;
   int (*get_module_eeprom)(struct efx_nic * , struct ethtool_eeprom * , u8 * ) ;
   int (*get_module_info)(struct efx_nic * , struct ethtool_modinfo * ) ;
};
enum efx_phy_mode {
    PHY_MODE_NORMAL = 0,
    PHY_MODE_TX_DISABLED = 1,
    PHY_MODE_LOW_POWER = 2,
    PHY_MODE_OFF = 4,
    PHY_MODE_SPECIAL = 8
} ;
struct efx_mac_stats {
   u64 tx_bytes ;
   u64 tx_good_bytes ;
   u64 tx_bad_bytes ;
   u64 tx_packets ;
   u64 tx_bad ;
   u64 tx_pause ;
   u64 tx_control ;
   u64 tx_unicast ;
   u64 tx_multicast ;
   u64 tx_broadcast ;
   u64 tx_lt64 ;
   u64 tx_64 ;
   u64 tx_65_to_127 ;
   u64 tx_128_to_255 ;
   u64 tx_256_to_511 ;
   u64 tx_512_to_1023 ;
   u64 tx_1024_to_15xx ;
   u64 tx_15xx_to_jumbo ;
   u64 tx_gtjumbo ;
   u64 tx_collision ;
   u64 tx_single_collision ;
   u64 tx_multiple_collision ;
   u64 tx_excessive_collision ;
   u64 tx_deferred ;
   u64 tx_late_collision ;
   u64 tx_excessive_deferred ;
   u64 tx_non_tcpudp ;
   u64 tx_mac_src_error ;
   u64 tx_ip_src_error ;
   u64 rx_bytes ;
   u64 rx_good_bytes ;
   u64 rx_bad_bytes ;
   u64 rx_packets ;
   u64 rx_good ;
   u64 rx_bad ;
   u64 rx_pause ;
   u64 rx_control ;
   u64 rx_unicast ;
   u64 rx_multicast ;
   u64 rx_broadcast ;
   u64 rx_lt64 ;
   u64 rx_64 ;
   u64 rx_65_to_127 ;
   u64 rx_128_to_255 ;
   u64 rx_256_to_511 ;
   u64 rx_512_to_1023 ;
   u64 rx_1024_to_15xx ;
   u64 rx_15xx_to_jumbo ;
   u64 rx_gtjumbo ;
   u64 rx_bad_lt64 ;
   u64 rx_bad_64_to_15xx ;
   u64 rx_bad_15xx_to_jumbo ;
   u64 rx_bad_gtjumbo ;
   u64 rx_overflow ;
   u64 rx_missed ;
   u64 rx_false_carrier ;
   u64 rx_symbol_error ;
   u64 rx_align_error ;
   u64 rx_length_error ;
   u64 rx_internal_error ;
   u64 rx_good_lt64 ;
};
union efx_multicast_hash {
   u8 byte[32U] ;
   efx_oword_t oword[2U] ;
};
struct efx_filter_state;
struct efx_vf;
struct efx_nic_type;
struct efx_nic {
   char name[16U] ;
   struct pci_dev *pci_dev ;
   struct efx_nic_type const *type ;
   int legacy_irq ;
   bool legacy_irq_enabled ;
   struct workqueue_struct *workqueue ;
   char workqueue_name[16U] ;
   struct work_struct reset_work ;
   resource_size_t membase_phys ;
   void *membase ;
   enum efx_int_mode interrupt_mode ;
   unsigned int timer_quantum_ns ;
   bool irq_rx_adaptive ;
   unsigned int irq_rx_moderation ;
   u32 msg_enable ;
   enum nic_state state ;
   unsigned long reset_pending ;
   struct efx_channel *channel[32U] ;
   char channel_name[32U][22U] ;
   struct efx_channel_type const *extra_channel_type[2U] ;
   unsigned int rxq_entries ;
   unsigned int txq_entries ;
   unsigned int txq_stop_thresh ;
   unsigned int txq_wake_thresh ;
   unsigned int tx_dc_base ;
   unsigned int rx_dc_base ;
   unsigned int sram_lim_qw ;
   unsigned int next_buffer_table ;
   unsigned int n_channels ;
   unsigned int n_rx_channels ;
   unsigned int rss_spread ;
   unsigned int tx_channel_offset ;
   unsigned int n_tx_channels ;
   unsigned int rx_buffer_len ;
   unsigned int rx_buffer_order ;
   u8 rx_hash_key[40U] ;
   u32 rx_indir_table[128U] ;
   unsigned int int_error_count ;
   unsigned long int_error_expire ;
   struct efx_buffer irq_status ;
   unsigned int irq_zero_count ;
   unsigned int irq_level ;
   struct delayed_work selftest_work ;
   struct list_head mtd_list ;
   void *nic_data ;
   struct mutex mac_lock ;
   struct work_struct mac_work ;
   bool port_enabled ;
   bool port_initialized ;
   struct net_device *net_dev ;
   struct efx_buffer stats_buffer ;
   unsigned int phy_type ;
   struct efx_phy_operations const *phy_op ;
   void *phy_data ;
   struct mdio_if_info mdio ;
   unsigned int mdio_bus ;
   enum efx_phy_mode phy_mode ;
   u32 link_advertising ;
   struct efx_link_state link_state ;
   unsigned int n_link_state_changes ;
   bool promiscuous ;
   union efx_multicast_hash multicast_hash ;
   u8 wanted_fc ;
   unsigned int fc_disable ;
   atomic_t rx_reset ;
   enum efx_loopback_mode loopback_mode ;
   u64 loopback_modes ;
   void *loopback_selftest ;
   struct efx_filter_state *filter_state ;
   atomic_t drain_pending ;
   atomic_t rxq_flush_pending ;
   atomic_t rxq_flush_outstanding ;
   wait_queue_head_t flush_wq ;
   struct efx_channel *vfdi_channel ;
   struct efx_vf *vf ;
   unsigned int vf_count ;
   unsigned int vf_init_count ;
   unsigned int vi_scale ;
   unsigned int vf_buftbl_base ;
   struct efx_buffer vfdi_status ;
   struct list_head local_addr_list ;
   struct list_head local_page_list ;
   struct mutex local_lock ;
   struct work_struct peer_work ;
   struct efx_ptp_data *ptp_data ;
   struct delayed_work monitor_work ;
   spinlock_t biu_lock ;
   int last_irq_cpu ;
   unsigned int n_rx_nodesc_drop_cnt ;
   struct efx_mac_stats mac_stats ;
   spinlock_t stats_lock ;
};
struct efx_nic_type {
   int (*probe)(struct efx_nic * ) ;
   void (*remove)(struct efx_nic * ) ;
   int (*init)(struct efx_nic * ) ;
   void (*dimension_resources)(struct efx_nic * ) ;
   void (*fini)(struct efx_nic * ) ;
   void (*monitor)(struct efx_nic * ) ;
   enum reset_type (*map_reset_reason)(enum reset_type ) ;
   int (*map_reset_flags)(u32 * ) ;
   int (*reset)(struct efx_nic * , enum reset_type ) ;
   int (*probe_port)(struct efx_nic * ) ;
   void (*remove_port)(struct efx_nic * ) ;
   bool (*handle_global_event)(struct efx_channel * , efx_qword_t * ) ;
   void (*prepare_flush)(struct efx_nic * ) ;
   void (*finish_flush)(struct efx_nic * ) ;
   void (*update_stats)(struct efx_nic * ) ;
   void (*start_stats)(struct efx_nic * ) ;
   void (*stop_stats)(struct efx_nic * ) ;
   void (*set_id_led)(struct efx_nic * , enum efx_led_mode ) ;
   void (*push_irq_moderation)(struct efx_channel * ) ;
   int (*reconfigure_port)(struct efx_nic * ) ;
   int (*reconfigure_mac)(struct efx_nic * ) ;
   bool (*check_mac_fault)(struct efx_nic * ) ;
   void (*get_wol)(struct efx_nic * , struct ethtool_wolinfo * ) ;
   int (*set_wol)(struct efx_nic * , u32 ) ;
   void (*resume_wol)(struct efx_nic * ) ;
   int (*test_chip)(struct efx_nic * , struct efx_self_tests * ) ;
   int (*test_nvram)(struct efx_nic * ) ;
   int revision ;
   unsigned int mem_map_size ;
   unsigned int txd_ptr_tbl_base ;
   unsigned int rxd_ptr_tbl_base ;
   unsigned int buf_tbl_base ;
   unsigned int evq_ptr_tbl_base ;
   unsigned int evq_rptr_tbl_base ;
   u64 max_dma_mask ;
   unsigned int rx_buffer_hash_size ;
   unsigned int rx_buffer_padding ;
   unsigned int max_interrupt_mode ;
   unsigned int phys_addr_channels ;
   unsigned int timer_period_max ;
   netdev_features_t offload_features ;
};
enum efx_filter_priority {
    EFX_FILTER_PRI_HINT = 0,
    EFX_FILTER_PRI_MANUAL = 1,
    EFX_FILTER_PRI_REQUIRED = 2
} ;
struct efx_loopback_self_tests {
   int tx_sent[4U] ;
   int tx_done[4U] ;
   int rx_good ;
   int rx_bad ;
};
struct efx_self_tests {
   int phy_alive ;
   int nvram ;
   int interrupt ;
   int eventq_dma[32U] ;
   int eventq_int[32U] ;
   int registers ;
   int phy_ext[20U] ;
   struct efx_loopback_self_tests loopback[18U] ;
};
typedef int ldv_func_ret_type___2;
typedef int ldv_func_ret_type___6;
typedef int ldv_func_ret_type___20;
struct paravirt_callee_save {
   void *func ;
};
struct pv_irq_ops {
   struct paravirt_callee_save save_fl ;
   struct paravirt_callee_save restore_fl ;
   struct paravirt_callee_save irq_disable ;
   struct paravirt_callee_save irq_enable ;
   void (*safe_halt)(void) ;
   void (*halt)(void) ;
   void (*adjust_exception_frame)(void) ;
};
enum irqreturn {
    IRQ_NONE = 0,
    IRQ_HANDLED = 1,
    IRQ_WAKE_THREAD = 2
} ;
typedef enum irqreturn irqreturn_t;
enum hrtimer_restart;
struct __wait_queue;
typedef struct __wait_queue wait_queue_t;
struct __wait_queue {
   unsigned int flags ;
   void *private ;
   int (*func)(wait_queue_t * , unsigned int , int , void * ) ;
   struct list_head task_list ;
};
struct i2c_device_id {
   char name[20U] ;
   kernel_ulong_t driver_data ;
};
struct rt_mutex {
   raw_spinlock_t wait_lock ;
   struct plist_head wait_list ;
   struct task_struct *owner ;
   int save_state ;
   char const *name ;
   char const *file ;
   int line ;
   void *magic ;
};
struct i2c_msg {
   __u16 addr ;
   __u16 flags ;
   __u16 len ;
   __u8 *buf ;
};
union i2c_smbus_data {
   __u8 byte ;
   __u16 word ;
   __u8 block[34U] ;
};
struct i2c_algorithm;
struct i2c_adapter;
struct i2c_client;
struct i2c_driver;
struct i2c_board_info;
struct i2c_driver {
   unsigned int class ;
   int (*attach_adapter)(struct i2c_adapter * ) ;
   int (*detach_adapter)(struct i2c_adapter * ) ;
   int (*probe)(struct i2c_client * , struct i2c_device_id const * ) ;
   int (*remove)(struct i2c_client * ) ;
   void (*shutdown)(struct i2c_client * ) ;
   int (*suspend)(struct i2c_client * , pm_message_t ) ;
   int (*resume)(struct i2c_client * ) ;
   void (*alert)(struct i2c_client * , unsigned int ) ;
   int (*command)(struct i2c_client * , unsigned int , void * ) ;
   struct device_driver driver ;
   struct i2c_device_id const *id_table ;
   int (*detect)(struct i2c_client * , struct i2c_board_info * ) ;
   unsigned short const *address_list ;
   struct list_head clients ;
};
struct i2c_client {
   unsigned short flags ;
   unsigned short addr ;
   char name[20U] ;
   struct i2c_adapter *adapter ;
   struct i2c_driver *driver ;
   struct device dev ;
   int irq ;
   struct list_head detected ;
};
struct i2c_board_info {
   char type[20U] ;
   unsigned short flags ;
   unsigned short addr ;
   void *platform_data ;
   struct dev_archdata *archdata ;
   struct device_node *of_node ;
   struct acpi_dev_node acpi_node ;
   int irq ;
};
struct i2c_algorithm {
   int (*master_xfer)(struct i2c_adapter * , struct i2c_msg * , int ) ;
   int (*smbus_xfer)(struct i2c_adapter * , u16 , unsigned short , char , u8 ,
                     int , union i2c_smbus_data * ) ;
   u32 (*functionality)(struct i2c_adapter * ) ;
};
struct i2c_adapter {
   struct module *owner ;
   unsigned int class ;
   struct i2c_algorithm const *algo ;
   void *algo_data ;
   struct rt_mutex bus_lock ;
   int timeout ;
   int retries ;
   struct device dev ;
   int nr ;
   char name[48U] ;
   struct completion dev_released ;
   struct mutex userspace_clients_lock ;
   struct list_head userspace_clients ;
};
struct i2c_algo_bit_data {
   void *data ;
   void (*setsda)(void * , int ) ;
   void (*setscl)(void * , int ) ;
   int (*getsda)(void * ) ;
   int (*getscl)(void * ) ;
   int (*pre_xfer)(struct i2c_adapter * ) ;
   void (*post_xfer)(struct i2c_adapter * ) ;
   int udelay ;
   int timeout ;
};
struct efx_spi_device {
   int device_id ;
   unsigned int size ;
   unsigned int addr_len ;
   unsigned char munge_address : 1 ;
   u8 erase_command ;
   unsigned int erase_size ;
   unsigned int block_size ;
};
struct falcon_board_type {
   u8 id ;
   int (*init)(struct efx_nic * ) ;
   void (*init_phy)(struct efx_nic * ) ;
   void (*fini)(struct efx_nic * ) ;
   void (*set_id_led)(struct efx_nic * , enum efx_led_mode ) ;
   int (*monitor)(struct efx_nic * ) ;
};
struct falcon_board {
   struct falcon_board_type const *type ;
   int major ;
   int minor ;
   struct i2c_adapter i2c_adap ;
   struct i2c_algo_bit_data i2c_data ;
   struct i2c_client *hwmon_client ;
   struct i2c_client *ioexp_client ;
};
struct falcon_nic_data {
   struct pci_dev *pci_dev2 ;
   struct falcon_board board ;
   unsigned int stats_disable_count ;
   bool stats_pending ;
   struct timer_list stats_timer ;
   u32 *stats_dma_done ;
   struct efx_spi_device spi_flash ;
   struct efx_spi_device spi_eeprom ;
   struct mutex spi_lock ;
   struct mutex mdio_lock ;
   bool xmac_poll_required ;
};
struct efx_nic_register_test {
   unsigned int address ;
   efx_oword_t mask ;
};
struct efx_nic_reg {
   unsigned int offset : 24 ;
   unsigned char min_revision : 2 ;
   unsigned char max_revision : 2 ;
};
struct efx_nic_reg_table {
   unsigned int offset : 24 ;
   unsigned char min_revision : 2 ;
   unsigned char max_revision : 2 ;
   unsigned char step : 6 ;
   unsigned int rows : 21 ;
};
typedef __u16 __le16;
struct exec_domain;
struct map_segment;
struct exec_domain {
   char const *name ;
   void (*handler)(int , struct pt_regs * ) ;
   unsigned char pers_low ;
   unsigned char pers_high ;
   unsigned long *signal_map ;
   unsigned long *signal_invmap ;
   struct map_segment *err_map ;
   struct map_segment *socktype_map ;
   struct map_segment *sockopt_map ;
   struct map_segment *af_map ;
   struct module *module ;
   struct exec_domain *next ;
};
struct __anonstruct_mm_segment_t_27 {
   unsigned long seg ;
};
typedef struct __anonstruct_mm_segment_t_27 mm_segment_t;
struct compat_timespec;
struct __anonstruct_futex_33 {
   u32 *uaddr ;
   u32 val ;
   u32 flags ;
   u32 bitset ;
   u64 time ;
   u32 *uaddr2 ;
};
struct __anonstruct_nanosleep_34 {
   clockid_t clockid ;
   struct timespec *rmtp ;
   struct compat_timespec *compat_rmtp ;
   u64 expires ;
};
struct pollfd;
struct __anonstruct_poll_35 {
   struct pollfd *ufds ;
   int nfds ;
   int has_timeout ;
   unsigned long tv_sec ;
   unsigned long tv_nsec ;
};
union __anonunion_ldv_6824_32 {
   struct __anonstruct_futex_33 futex ;
   struct __anonstruct_nanosleep_34 nanosleep ;
   struct __anonstruct_poll_35 poll ;
};
struct restart_block {
   long (*fn)(struct restart_block * ) ;
   union __anonunion_ldv_6824_32 ldv_6824 ;
};
struct thread_info {
   struct task_struct *task ;
   struct exec_domain *exec_domain ;
   __u32 flags ;
   __u32 status ;
   __u32 cpu ;
   int preempt_count ;
   mm_segment_t addr_limit ;
   struct restart_block restart_block ;
   void *sysenter_return ;
   unsigned char sig_on_uaccess_error : 1 ;
   unsigned char uaccess_err : 1 ;
};
enum hrtimer_restart;
typedef s32 compat_time_t;
struct compat_timespec {
   compat_time_t tv_sec ;
   s32 tv_nsec ;
};
struct falcon_nvconfig_board_v2 {
   __le16 nports ;
   u8 port0_phy_addr ;
   u8 port0_phy_type ;
   u8 port1_phy_addr ;
   u8 port1_phy_type ;
   __le16 asic_sub_revision ;
   __le16 board_revision ;
};
struct falcon_nvconfig_board_v3 {
   __le32 spi_device_type[2U] ;
};
struct falcon_nvconfig {
   efx_oword_t ee_vpd_cfg_reg ;
   u8 mac_address[2U][8U] ;
   efx_oword_t pcie_sd_ctl0123_reg ;
   efx_oword_t pcie_sd_ctl45_reg ;
   efx_oword_t pcie_pcs_ctl_stat_reg ;
   efx_oword_t hw_init_reg ;
   efx_oword_t nic_stat_reg ;
   efx_oword_t glb_ctl_reg ;
   efx_oword_t srm_cfg_reg ;
   efx_oword_t spare_reg ;
   __le16 board_magic_num ;
   __le16 board_struct_ver ;
   __le16 board_checksum ;
   struct falcon_nvconfig_board_v2 board_v2 ;
   efx_oword_t ee_base_page_reg ;
   struct falcon_nvconfig_board_v3 board_v3 ;
};
typedef int ldv_func_ret_type___7;
typedef __u16 uint16_t;
enum hrtimer_restart;
enum efx_mcdi_mode {
    MCDI_MODE_POLL = 0,
    MCDI_MODE_EVENTS = 1
} ;
struct efx_mcdi_iface {
   atomic_t state ;
   wait_queue_head_t wq ;
   spinlock_t iface_lock ;
   enum efx_mcdi_mode mode ;
   unsigned int credits ;
   unsigned int seqno ;
   unsigned int resprc ;
   size_t resplen ;
};
struct efx_mcdi_mon_attribute;
struct efx_mcdi_mon {
   struct efx_buffer dma_buf ;
   struct mutex update_lock ;
   unsigned long last_update ;
   struct device *device ;
   struct efx_mcdi_mon_attribute *attrs ;
   unsigned int n_attrs ;
};
struct siena_nic_data {
   struct efx_mcdi_iface mcdi ;
   int wol_filter_id ;
   struct efx_mcdi_mon hwmon ;
};
typedef __u16 __sum16;
enum hrtimer_restart;
struct in6_addr;
struct skb_frag_struct;
typedef struct skb_frag_struct skb_frag_t;
struct __anonstruct_page_144 {
   struct page *p ;
};
struct skb_frag_struct {
   struct __anonstruct_page_144 page ;
   __u32 page_offset ;
   __u32 size ;
};
struct skb_shared_hwtstamps {
   ktime_t hwtstamp ;
   ktime_t syststamp ;
};
struct skb_shared_info {
   unsigned char nr_frags ;
   __u8 tx_flags ;
   unsigned short gso_size ;
   unsigned short gso_segs ;
   unsigned short gso_type ;
   struct sk_buff *frag_list ;
   struct skb_shared_hwtstamps hwtstamps ;
   __be32 ip6_frag_id ;
   atomic_t dataref ;
   void *destructor_arg ;
   skb_frag_t frags[17U] ;
};
struct icmpv6_mib_device {
   atomic_long_t mibs[5U] ;
};
struct icmpv6msg_mib_device {
   atomic_long_t mibs[512U] ;
};
union __anonunion_in6_u_209 {
   __u8 u6_addr8[16U] ;
   __be16 u6_addr16[8U] ;
   __be32 u6_addr32[4U] ;
};
struct in6_addr {
   union __anonunion_in6_u_209 in6_u ;
};
struct tcphdr {
   __be16 source ;
   __be16 dest ;
   __be32 seq ;
   __be32 ack_seq ;
   unsigned char res1 : 4 ;
   unsigned char doff : 4 ;
   unsigned char fin : 1 ;
   unsigned char syn : 1 ;
   unsigned char rst : 1 ;
   unsigned char psh : 1 ;
   unsigned char ack : 1 ;
   unsigned char urg : 1 ;
   unsigned char ece : 1 ;
   unsigned char cwr : 1 ;
   __be16 window ;
   __sum16 check ;
   __be16 urg_ptr ;
};
struct iphdr {
   unsigned char ihl : 4 ;
   unsigned char version : 4 ;
   __u8 tos ;
   __be16 tot_len ;
   __be16 id ;
   __be16 frag_off ;
   __u8 ttl ;
   __u8 protocol ;
   __sum16 check ;
   __be32 saddr ;
   __be32 daddr ;
};
struct ipv6hdr {
   unsigned char priority : 4 ;
   unsigned char version : 4 ;
   __u8 flow_lbl[3U] ;
   __be16 payload_len ;
   __u8 nexthdr ;
   __u8 hop_limit ;
   struct in6_addr saddr ;
   struct in6_addr daddr ;
};
struct ipv6_devconf {
   __s32 forwarding ;
   __s32 hop_limit ;
   __s32 mtu6 ;
   __s32 accept_ra ;
   __s32 accept_redirects ;
   __s32 autoconf ;
   __s32 dad_transmits ;
   __s32 rtr_solicits ;
   __s32 rtr_solicit_interval ;
   __s32 rtr_solicit_delay ;
   __s32 force_mld_version ;
   __s32 use_tempaddr ;
   __s32 temp_valid_lft ;
   __s32 temp_prefered_lft ;
   __s32 regen_max_retry ;
   __s32 max_desync_factor ;
   __s32 max_addresses ;
   __s32 accept_ra_defrtr ;
   __s32 accept_ra_pinfo ;
   __s32 accept_ra_rtr_pref ;
   __s32 rtr_probe_interval ;
   __s32 accept_ra_rt_info_max_plen ;
   __s32 proxy_ndp ;
   __s32 accept_source_route ;
   __s32 optimistic_dad ;
   __s32 mc_forwarding ;
   __s32 disable_ipv6 ;
   __s32 accept_dad ;
   __s32 force_tllao ;
   __s32 ndisc_notify ;
   void *sysctl ;
};
struct ip6_sf_list {
   struct ip6_sf_list *sf_next ;
   struct in6_addr sf_addr ;
   unsigned long sf_count[2U] ;
   unsigned char sf_gsresp ;
   unsigned char sf_oldin ;
   unsigned char sf_crcount ;
};
struct ifmcaddr6 {
   struct in6_addr mca_addr ;
   struct inet6_dev *idev ;
   struct ifmcaddr6 *next ;
   struct ip6_sf_list *mca_sources ;
   struct ip6_sf_list *mca_tomb ;
   unsigned int mca_sfmode ;
   unsigned char mca_crcount ;
   unsigned long mca_sfcount[2U] ;
   struct timer_list mca_timer ;
   unsigned int mca_flags ;
   int mca_users ;
   atomic_t mca_refcnt ;
   spinlock_t mca_lock ;
   unsigned long mca_cstamp ;
   unsigned long mca_tstamp ;
};
struct ifacaddr6 {
   struct in6_addr aca_addr ;
   struct inet6_dev *aca_idev ;
   struct rt6_info *aca_rt ;
   struct ifacaddr6 *aca_next ;
   int aca_users ;
   atomic_t aca_refcnt ;
   spinlock_t aca_lock ;
   unsigned long aca_cstamp ;
   unsigned long aca_tstamp ;
};
struct ipv6_devstat {
   struct proc_dir_entry *proc_dir_entry ;
   struct ipstats_mib *ipv6[1U] ;
   struct icmpv6_mib_device *icmpv6dev ;
   struct icmpv6msg_mib_device *icmpv6msgdev ;
};
struct inet6_dev {
   struct net_device *dev ;
   struct list_head addr_list ;
   struct ifmcaddr6 *mc_list ;
   struct ifmcaddr6 *mc_tomb ;
   spinlock_t mc_lock ;
   unsigned char mc_qrv ;
   unsigned char mc_gq_running ;
   unsigned char mc_ifc_count ;
   unsigned long mc_v1_seen ;
   unsigned long mc_maxdelay ;
   struct timer_list mc_gq_timer ;
   struct timer_list mc_ifc_timer ;
   struct ifacaddr6 *ac_list ;
   rwlock_t lock ;
   atomic_t refcnt ;
   __u32 if_flags ;
   int dead ;
   u8 rndid[8U] ;
   struct timer_list regen_timer ;
   struct list_head tempaddr_list ;
   struct neigh_parms *nd_parms ;
   struct inet6_dev *next ;
   struct ipv6_devconf cnf ;
   struct ipv6_devstat stats ;
   unsigned long tstamp ;
   struct callback_head rcu ;
};
struct vlan_ethhdr {
   unsigned char h_dest[6U] ;
   unsigned char h_source[6U] ;
   __be16 h_vlan_proto ;
   __be16 h_vlan_TCI ;
   __be16 h_vlan_encapsulated_proto ;
};
struct tso_state {
   unsigned int out_len ;
   unsigned int seqnum ;
   unsigned int ipv4_id ;
   unsigned int packet_space ;
   dma_addr_t dma_addr ;
   unsigned int in_len ;
   unsigned int unmap_len ;
   dma_addr_t unmap_addr ;
   unsigned short dma_flags ;
   __be16 protocol ;
   unsigned int ip_off ;
   unsigned int tcp_off ;
   unsigned int header_len ;
   unsigned int ip_base_len ;
};
enum hrtimer_restart;
enum gro_result {
    GRO_MERGED = 0,
    GRO_MERGED_FREE = 1,
    GRO_HELD = 2,
    GRO_NORMAL = 3,
    GRO_DROP = 4
} ;
typedef enum gro_result gro_result_t;
struct efx_rx_page_state {
   unsigned int refcnt ;
   dma_addr_t dma_addr ;
   unsigned int __pad[0U] ;
};
enum efx_rx_alloc_method {
    RX_ALLOC_METHOD_AUTO = 0,
    RX_ALLOC_METHOD_SKB = 1,
    RX_ALLOC_METHOD_PAGE = 2
} ;
enum hrtimer_restart;
enum efx_filter_flags {
    EFX_FILTER_FLAG_RX_RSS = 1,
    EFX_FILTER_FLAG_RX_SCATTER = 2,
    EFX_FILTER_FLAG_RX = 8,
    EFX_FILTER_FLAG_TX = 16
} ;
struct efx_filter_spec {
   unsigned char type : 4 ;
   unsigned char priority : 4 ;
   u8 flags ;
   u16 dmaq_id ;
   u32 data[3U] ;
};
enum efx_filter_table_id {
    EFX_FILTER_TABLE_RX_IP = 0,
    EFX_FILTER_TABLE_RX_MAC = 1,
    EFX_FILTER_TABLE_RX_DEF = 2,
    EFX_FILTER_TABLE_TX_MAC = 3,
    EFX_FILTER_TABLE_COUNT = 4
} ;
struct efx_filter_table {
   enum efx_filter_table_id id ;
   u32 offset ;
   unsigned int size ;
   unsigned int step ;
   unsigned int used ;
   unsigned long *used_bitmap ;
   struct efx_filter_spec *spec ;
   unsigned int search_depth[10U] ;
};
struct efx_filter_state {
   spinlock_t lock ;
   struct efx_filter_table table[4U] ;
   u32 *rps_flow_id ;
   unsigned int rps_expire_index ;
};
enum hrtimer_restart;
enum hrtimer_restart;
enum hrtimer_restart;
struct udphdr {
   __be16 source ;
   __be16 dest ;
   __be16 len ;
   __sum16 check ;
};
struct efx_loopback_payload {
   struct ethhdr header ;
   struct iphdr ip ;
   struct udphdr udp ;
   __be16 iteration ;
   char const msg[64U] ;
};
struct efx_loopback_state {
   bool flush ;
   int packet_count ;
   struct sk_buff **skbs ;
   bool offload_csum ;
   atomic_t rx_good ;
   atomic_t rx_bad ;
   struct efx_loopback_payload payload ;
};
enum hrtimer_restart;
struct ethtool_string {
   char name[32U] ;
};
enum ldv_29381 {
    EFX_ETHTOOL_STAT_SOURCE_mac_stats = 0,
    EFX_ETHTOOL_STAT_SOURCE_nic = 1,
    EFX_ETHTOOL_STAT_SOURCE_channel = 2,
    EFX_ETHTOOL_STAT_SOURCE_tx_queue = 3
} ;
struct efx_ethtool_stat {
   char const *name ;
   enum ldv_29381 source ;
   unsigned int offset ;
   u64 (*get_stat)(void * ) ;
};
enum hrtimer_restart;
struct qt202x_phy_data {
   enum efx_phy_mode phy_mode ;
   bool bug17190_in_bad_state ;
   unsigned long bug17190_timer ;
   u32 firmware_ver ;
};
enum hrtimer_restart;
typedef int ldv_func_ret_type___4;
enum hrtimer_restart;
struct tenxpress_phy_data {
   enum efx_loopback_mode loopback_mode ;
   enum efx_phy_mode phy_mode ;
   int bad_lp_tries ;
};
enum hrtimer_restart;
struct txc43128_data {
   unsigned long bug10934_timer ;
   enum efx_phy_mode phy_mode ;
   enum efx_loopback_mode loopback_mode ;
};
enum hrtimer_restart;
enum hrtimer_restart;
enum hrtimer_restart;
struct efx_mcdi_phy_data {
   u32 flags ;
   u32 type ;
   u32 supported_cap ;
   u32 channel ;
   u32 port ;
   u32 stats_mask ;
   u8 name[20U] ;
   u32 media ;
   u32 mmd_mask ;
   u8 revision[20U] ;
   u32 forced_cap ;
};
enum hrtimer_restart;
enum efx_hwmon_type {
    EFX_HWMON_UNKNOWN = 0,
    EFX_HWMON_TEMP = 1,
    EFX_HWMON_COOL = 2,
    EFX_HWMON_IN = 3
} ;
struct __anonstruct_efx_mcdi_sensor_type_225 {
   char const *label ;
   enum efx_hwmon_type hwmon_type ;
   int port ;
};
struct efx_mcdi_mon_attribute {
   struct device_attribute dev_attr ;
   unsigned int index ;
   unsigned int type ;
   unsigned int limit_value ;
   char name[12U] ;
};
enum hrtimer_restart;
struct hwtstamp_config {
   int flags ;
   int tx_type ;
   int rx_filter ;
};
struct cdev {
   struct kobject kobj ;
   struct module *owner ;
   struct file_operations const *ops ;
   struct list_head list ;
   dev_t dev ;
   unsigned int count ;
};
struct pps_event_time {
   struct timespec ts_real ;
};
struct ptp_clock_time {
   __s64 sec ;
   __u32 nsec ;
   __u32 reserved ;
};
struct ptp_extts_request {
   unsigned int index ;
   unsigned int flags ;
   unsigned int rsv[2U] ;
};
struct ptp_perout_request {
   struct ptp_clock_time start ;
   struct ptp_clock_time period ;
   unsigned int index ;
   unsigned int flags ;
   unsigned int rsv[4U] ;
};
enum ldv_29063 {
    PTP_CLK_REQ_EXTTS = 0,
    PTP_CLK_REQ_PEROUT = 1,
    PTP_CLK_REQ_PPS = 2
} ;
union __anonunion_ldv_41631_238 {
   struct ptp_extts_request extts ;
   struct ptp_perout_request perout ;
};
struct ptp_clock_request {
   enum ldv_29063 type ;
   union __anonunion_ldv_41631_238 ldv_41631 ;
};
struct ptp_clock_info {
   struct module *owner ;
   char name[16U] ;
   s32 max_adj ;
   int n_alarm ;
   int n_ext_ts ;
   int n_per_out ;
   int pps ;
   int (*adjfreq)(struct ptp_clock_info * , s32 ) ;
   int (*adjtime)(struct ptp_clock_info * , s64 ) ;
   int (*gettime)(struct ptp_clock_info * , struct timespec * ) ;
   int (*settime)(struct ptp_clock_info * , struct timespec const * ) ;
   int (*enable)(struct ptp_clock_info * , struct ptp_clock_request * , int ) ;
};
struct ptp_clock;
union __anonunion_ldv_41673_239 {
   u64 timestamp ;
   struct pps_event_time pps_times ;
};
struct ptp_clock_event {
   int type ;
   int index ;
   union __anonunion_ldv_41673_239 ldv_41673 ;
};
enum ptp_packet_state {
    PTP_PACKET_STATE_UNMATCHED = 0,
    PTP_PACKET_STATE_MATCHED = 1,
    PTP_PACKET_STATE_TIMED_OUT = 2,
    PTP_PACKET_STATE_MATCH_UNWANTED = 3
} ;
struct efx_ptp_match {
   u32 words[2U] ;
   unsigned long expiry ;
   enum ptp_packet_state state ;
};
struct efx_ptp_event_rx {
   struct list_head link ;
   u32 seq0 ;
   u32 seq1 ;
   ktime_t hwtimestamp ;
   unsigned long expiry ;
};
struct efx_ptp_timeset {
   u32 host_start ;
   u32 seconds ;
   u32 nanoseconds ;
   u32 host_end ;
   u32 waitns ;
   u32 window ;
};
struct efx_ptp_data {
   struct efx_channel *channel ;
   struct sk_buff_head rxq ;
   struct sk_buff_head txq ;
   struct list_head evt_list ;
   struct list_head evt_free_list ;
   spinlock_t evt_lock ;
   struct efx_ptp_event_rx rx_evts[8U] ;
   struct workqueue_struct *workwq ;
   struct work_struct work ;
   bool reset_required ;
   u32 rxfilter_event ;
   u32 rxfilter_general ;
   bool rxfilter_installed ;
   struct hwtstamp_config config ;
   bool enabled ;
   unsigned int mode ;
   efx_qword_t evt_frags[3U] ;
   int evt_frag_idx ;
   int evt_code ;
   struct efx_buffer start ;
   struct pps_event_time host_time_pps ;
   unsigned int last_sync_ns ;
   unsigned int base_sync_ns ;
   bool base_sync_valid ;
   s64 current_adjfreq ;
   struct ptp_clock *phc_clock ;
   struct ptp_clock_info phc_clock_info ;
   struct work_struct pps_work ;
   struct workqueue_struct *pps_workwq ;
   bool nic_ts_enabled ;
   u8 txbuf[252U] ;
   struct efx_ptp_timeset timeset[12U] ;
};
typedef unsigned char u_char;
typedef unsigned long u_long;
enum hrtimer_restart;
struct kvec {
   void *iov_base ;
   size_t iov_len ;
};
struct otp_info {
   __u32 start ;
   __u32 length ;
   __u32 locked ;
};
struct nand_oobfree {
   __u32 offset ;
   __u32 length ;
};
struct mtd_ecc_stats {
   __u32 corrected ;
   __u32 failed ;
   __u32 badblocks ;
   __u32 bbtblocks ;
};
struct erase_info {
   struct mtd_info *mtd ;
   uint64_t addr ;
   uint64_t len ;
   uint64_t fail_addr ;
   u_long time ;
   u_long retries ;
   unsigned int dev ;
   unsigned int cell ;
   void (*callback)(struct erase_info * ) ;
   u_long priv ;
   u_char state ;
   struct erase_info *next ;
};
struct mtd_erase_region_info {
   uint64_t offset ;
   uint32_t erasesize ;
   uint32_t numblocks ;
   unsigned long *lockmap ;
};
struct mtd_oob_ops {
   unsigned int mode ;
   size_t len ;
   size_t retlen ;
   size_t ooblen ;
   size_t oobretlen ;
   uint32_t ooboffs ;
   uint8_t *datbuf ;
   uint8_t *oobbuf ;
};
struct nand_ecclayout {
   __u32 eccbytes ;
   __u32 eccpos[640U] ;
   __u32 oobavail ;
   struct nand_oobfree oobfree[32U] ;
};
struct mtd_info {
   u_char type ;
   uint32_t flags ;
   uint64_t size ;
   uint32_t erasesize ;
   uint32_t writesize ;
   uint32_t writebufsize ;
   uint32_t oobsize ;
   uint32_t oobavail ;
   unsigned int erasesize_shift ;
   unsigned int writesize_shift ;
   unsigned int erasesize_mask ;
   unsigned int writesize_mask ;
   unsigned int bitflip_threshold ;
   char const *name ;
   int index ;
   struct nand_ecclayout *ecclayout ;
   unsigned int ecc_strength ;
   int numeraseregions ;
   struct mtd_erase_region_info *eraseregions ;
   int (*_erase)(struct mtd_info * , struct erase_info * ) ;
   int (*_point)(struct mtd_info * , loff_t , size_t , size_t * , void ** , resource_size_t * ) ;
   int (*_unpoint)(struct mtd_info * , loff_t , size_t ) ;
   unsigned long (*_get_unmapped_area)(struct mtd_info * , unsigned long , unsigned long ,
                                       unsigned long ) ;
   int (*_read)(struct mtd_info * , loff_t , size_t , size_t * , u_char * ) ;
   int (*_write)(struct mtd_info * , loff_t , size_t , size_t * , u_char const * ) ;
   int (*_panic_write)(struct mtd_info * , loff_t , size_t , size_t * , u_char const * ) ;
   int (*_read_oob)(struct mtd_info * , loff_t , struct mtd_oob_ops * ) ;
   int (*_write_oob)(struct mtd_info * , loff_t , struct mtd_oob_ops * ) ;
   int (*_get_fact_prot_info)(struct mtd_info * , struct otp_info * , size_t ) ;
   int (*_read_fact_prot_reg)(struct mtd_info * , loff_t , size_t , size_t * , u_char * ) ;
   int (*_get_user_prot_info)(struct mtd_info * , struct otp_info * , size_t ) ;
   int (*_read_user_prot_reg)(struct mtd_info * , loff_t , size_t , size_t * , u_char * ) ;
   int (*_write_user_prot_reg)(struct mtd_info * , loff_t , size_t , size_t * ,
                               u_char * ) ;
   int (*_lock_user_prot_reg)(struct mtd_info * , loff_t , size_t ) ;
   int (*_writev)(struct mtd_info * , struct kvec const * , unsigned long , loff_t ,
                  size_t * ) ;
   void (*_sync)(struct mtd_info * ) ;
   int (*_lock)(struct mtd_info * , loff_t , uint64_t ) ;
   int (*_unlock)(struct mtd_info * , loff_t , uint64_t ) ;
   int (*_is_locked)(struct mtd_info * , loff_t , uint64_t ) ;
   int (*_block_isbad)(struct mtd_info * , loff_t ) ;
   int (*_block_markbad)(struct mtd_info * , loff_t ) ;
   int (*_suspend)(struct mtd_info * ) ;
   void (*_resume)(struct mtd_info * ) ;
   int (*_get_device)(struct mtd_info * ) ;
   void (*_put_device)(struct mtd_info * ) ;
   struct backing_dev_info *backing_dev_info ;
   struct notifier_block reboot_notifier ;
   struct mtd_ecc_stats ecc_stats ;
   int subpage_sft ;
   void *priv ;
   struct module *owner ;
   struct device dev ;
   int usecount ;
};
struct mtd_partition;
struct mtd_part_parser_data;
struct __anonstruct_mcdi_231 {
   bool updating ;
   u8 nvram_type ;
   u16 fw_subtype ;
};
union __anonunion_ldv_42817_230 {
   struct __anonstruct_mcdi_231 mcdi ;
   size_t offset ;
};
struct efx_mtd_partition {
   struct mtd_info mtd ;
   union __anonunion_ldv_42817_230 ldv_42817 ;
   char const *type_name ;
   char name[36U] ;
};
struct efx_mtd_ops {
   int (*read)(struct mtd_info * , loff_t , size_t , size_t * , u8 * ) ;
   int (*erase)(struct mtd_info * , loff_t , size_t ) ;
   int (*write)(struct mtd_info * , loff_t , size_t , size_t * , u8 const * ) ;
   int (*sync)(struct mtd_info * ) ;
};
struct efx_mtd {
   struct list_head node ;
   struct efx_nic *efx ;
   struct efx_spi_device const *spi ;
   char const *name ;
   struct efx_mtd_ops const *ops ;
   size_t n_parts ;
   struct efx_mtd_partition part[0U] ;
};
struct siena_nvram_type_info {
   int port ;
   char const *name ;
};
typedef int ldv_func_ret_type___8;
typedef int ldv_func_ret_type___10;
enum hrtimer_restart;
struct pci_sysdata {
   int domain ;
   int node ;
   void *iommu ;
};
struct vfdi_status;
struct vfdi_endpoint {
   u8 mac_addr[6U] ;
   __be16 tci ;
};
struct __anonstruct_init_evq_231 {
   u32 index ;
   u32 buf_count ;
   u64 addr[] ;
};
struct __anonstruct_init_rxq_232 {
   u32 index ;
   u32 buf_count ;
   u32 evq ;
   u32 label ;
   u32 flags ;
   u32 reserved ;
   u64 addr[] ;
};
struct __anonstruct_init_txq_233 {
   u32 index ;
   u32 buf_count ;
   u32 evq ;
   u32 label ;
   u32 flags ;
   u32 reserved ;
   u64 addr[] ;
};
struct __anonstruct_mac_filter_234 {
   u32 rxq ;
   u32 flags ;
};
struct __anonstruct_set_status_page_235 {
   u64 dma_addr ;
   u64 peer_page_count ;
   u64 peer_page_addr[] ;
};
union __anonunion_u_230 {
   struct __anonstruct_init_evq_231 init_evq ;
   struct __anonstruct_init_rxq_232 init_rxq ;
   struct __anonstruct_init_txq_233 init_txq ;
   struct __anonstruct_mac_filter_234 mac_filter ;
   struct __anonstruct_set_status_page_235 set_status_page ;
};
struct vfdi_req {
   u32 op ;
   u32 reserved1 ;
   s32 rc ;
   u32 reserved2 ;
   union __anonunion_u_230 u ;
};
struct vfdi_status {
   u32 generation_start ;
   u32 generation_end ;
   u32 version ;
   u32 length ;
   u8 vi_scale ;
   u8 max_tx_channels ;
   u8 rss_rxq_count ;
   u8 reserved1 ;
   u16 peer_count ;
   u16 reserved2 ;
   struct vfdi_endpoint local ;
   struct vfdi_endpoint peers[256U] ;
   u32 timer_quantum_ns ;
};
enum efx_vf_tx_filter_mode {
    VF_TX_FILTER_OFF = 0,
    VF_TX_FILTER_AUTO = 1,
    VF_TX_FILTER_ON = 2
} ;
struct efx_vf {
   struct efx_nic *efx ;
   unsigned int pci_rid ;
   char pci_name[13U] ;
   unsigned int index ;
   struct work_struct req ;
   u64 req_addr ;
   int req_type ;
   unsigned int req_seqno ;
   unsigned int msg_seqno ;
   bool busy ;
   struct efx_buffer buf ;
   unsigned int buftbl_base ;
   bool rx_filtering ;
   enum efx_filter_flags rx_filter_flags ;
   unsigned int rx_filter_qid ;
   int rx_filter_id ;
   enum efx_vf_tx_filter_mode tx_filter_mode ;
   int tx_filter_id ;
   struct vfdi_endpoint addr ;
   u64 status_addr ;
   struct mutex status_lock ;
   u64 *peer_page_addrs ;
   unsigned int peer_page_count ;
   u64 evq0_addrs[16U] ;
   unsigned int evq0_count ;
   wait_queue_head_t flush_waitq ;
   struct mutex txq_lock ;
   unsigned long txq_mask[1U] ;
   unsigned int txq_count ;
   unsigned long rxq_mask[1U] ;
   unsigned int rxq_count ;
   unsigned long rxq_retry_mask[1U] ;
   atomic_t rxq_retry_count ;
   struct work_struct reset_work ;
};
struct efx_memcpy_req {
   unsigned int from_rid ;
   void *from_buf ;
   u64 from_addr ;
   unsigned int to_rid ;
   u64 to_addr ;
   unsigned int length ;
};
struct efx_local_addr {
   struct list_head link ;
   u8 addr[6U] ;
};
struct efx_endpoint_page {
   struct list_head link ;
   void *ptr ;
   dma_addr_t addr ;
};
typedef int (*efx_vfdi_op_t)(struct efx_vf * );
long ldv__builtin_expect(long exp , long c ) ;
__inline static void set_bit(unsigned int nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; bts %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr): "memory");
  return;
}
}
__inline static void __set_bit(int nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile ("bts %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr): "memory");
  return;
}
}
__inline static void clear_bit(int nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; btr %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr));
  return;
}
}
__inline static int test_and_set_bit(int nr , unsigned long volatile *addr )
{
  int oldbit ;
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; bts %2,%1\n\tsbb %0,%0": "=r" (oldbit),
                       "+m" (*((long volatile *)addr)): "Ir" (nr): "memory");
  return (oldbit);
}
}
__inline static int test_and_clear_bit(int nr , unsigned long volatile *addr )
{
  int oldbit ;
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; btr %2,%1\n\tsbb %0,%0": "=r" (oldbit),
                       "+m" (*((long volatile *)addr)): "Ir" (nr): "memory");
  return (oldbit);
}
}
__inline static int constant_test_bit(unsigned int nr , unsigned long const volatile *addr )
{
  {
  return ((int )((unsigned long )*(addr + (unsigned long )(nr / 64U)) >> ((int )nr & 63)) & 1);
}
}
__inline static int variable_test_bit(int nr , unsigned long const volatile *addr )
{
  int oldbit ;
  {
  __asm__ volatile ("bt %2,%1\n\tsbb %0,%0": "=r" (oldbit): "m" (*((unsigned long *)addr)),
                       "Ir" (nr));
  return (oldbit);
}
}
__inline static int fls(int x )
{
  int r ;
  {
  __asm__ ("bsrl %1,%0": "=r" (r): "rm" (x), "0" (-1));
  return (r + 1);
}
}
__inline static int fls64(__u64 x )
{
  int bitpos ;
  {
  bitpos = -1;
  __asm__ ("bsrq %1,%q0": "+r" (bitpos): "rm" (x));
  return (bitpos + 1);
}
}
extern unsigned long find_next_bit(unsigned long const * , unsigned long , unsigned long ) ;
__inline static void __set_bit_le(int nr , void *addr )
{
  {
  __set_bit(nr, (unsigned long volatile *)addr);
  return;
}
}
__inline static unsigned int fls_long(unsigned long l )
{
  int tmp___0 ;
  {
  tmp___0 = fls64((__u64 )l);
  return ((unsigned int )tmp___0);
}
}
__inline static unsigned long __roundup_pow_of_two(unsigned long n )
{
  unsigned int tmp ;
  {
  tmp = fls_long(n - 1UL);
  return (1UL << (int )tmp);
}
}
extern int printk(char const * , ...) ;
extern void dump_stack(void) ;
extern int __dynamic_netdev_dbg(struct _ddebug * , struct net_device const * , char const *
                                , ...) ;
extern int sprintf(char * , char const * , ...) ;
extern int snprintf(char * , size_t , char const * , ...) ;
__inline static void INIT_LIST_HEAD(struct list_head *list )
{
  {
  list->next = list;
  list->prev = list;
  return;
}
}
extern void __bad_percpu_size(void) ;
extern unsigned long __per_cpu_offset[4096U] ;
__inline static int __get_order(unsigned long size )
{
  int order ;
  {
  size = size - 1UL;
  size = size >> 12;
  order = fls64((__u64 )size);
  return (order);
}
}
extern void *memcpy(void * , void const * , size_t ) ;
extern void *memset(void * , int , size_t ) ;
extern char *strcpy(char * , char const * ) ;
extern size_t strlcpy(char * , char const * , size_t ) ;
extern void __bitmap_or(unsigned long * , unsigned long const * , unsigned long const * ,
                        int ) ;
__inline static void bitmap_or(unsigned long *dst , unsigned long const *src1 ,
                               unsigned long const *src2 , int nbits )
{
  {
  __bitmap_or(dst, src1, src2, nbits);
  return;
}
}
extern void warn_slowpath_null(char const * , int const ) ;
extern int nr_cpu_ids ;
extern struct cpumask const * const cpu_online_mask ;
__inline static unsigned int cpumask_check(unsigned int cpu )
{
  bool __warned ;
  int __ret_warn_once ;
  int __ret_warn_on ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  __ret_warn_once = (unsigned int )nr_cpu_ids <= cpu;
  tmp___1 = ldv__builtin_expect(__ret_warn_once != 0, 0L);
  if (tmp___1 != 0L) {
    __ret_warn_on = ! __warned;
    tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp != 0L) {
      warn_slowpath_null("include/linux/cpumask.h", 108);
    } else {
    }
    tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___0 != 0L) {
      __warned = 1;
    } else {
    }
  } else {
  }
  ldv__builtin_expect(__ret_warn_once != 0, 0L);
  return (cpu);
}
}
__inline static unsigned int cpumask_next(int n , struct cpumask const *srcp )
{
  unsigned long tmp ;
  {
  if (n != -1) {
    cpumask_check((unsigned int )n);
  } else {
  }
  tmp = find_next_bit((unsigned long const *)(& srcp->bits), (unsigned long )nr_cpu_ids,
                      (unsigned long )(n + 1));
  return ((unsigned int )tmp);
}
}
__inline static void cpumask_or(struct cpumask *dstp , struct cpumask const *src1p ,
                                struct cpumask const *src2p )
{
  {
  bitmap_or((unsigned long *)(& dstp->bits), (unsigned long const *)(& src1p->bits),
            (unsigned long const *)(& src2p->bits), nr_cpu_ids);
  return;
}
}
extern bool zalloc_cpumask_var(cpumask_var_t ** , gfp_t ) ;
extern void free_cpumask_var(cpumask_var_t ) ;
__inline static int atomic_read(atomic_t const *v )
{
  {
  return ((int )*((int volatile *)(& v->counter)));
}
}
extern void lockdep_init_map(struct lockdep_map * , char const * , struct lock_class_key * ,
                             int ) ;
extern void __mutex_init(struct mutex * , char const * , struct lock_class_key * ) ;
__inline static int mutex_is_locked(struct mutex *lock )
{
  int tmp ;
  {
  tmp = atomic_read((atomic_t const *)(& lock->count));
  return (tmp != 1);
}
}
__inline static int ldv_mutex_is_locked_8(struct mutex *lock ) ;
extern int mutex_trylock(struct mutex * ) ;
int ldv_mutex_trylock_4(struct mutex *ldv_func_arg1 ) ;
int ldv_mutex_trylock_22(struct mutex *ldv_func_arg1 ) ;
extern void mutex_unlock(struct mutex * ) ;
void ldv_mutex_unlock_2(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_5(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_7(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_10(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_12(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_14(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_15(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_17(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_19(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_21(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_23(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_25(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_27(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_29(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_30(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_32(struct mutex *ldv_func_arg1 ) ;
extern void mutex_lock(struct mutex * ) ;
void ldv_mutex_lock_1(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_3(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_6(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_9(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_11(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_13(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_16(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_18(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_20(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_24(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_26(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_28(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_31(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_cred_guard_mutex(struct mutex *lock ) ;
void ldv_mutex_unlock_cred_guard_mutex(struct mutex *lock ) ;
void ldv_mutex_lock_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_lock(struct mutex *lock ) ;
void ldv_mutex_lock_mac_lock(struct mutex *lock ) ;
int ldv_mutex_trylock_mac_lock(struct mutex *lock ) ;
int ldv_mutex_is_locked_mac_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_mac_lock(struct mutex *lock ) ;
void ldv_mutex_lock_mutex(struct mutex *lock ) ;
int ldv_mutex_trylock_mutex(struct mutex *lock ) ;
void ldv_mutex_unlock_mutex(struct mutex *lock ) ;
extern void local_bh_disable(void) ;
extern void local_bh_enable(void) ;
extern void __raw_spin_lock_init(raw_spinlock_t * , char const * , struct lock_class_key * ) ;
extern void _raw_spin_lock(raw_spinlock_t * ) ;
extern void _raw_spin_lock_bh(raw_spinlock_t * ) ;
extern void _raw_spin_unlock(raw_spinlock_t * ) ;
extern void _raw_spin_unlock_bh(raw_spinlock_t * ) ;
__inline static raw_spinlock_t *spinlock_check(spinlock_t *lock )
{
  {
  return (& lock->ldv_5961.rlock);
}
}
__inline static void spin_lock(spinlock_t *lock )
{
  {
  _raw_spin_lock(& lock->ldv_5961.rlock);
  return;
}
}
__inline static void spin_lock_bh(spinlock_t *lock )
{
  {
  _raw_spin_lock_bh(& lock->ldv_5961.rlock);
  return;
}
}
__inline static void spin_unlock(spinlock_t *lock )
{
  {
  _raw_spin_unlock(& lock->ldv_5961.rlock);
  return;
}
}
__inline static void spin_unlock_bh(spinlock_t *lock )
{
  {
  _raw_spin_unlock_bh(& lock->ldv_5961.rlock);
  return;
}
}
extern void __init_waitqueue_head(wait_queue_head_t * , char const * , struct lock_class_key * ) ;
extern unsigned long volatile jiffies ;
extern unsigned long msecs_to_jiffies(unsigned int const ) ;
extern void init_timer_key(struct timer_list * , unsigned int , char const * ,
                           struct lock_class_key * ) ;
extern int mod_timer(struct timer_list * , unsigned long ) ;
extern void delayed_work_timer_fn(unsigned long ) ;
extern void __init_work(struct work_struct * , int ) ;
extern struct workqueue_struct *__alloc_workqueue_key(char const * , unsigned int ,
                                                      int , struct lock_class_key * ,
                                                      char const * , ...) ;
extern void destroy_workqueue(struct workqueue_struct * ) ;
extern bool queue_work(struct workqueue_struct * , struct work_struct * ) ;
extern bool queue_delayed_work(struct workqueue_struct * , struct delayed_work * ,
                               unsigned long ) ;
extern bool cancel_work_sync(struct work_struct * ) ;
extern bool cancel_delayed_work_sync(struct delayed_work * ) ;
extern void *ioremap_nocache(resource_size_t , unsigned long ) ;
extern void iounmap(void volatile * ) ;
extern cpumask_var_t cpu_sibling_map ;
extern int cpu_number ;
extern void __bad_size_call_parameter(void) ;
__inline static char const *kobject_name(struct kobject const *kobj )
{
  {
  return ((char const *)kobj->name);
}
}
extern struct module __this_module ;
extern int device_create_file(struct device * , struct device_attribute const * ) ;
extern void device_remove_file(struct device * , struct device_attribute const * ) ;
__inline static char const *dev_name(struct device const *dev )
{
  char const *tmp ;
  {
  if ((unsigned long )dev->init_name != (unsigned long )((char const * )0)) {
    return ((char const *)dev->init_name);
  } else {
  }
  tmp = kobject_name(& dev->kobj);
  return (tmp);
}
}
extern void *dev_get_drvdata(struct device const * ) ;
extern int dev_set_drvdata(struct device * , void * ) ;
extern int pci_enable_device(struct pci_dev * ) ;
extern void pci_disable_device(struct pci_dev * ) ;
extern void pci_set_master(struct pci_dev * ) ;
extern int pci_save_state(struct pci_dev * ) ;
extern void pci_restore_state(struct pci_dev * ) ;
extern int pci_set_power_state(struct pci_dev * , pci_power_t ) ;
extern ssize_t pci_read_vpd(struct pci_dev * , loff_t , size_t , void * ) ;
extern int pci_request_region(struct pci_dev * , int , char const * ) ;
extern void pci_release_region(struct pci_dev * , int ) ;
extern int __pci_register_driver(struct pci_driver * , struct module * , char const * ) ;
extern void pci_unregister_driver(struct pci_driver * ) ;
extern int pci_enable_msi_block(struct pci_dev * , unsigned int ) ;
extern void pci_disable_msi(struct pci_dev * ) ;
extern int pci_enable_msix(struct pci_dev * , struct msix_entry * , int ) ;
extern void pci_disable_msix(struct pci_dev * ) ;
extern void kfree(void const * ) ;
extern int __VERIFIER_nondet_int(void);
extern void __VERIFIER_assume(int);
extern void *malloc(size_t size);
long ldv_is_err(const void *ptr)
{
  return ((unsigned long)ptr > ((unsigned long)-4095));
}
void *ldv_malloc(size_t size)
{
 if (__VERIFIER_nondet_int()) {
  void *res = malloc(size);
  __VERIFIER_assume(!ldv_is_err(res));
  return res;
 } else {
  return ((void *)0);
 }
}
void *__kmalloc(size_t size, gfp_t t)
{
 return ldv_malloc(size);
}
__inline static void *kmalloc(size_t size , gfp_t flags )
{
  void *tmp___2 ;
  {
  tmp___2 = __kmalloc(size, flags);
  return (tmp___2);
}
}
__inline static void *kzalloc(size_t size , gfp_t flags )
{
  void *tmp ;
  {
  tmp = kmalloc(size, flags | 32768U);
  return (tmp);
}
}
extern int dma_supported(struct device * , u64 ) ;
extern int dma_set_mask(struct device * , u64 ) ;
__inline static int dma_set_coherent_mask(struct device *dev , u64 mask )
{
  int tmp ;
  {
  tmp = dma_supported(dev, mask);
  if (tmp == 0) {
    return (-5);
  } else {
  }
  dev->coherent_dma_mask = mask;
  return (0);
}
}
__inline static void *pci_get_drvdata(struct pci_dev *pdev )
{
  void *tmp ;
  {
  tmp = dev_get_drvdata((struct device const *)(& pdev->dev));
  return (tmp);
}
}
__inline static void pci_set_drvdata(struct pci_dev *pdev , void *data )
{
  {
  dev_set_drvdata(& pdev->dev, data);
  return;
}
}
__inline static char const *pci_name(struct pci_dev const *pdev )
{
  char const *tmp ;
  {
  tmp = dev_name(& pdev->dev);
  return (tmp);
}
}
__inline static u16 pci_vpd_lrdt_size(u8 const *lrdt )
{
  {
  return ((int )((u16 )*(lrdt + 1UL)) + ((int )((u16 )*(lrdt + 2UL)) << 8U));
}
}
__inline static u8 pci_vpd_info_field_size(u8 const *info_field )
{
  {
  return ((u8 )*(info_field + 2UL));
}
}
extern int pci_vpd_find_tag(u8 const * , unsigned int , unsigned int , u8 ) ;
extern int pci_vpd_find_info_keyword(u8 const * , unsigned int , unsigned int ,
                                     char const * ) ;
extern void msleep(unsigned int ) ;
extern void get_random_bytes(void * , int ) ;
__inline static u32 ethtool_rxfh_indir_default(u32 index , u32 n_rx_rings )
{
  {
  return (index % n_rx_rings);
}
}
extern void synchronize_irq(unsigned int ) ;
extern void __napi_schedule(struct napi_struct * ) ;
__inline static bool napi_disable_pending(struct napi_struct *n )
{
  int tmp ;
  {
  tmp = constant_test_bit(1U, (unsigned long const volatile *)(& n->state));
  return (tmp != 0);
}
}
__inline static bool napi_schedule_prep(struct napi_struct *n )
{
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = napi_disable_pending(n);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    tmp___1 = test_and_set_bit(0, (unsigned long volatile *)(& n->state));
    if (tmp___1 == 0) {
      tmp___2 = 1;
    } else {
      tmp___2 = 0;
    }
  } else {
    tmp___2 = 0;
  }
  return ((bool )tmp___2);
}
}
__inline static void napi_schedule(struct napi_struct *n )
{
  bool tmp ;
  {
  tmp = napi_schedule_prep(n);
  if ((int )tmp) {
    __napi_schedule(n);
  } else {
  }
  return;
}
}
extern void napi_complete(struct napi_struct * ) ;
__inline static void napi_disable(struct napi_struct *n )
{
  int tmp ;
  {
  set_bit(1U, (unsigned long volatile *)(& n->state));
  goto ldv_38080;
  ldv_38079:
  msleep(1U);
  ldv_38080:
  tmp = test_and_set_bit(0, (unsigned long volatile *)(& n->state));
  if (tmp != 0) {
    goto ldv_38079;
  } else {
  }
  clear_bit(1, (unsigned long volatile *)(& n->state));
  return;
}
}
__inline static void napi_enable(struct napi_struct *n )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = constant_test_bit(0U, (unsigned long const volatile *)(& n->state));
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/netdevice.h"),
                         "i" (468), "i" (12UL));
    ldv_38085: ;
    goto ldv_38085;
  } else {
  }
  __asm__ volatile ("": : : "memory");
  clear_bit(0, (unsigned long volatile *)(& n->state));
  return;
}
}
__inline static struct netdev_queue *netdev_get_tx_queue(struct net_device const *dev ,
                                                         unsigned int index )
{
  {
  return ((struct netdev_queue *)dev->_tx + (unsigned long )index);
}
}
__inline static void *netdev_priv(struct net_device const *dev )
{
  {
  return ((void *)dev + 2816U);
}
}
extern void netif_napi_add(struct net_device * , struct napi_struct * , int (*)(struct napi_struct * ,
                                                                                int ) ,
                           int ) ;
extern void netif_napi_del(struct napi_struct * ) ;
extern int register_netdevice_notifier(struct notifier_block * ) ;
extern int unregister_netdevice_notifier(struct notifier_block * ) ;
extern int dev_alloc_name(struct net_device * , char const * ) ;
extern int dev_close(struct net_device * ) ;
extern int register_netdevice(struct net_device * ) ;
extern void unregister_netdevice_queue(struct net_device * , struct list_head * ) ;
__inline static void unregister_netdevice(struct net_device *dev )
{
  {
  unregister_netdevice_queue(dev, 0);
  return;
}
}
extern void free_netdev(struct net_device * ) ;
extern int netpoll_trap(void) ;
extern void __netif_schedule(struct Qdisc * ) ;
__inline static void netif_schedule_queue(struct netdev_queue *txq )
{
  {
  if ((txq->state & 3UL) == 0UL) {
    __netif_schedule(txq->qdisc);
  } else {
  }
  return;
}
}
__inline static void netif_tx_start_queue(struct netdev_queue *dev_queue )
{
  {
  clear_bit(0, (unsigned long volatile *)(& dev_queue->state));
  return;
}
}
__inline static void netif_tx_wake_queue(struct netdev_queue *dev_queue )
{
  int tmp ;
  int tmp___0 ;
  {
  tmp = netpoll_trap();
  if (tmp != 0) {
    netif_tx_start_queue(dev_queue);
    return;
  } else {
  }
  tmp___0 = test_and_clear_bit(0, (unsigned long volatile *)(& dev_queue->state));
  if (tmp___0 != 0) {
    __netif_schedule(dev_queue->qdisc);
  } else {
  }
  return;
}
}
__inline static void netif_tx_wake_all_queues(struct net_device *dev )
{
  unsigned int i ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  i = 0U;
  goto ldv_38863;
  ldv_38862:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  netif_tx_wake_queue(txq);
  i = i + 1U;
  ldv_38863: ;
  if (dev->num_tx_queues > i) {
    goto ldv_38862;
  } else {
  }
  return;
}
}
__inline static void netif_tx_stop_queue(struct netdev_queue *dev_queue )
{
  int __ret_warn_on ;
  long tmp ;
  long tmp___0 ;
  {
  __ret_warn_on = (unsigned long )dev_queue == (unsigned long )((struct netdev_queue *)0);
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("include/linux/netdevice.h", 1880);
  } else {
  }
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    printk("\016netif_stop_queue() cannot be called before register_netdev()\n");
    return;
  } else {
  }
  set_bit(0U, (unsigned long volatile *)(& dev_queue->state));
  return;
}
}
__inline static bool netif_running(struct net_device const *dev )
{
  int tmp ;
  {
  tmp = constant_test_bit(0U, (unsigned long const volatile *)(& dev->state));
  return (tmp != 0);
}
}
extern int netif_set_real_num_tx_queues(struct net_device * , unsigned int ) ;
extern int netif_set_real_num_rx_queues(struct net_device * , unsigned int ) ;
__inline static bool netif_carrier_ok(struct net_device const *dev )
{
  int tmp ;
  {
  tmp = constant_test_bit(2U, (unsigned long const volatile *)(& dev->state));
  return (tmp == 0);
}
}
extern void netif_carrier_on(struct net_device * ) ;
extern void netif_carrier_off(struct net_device * ) ;
__inline static bool netif_device_present(struct net_device *dev )
{
  int tmp ;
  {
  tmp = constant_test_bit(1U, (unsigned long const volatile *)(& dev->state));
  return (tmp != 0);
}
}
extern void netif_device_detach(struct net_device * ) ;
extern void netif_device_attach(struct net_device * ) ;
__inline static void __netif_tx_lock(struct netdev_queue *txq , int cpu )
{
  {
  spin_lock(& txq->_xmit_lock);
  txq->xmit_lock_owner = cpu;
  return;
}
}
__inline static void __netif_tx_unlock(struct netdev_queue *txq )
{
  {
  txq->xmit_lock_owner = -1;
  spin_unlock(& txq->_xmit_lock);
  return;
}
}
__inline static void netif_tx_lock(struct net_device *dev )
{
  unsigned int i ;
  int cpu ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  spin_lock(& dev->tx_global_lock);
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_39303;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39303;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39303;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39303;
  default:
  __bad_percpu_size();
  }
  ldv_39303:
  pscr_ret__ = pfo_ret__;
  goto ldv_39309;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39313;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39313;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39313;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39313;
  default:
  __bad_percpu_size();
  }
  ldv_39313:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_39309;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39322;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39322;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39322;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39322;
  default:
  __bad_percpu_size();
  }
  ldv_39322:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_39309;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39331;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39331;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39331;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39331;
  default:
  __bad_percpu_size();
  }
  ldv_39331:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_39309;
  default:
  __bad_size_call_parameter();
  goto ldv_39309;
  }
  ldv_39309:
  cpu = pscr_ret__;
  i = 0U;
  goto ldv_39341;
  ldv_39340:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  __netif_tx_lock(txq, cpu);
  set_bit(2U, (unsigned long volatile *)(& txq->state));
  __netif_tx_unlock(txq);
  i = i + 1U;
  ldv_39341: ;
  if (dev->num_tx_queues > i) {
    goto ldv_39340;
  } else {
  }
  return;
}
}
__inline static void netif_tx_unlock(struct net_device *dev )
{
  unsigned int i ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  i = 0U;
  goto ldv_39352;
  ldv_39351:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  clear_bit(2, (unsigned long volatile *)(& txq->state));
  netif_schedule_queue(txq);
  i = i + 1U;
  ldv_39352: ;
  if (dev->num_tx_queues > i) {
    goto ldv_39351;
  } else {
  }
  spin_unlock(& dev->tx_global_lock);
  return;
}
}
__inline static void netif_tx_disable(struct net_device *dev )
{
  unsigned int i ;
  int cpu ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  local_bh_disable();
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_39367;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39367;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39367;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39367;
  default:
  __bad_percpu_size();
  }
  ldv_39367:
  pscr_ret__ = pfo_ret__;
  goto ldv_39373;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39377;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39377;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39377;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39377;
  default:
  __bad_percpu_size();
  }
  ldv_39377:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_39373;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39386;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39386;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39386;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39386;
  default:
  __bad_percpu_size();
  }
  ldv_39386:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_39373;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39395;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39395;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39395;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39395;
  default:
  __bad_percpu_size();
  }
  ldv_39395:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_39373;
  default:
  __bad_size_call_parameter();
  goto ldv_39373;
  }
  ldv_39373:
  cpu = pscr_ret__;
  i = 0U;
  goto ldv_39405;
  ldv_39404:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  __netif_tx_lock(txq, cpu);
  netif_tx_stop_queue(txq);
  __netif_tx_unlock(txq);
  i = i + 1U;
  ldv_39405: ;
  if (dev->num_tx_queues > i) {
    goto ldv_39404;
  } else {
  }
  local_bh_enable();
  return;
}
}
__inline static void netif_addr_lock_bh(struct net_device *dev )
{
  {
  spin_lock_bh(& dev->addr_list_lock);
  return;
}
}
__inline static void netif_addr_unlock_bh(struct net_device *dev )
{
  {
  spin_unlock_bh(& dev->addr_list_lock);
  return;
}
}
extern int netdev_printk(char const * , struct net_device const * , char const *
                         , ...) ;
extern int netdev_err(struct net_device const * , char const * , ...) ;
extern int netdev_warn(struct net_device const * , char const * , ...) ;
extern int netdev_info(struct net_device const * , char const * , ...) ;
extern int eth_validate_addr(struct net_device * ) ;
extern struct net_device *alloc_etherdev_mqs(int , unsigned int , unsigned int ) ;
__inline static bool is_zero_ether_addr(u8 const *addr )
{
  {
  return ((unsigned int )((((((int )((unsigned char )*addr) | (int )((unsigned char )*(addr + 1UL))) | (int )((unsigned char )*(addr + 2UL))) | (int )((unsigned char )*(addr + 3UL))) | (int )((unsigned char )*(addr + 4UL))) | (int )((unsigned char )*(addr + 5UL))) == 0U);
}
}
__inline static bool is_multicast_ether_addr(u8 const *addr )
{
  {
  return (((int )*addr & 1) != 0);
}
}
__inline static bool is_valid_ether_addr(u8 const *addr )
{
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp = is_multicast_ether_addr(addr);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    tmp___1 = is_zero_ether_addr(addr);
    if (tmp___1) {
      tmp___2 = 0;
    } else {
      tmp___2 = 1;
    }
    if (tmp___2) {
      tmp___3 = 1;
    } else {
      tmp___3 = 0;
    }
  } else {
    tmp___3 = 0;
  }
  return ((bool )tmp___3);
}
}
extern void rtnl_lock(void) ;
extern void rtnl_unlock(void) ;
extern int rtnl_is_locked(void) ;
extern u32 crc32_le(u32 , unsigned char const * , size_t ) ;
extern struct cpu_rmap *alloc_cpu_rmap(unsigned int , gfp_t ) ;
__inline static struct cpu_rmap *alloc_irq_cpu_rmap(unsigned int size )
{
  struct cpu_rmap *tmp ;
  {
  tmp = alloc_cpu_rmap(size, 208U);
  return (tmp);
}
}
extern void free_irq_cpu_rmap(struct cpu_rmap * ) ;
extern int irq_cpu_rmap_add(struct cpu_rmap * , int ) ;
__inline static struct mii_ioctl_data *if_mii(struct ifreq *rq )
{
  {
  return ((struct mii_ioctl_data *)(& rq->ifr_ifru));
}
}
extern int mdio_mii_ioctl(struct mdio_if_info const * , struct mii_ioctl_data * ,
                          int ) ;
char const * const efx_loopback_mode_names[27U] ;
unsigned int const efx_loopback_mode_max ;
char const * const efx_reset_type_names[12U] ;
unsigned int const efx_reset_type_max ;
__inline static struct efx_channel *efx_get_channel(struct efx_nic *efx , unsigned int index )
{
  {
  return (efx->channel[index]);
}
}
__inline static bool efx_channel_has_tx_queues(struct efx_channel *channel )
{
  {
  return ((unsigned int )channel->channel - (channel->efx)->tx_channel_offset < (channel->efx)->n_tx_channels);
}
}
__inline static bool efx_tx_queue_used(struct efx_tx_queue *tx_queue )
{
  {
  return ((bool )((unsigned int )((tx_queue->efx)->net_dev)->num_tc > 1U || (tx_queue->queue & 2U) == 0U));
}
}
__inline static bool efx_channel_has_rx_queue(struct efx_channel *channel )
{
  {
  return (channel->rx_queue.core_index >= 0);
}
}
__inline static struct efx_rx_queue *efx_channel_get_rx_queue(struct efx_channel *channel )
{
  {
  return (& channel->rx_queue);
}
}
int efx_probe_tx_queue(struct efx_tx_queue *tx_queue ) ;
void efx_remove_tx_queue(struct efx_tx_queue *tx_queue ) ;
void efx_init_tx_queue(struct efx_tx_queue *tx_queue ) ;
void efx_init_tx_queue_core_txq(struct efx_tx_queue *tx_queue ) ;
void efx_fini_tx_queue(struct efx_tx_queue *tx_queue ) ;
void efx_release_tx_buffers(struct efx_tx_queue *tx_queue ) ;
netdev_tx_t efx_hard_start_xmit(struct sk_buff *skb , struct net_device *net_dev ) ;
int efx_setup_tc(struct net_device *net_dev , u8 num_tc ) ;
unsigned int efx_tx_max_skb_descs(struct efx_nic *efx ) ;
int efx_probe_rx_queue(struct efx_rx_queue *rx_queue ) ;
void efx_remove_rx_queue(struct efx_rx_queue *rx_queue ) ;
void efx_init_rx_queue(struct efx_rx_queue *rx_queue ) ;
void efx_fini_rx_queue(struct efx_rx_queue *rx_queue ) ;
void efx_rx_strategy(struct efx_channel *channel ) ;
void efx_fast_push_rx_descriptors(struct efx_rx_queue *rx_queue ) ;
void efx_rx_slow_fill(unsigned long context ) ;
void __efx_rx_packet(struct efx_channel *channel , struct efx_rx_buffer *rx_buf ) ;
void efx_schedule_slow_fill(struct efx_rx_queue *rx_queue ) ;
int efx_probe_filters(struct efx_nic *efx ) ;
void efx_restore_filters(struct efx_nic *efx ) ;
void efx_remove_filters(struct efx_nic *efx ) ;
void efx_filter_clear_rx(struct efx_nic *efx , enum efx_filter_priority priority ) ;
int efx_filter_rfs(struct net_device *net_dev , struct sk_buff const *skb , u16 rxq_index ,
                   u32 flow_id ) ;
bool __efx_filter_rfs_expire(struct efx_nic *efx , unsigned int quota ) ;
__inline static void efx_filter_rfs_expire(struct efx_channel *channel )
{
  bool tmp ;
  {
  if (channel->rfs_filters_added > 59U) {
    tmp = __efx_filter_rfs_expire(channel->efx, 100U);
    if ((int )tmp) {
      channel->rfs_filters_added = channel->rfs_filters_added - 60U;
    } else {
    }
  } else {
  }
  return;
}
}
int efx_channel_dummy_op_int(struct efx_channel *channel ) ;
void efx_channel_dummy_op_void(struct efx_channel *channel ) ;
void efx_process_channel_now(struct efx_channel *channel ) ;
int efx_realloc_channels(struct efx_nic *efx , u32 rxq_entries , u32 txq_entries ) ;
int efx_reconfigure_port(struct efx_nic *efx ) ;
int __efx_reconfigure_port(struct efx_nic *efx ) ;
struct ethtool_ops const efx_ethtool_ops ;
int efx_reset(struct efx_nic *efx , enum reset_type method ) ;
void efx_reset_down(struct efx_nic *efx , enum reset_type method ) ;
int efx_reset_up(struct efx_nic *efx , enum reset_type method , bool ok ) ;
void efx_schedule_reset(struct efx_nic *efx , enum reset_type type ) ;
int efx_init_irq_moderation(struct efx_nic *efx , unsigned int tx_usecs , unsigned int rx_usecs ,
                            bool rx_adaptive , bool rx_may_override_tx ) ;
void efx_get_irq_moderation(struct efx_nic *efx , unsigned int *tx_usecs , unsigned int *rx_usecs ,
                            bool *rx_adaptive ) ;
int efx_port_dummy_op_int(struct efx_nic *efx ) ;
void efx_port_dummy_op_void(struct efx_nic *efx ) ;
int efx_mtd_probe(struct efx_nic *efx ) ;
void efx_mtd_rename(struct efx_nic *efx ) ;
void efx_mtd_remove(struct efx_nic *efx ) ;
__inline static void efx_schedule_channel(struct efx_channel *channel )
{
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  {
  if (0) {
    if (((channel->efx)->msg_enable & 512U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_45807;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_45807;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_45807;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_45807;
      default:
      __bad_percpu_size();
      }
      ldv_45807:
      pscr_ret__ = pfo_ret__;
      goto ldv_45813;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_45817;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_45817;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_45817;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_45817;
      default:
      __bad_percpu_size();
      }
      ldv_45817:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_45813;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_45826;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_45826;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_45826;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_45826;
      default:
      __bad_percpu_size();
      }
      ldv_45826:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_45813;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_45835;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_45835;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_45835;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_45835;
      default:
      __bad_percpu_size();
      }
      ldv_45835:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_45813;
      default:
      __bad_size_call_parameter();
      goto ldv_45813;
      }
      ldv_45813:
      netdev_printk("\017", (struct net_device const *)(channel->efx)->net_dev,
                    "channel %d scheduling NAPI poll on CPU%d\n", channel->channel,
                    pscr_ret__);
    } else {
    }
  } else {
  }
  channel->work_pending = 1;
  napi_schedule(& channel->napi_str);
  return;
}
}
void efx_link_status_changed(struct efx_nic *efx ) ;
void efx_link_set_advertising(struct efx_nic *efx , u32 advertising ) ;
void efx_link_set_wanted_fc(struct efx_nic *efx , u8 wanted_fc ) ;
__inline static void efx_device_detach_sync(struct efx_nic *efx )
{
  struct net_device *dev ;
  {
  dev = efx->net_dev;
  netif_tx_lock(dev);
  netif_device_detach(dev);
  netif_tx_unlock(dev);
  return;
}
}
int efx_mcdi_poll_reboot(struct efx_nic *efx ) ;
void efx_mcdi_mode_poll(struct efx_nic *efx ) ;
void efx_mcdi_mode_event(struct efx_nic *efx ) ;
__inline static int efx_nic_rev(struct efx_nic *efx )
{
  {
  return ((int )(efx->type)->revision);
}
}
__inline static bool efx_sriov_wanted(struct efx_nic *efx )
{
  {
  return (efx->vf_count != 0U);
}
}
__inline static unsigned int efx_vf_size(struct efx_nic *efx )
{
  {
  return ((unsigned int )(1 << (int )efx->vi_scale));
}
}
int efx_init_sriov(void) ;
int efx_sriov_init(struct efx_nic *efx ) ;
void efx_sriov_mac_address_changed(struct efx_nic *efx ) ;
void efx_sriov_reset(struct efx_nic *efx ) ;
void efx_sriov_fini(struct efx_nic *efx ) ;
void efx_fini_sriov(void) ;
int efx_sriov_set_vf_mac(struct net_device *net_dev , int vf_i , u8 *mac ) ;
int efx_sriov_set_vf_vlan(struct net_device *net_dev , int vf_i , u16 vlan , u8 qos ) ;
int efx_sriov_get_vf_config(struct net_device *net_dev , int vf_i , struct ifla_vf_info *ivi ) ;
int efx_sriov_set_vf_spoofchk(struct net_device *net_dev , int vf_i , bool spoofchk ) ;
int efx_ptp_ioctl(struct efx_nic *efx , struct ifreq *ifr , int cmd ) ;
struct efx_nic_type const falcon_a1_nic_type ;
struct efx_nic_type const falcon_b0_nic_type ;
struct efx_nic_type const siena_a0_nic_type ;
void efx_nic_generate_fill_event(struct efx_rx_queue *rx_queue ) ;
int efx_nic_probe_eventq(struct efx_channel *channel ) ;
void efx_nic_init_eventq(struct efx_channel *channel ) ;
void efx_nic_fini_eventq(struct efx_channel *channel ) ;
void efx_nic_remove_eventq(struct efx_channel *channel ) ;
int efx_nic_process_eventq(struct efx_channel *channel , int budget ) ;
void efx_nic_eventq_read_ack(struct efx_channel *channel ) ;
int efx_nic_init_interrupt(struct efx_nic *efx ) ;
void efx_nic_enable_interrupts(struct efx_nic *efx ) ;
void efx_nic_disable_interrupts(struct efx_nic *efx ) ;
void efx_nic_fini_interrupt(struct efx_nic *efx ) ;
int efx_nic_flush_queues(struct efx_nic *efx ) ;
void efx_selftest_async_start(struct efx_nic *efx ) ;
void efx_selftest_async_cancel(struct efx_nic *efx ) ;
void efx_selftest_async_work(struct work_struct *data ) ;
unsigned int const efx_loopback_mode_max = 27U;
char const * const efx_loopback_mode_names[27U] =
  { "NONE", "DATAPATH", "GMAC", "XGMII",
        "XGXS", "XAUI", "GMII", "SGMII",
        "XGBR", "XFI", "XAUI_FAR", "GMII_FAR",
        "SGMII_FAR", "XFI_FAR", "GPHY", "PHYXS",
        "PCS", "PMA/PMD", "XPORT", "XGMII_WS",
        "XAUI_WS", "XAUI_WS_FAR", "XAUI_WS_NEAR", "GMII_WS",
        "XFI_WS", "XFI_WS_FAR", "PHYXS_WS"};
unsigned int const efx_reset_type_max = 12U;
char const * const efx_reset_type_names[12U] =
  { "INVISIBLE", "ALL", "WORLD", "DISABLE",
        0, "TX_WATCHDOG", "INT_ERROR", "RX_RECOVERY",
        "RX_DESC_FETCH", "TX_DESC_FETCH", "TX_SKIP", "MC_FAILURE"};
static struct workqueue_struct *reset_workqueue ;
static bool separate_tx_channels ;
static int napi_weight = 64;
static unsigned int efx_monitor_interval = 250U;
static unsigned int rx_irq_mod_usec = 60U;
static unsigned int tx_irq_mod_usec = 150U;
static unsigned int interrupt_mode ;
static unsigned int rss_cpus ;
static bool phy_flash_cfg ;
static unsigned int irq_adapt_low_thresh = 8000U;
static unsigned int irq_adapt_high_thresh = 16000U;
static unsigned int debug = 8439U;
static void efx_start_interrupts(struct efx_nic *efx , bool may_keep_eventq ) ;
static void efx_stop_interrupts(struct efx_nic *efx , bool may_keep_eventq ) ;
static void efx_remove_channel(struct efx_channel *channel ) ;
static void efx_remove_channels(struct efx_nic *efx ) ;
static struct efx_channel_type const efx_default_channel_type ;
static void efx_remove_port(struct efx_nic *efx ) ;
static void efx_init_napi_channel(struct efx_channel *channel ) ;
static void efx_fini_napi(struct efx_nic *efx ) ;
static void efx_fini_napi_channel(struct efx_channel *channel ) ;
static void efx_fini_struct(struct efx_nic *efx ) ;
static void efx_start_all(struct efx_nic *efx ) ;
static void efx_stop_all(struct efx_nic *efx ) ;
static int efx_check_disabled(struct efx_nic *efx )
{
  {
  if ((unsigned int )efx->state == 2U) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "device is disabled due to earlier errors\n");
    } else {
    }
    return (-5);
  } else {
  }
  return (0);
}
}
static int efx_process_channel(struct efx_channel *channel , int budget )
{
  int spent ;
  long tmp ;
  struct efx_rx_queue *rx_queue ;
  struct efx_rx_queue *tmp___0 ;
  bool tmp___1 ;
  {
  tmp = ldv__builtin_expect((long )(! channel->enabled), 0L);
  if (tmp != 0L) {
    return (0);
  } else {
  }
  spent = efx_nic_process_eventq(channel, budget);
  if (spent != 0) {
    tmp___1 = efx_channel_has_rx_queue(channel);
    if ((int )tmp___1) {
      tmp___0 = efx_channel_get_rx_queue(channel);
      rx_queue = tmp___0;
      if ((unsigned long )channel->rx_pkt != (unsigned long )((struct efx_rx_buffer *)0)) {
        __efx_rx_packet(channel, channel->rx_pkt);
        channel->rx_pkt = 0;
      } else {
      }
      if ((int )rx_queue->enabled) {
        efx_rx_strategy(channel);
        efx_fast_push_rx_descriptors(rx_queue);
      } else {
      }
    } else {
    }
  } else {
  }
  return (spent);
}
}
__inline static void efx_channel_processed(struct efx_channel *channel )
{
  {
  channel->work_pending = 0;
  __asm__ volatile ("": : : "memory");
  efx_nic_eventq_read_ack(channel);
  return;
}
}
static int efx_poll(struct napi_struct *napi , int budget )
{
  struct efx_channel *channel ;
  struct napi_struct const *__mptr ;
  struct efx_nic *efx ;
  int spent ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  long tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  {
  __mptr = (struct napi_struct const *)napi;
  channel = (struct efx_channel *)__mptr + 0xffffffffffffffd0UL;
  efx = channel->efx;
  if (0) {
    if ((efx->msg_enable & 512U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_46576;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_46576;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_46576;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_46576;
      default:
      __bad_percpu_size();
      }
      ldv_46576:
      pscr_ret__ = pfo_ret__;
      goto ldv_46582;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_46586;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_46586;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_46586;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_46586;
      default:
      __bad_percpu_size();
      }
      ldv_46586:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_46582;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_46595;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_46595;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_46595;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_46595;
      default:
      __bad_percpu_size();
      }
      ldv_46595:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_46582;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_46604;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_46604;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_46604;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_46604;
      default:
      __bad_percpu_size();
      }
      ldv_46604:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_46582;
      default:
      __bad_size_call_parameter();
      goto ldv_46582;
      }
      ldv_46582:
      netdev_printk("\017", (struct net_device const *)efx->net_dev, "channel %d NAPI poll executing on CPU %d\n",
                    channel->channel, pscr_ret__);
    } else {
    }
  } else {
  }
  spent = efx_process_channel(channel, budget);
  if (spent < budget) {
    tmp___1 = efx_channel_has_rx_queue(channel);
    if ((int )tmp___1 && (int )efx->irq_rx_adaptive) {
      channel->irq_count = channel->irq_count + 1U;
      tmp___2 = ldv__builtin_expect(channel->irq_count == 1000U, 0L);
      if (tmp___2 != 0L) {
        tmp___0 = ldv__builtin_expect(channel->irq_mod_score < irq_adapt_low_thresh,
                                   0L);
        if (tmp___0 != 0L) {
          if (channel->irq_moderation > 1U) {
            channel->irq_moderation = channel->irq_moderation - 1U;
            (*((efx->type)->push_irq_moderation))(channel);
          } else {
            tmp = ldv__builtin_expect(channel->irq_mod_score > irq_adapt_high_thresh,
                                   0L);
            if (tmp != 0L) {
              if (channel->irq_moderation < efx->irq_rx_moderation) {
                channel->irq_moderation = channel->irq_moderation + 1U;
                (*((efx->type)->push_irq_moderation))(channel);
              } else {
              }
            } else {
            }
          }
        } else {
        }
        channel->irq_count = 0U;
        channel->irq_mod_score = 0U;
      } else {
      }
    } else {
    }
    efx_filter_rfs_expire(channel);
    napi_complete(napi);
    efx_channel_processed(channel);
  } else {
  }
  return (spent);
}
}
void efx_process_channel_now(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  long tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  efx = channel->efx;
  tmp = ldv__builtin_expect((unsigned int )channel->channel >= efx->n_channels, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (431), "i" (12UL));
    ldv_46617: ;
    goto ldv_46617;
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )(! channel->enabled), 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (432), "i" (12UL));
    ldv_46618: ;
    goto ldv_46618;
  } else {
  }
  tmp___1 = ldv__builtin_expect((unsigned long )efx->loopback_selftest == (unsigned long )((void *)0),
                             0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (433), "i" (12UL));
    ldv_46619: ;
    goto ldv_46619;
  } else {
  }
  efx_nic_disable_interrupts(efx);
  if (efx->legacy_irq != 0) {
    synchronize_irq((unsigned int )efx->legacy_irq);
    efx->legacy_irq_enabled = 0;
  } else {
  }
  if (channel->irq != 0) {
    synchronize_irq((unsigned int )channel->irq);
  } else {
  }
  napi_disable(& channel->napi_str);
  efx_process_channel(channel, (int )(channel->eventq_mask + 1U));
  efx_channel_processed(channel);
  napi_enable(& channel->napi_str);
  if (efx->legacy_irq != 0) {
    efx->legacy_irq_enabled = 1;
  } else {
  }
  efx_nic_enable_interrupts(efx);
  return;
}
}
static int efx_probe_eventq(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  unsigned long entries ;
  struct _ddebug descriptor ;
  long tmp ;
  unsigned long _max1 ;
  unsigned long _max2 ;
  int tmp___0 ;
  {
  efx = channel->efx;
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "chan %d create event queue\n";
    descriptor.lineno = 471U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "chan %d create event queue\n", channel->channel);
    } else {
    }
  } else {
  }
  entries = __roundup_pow_of_two((unsigned long )((efx->rxq_entries + efx->txq_entries) + 128U));
  _max1 = entries;
  _max2 = 512UL;
  channel->eventq_mask = (unsigned int )(_max1 > _max2 ? _max1 : _max2) - 1U;
  tmp___0 = efx_nic_probe_eventq(channel);
  return (tmp___0);
}
}
static void efx_init_eventq(struct efx_channel *channel )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )(channel->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "chan %d init event queue\n";
    descriptor.lineno = 486U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "chan %d init event queue\n", channel->channel);
    } else {
    }
  } else {
  }
  channel->eventq_read_ptr = 0U;
  efx_nic_init_eventq(channel);
  return;
}
}
static void efx_start_eventq(struct efx_channel *channel )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if (((channel->efx)->msg_enable & 32U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_start_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "chan %d start event queue\n";
    descriptor.lineno = 497U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "chan %d start event queue\n", channel->channel);
    } else {
    }
  } else {
  }
  channel->work_pending = 0;
  channel->enabled = 1;
  __asm__ volatile ("": : : "memory");
  napi_enable(& channel->napi_str);
  efx_nic_eventq_read_ack(channel);
  return;
}
}
static void efx_stop_eventq(struct efx_channel *channel )
{
  {
  if (! channel->enabled) {
    return;
  } else {
  }
  napi_disable(& channel->napi_str);
  channel->enabled = 0;
  return;
}
}
static void efx_fini_eventq(struct efx_channel *channel )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )(channel->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "chan %d fini event queue\n";
    descriptor.lineno = 524U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "chan %d fini event queue\n", channel->channel);
    } else {
    }
  } else {
  }
  efx_nic_fini_eventq(channel);
  return;
}
}
static void efx_remove_eventq(struct efx_channel *channel )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )(channel->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "chan %d remove event queue\n";
    descriptor.lineno = 532U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "chan %d remove event queue\n", channel->channel);
    } else {
    }
  } else {
  }
  efx_nic_remove_eventq(channel);
  return;
}
}
static struct efx_channel *efx_alloc_channel(struct efx_nic *efx , int i , struct efx_channel *old_channel )
{
  struct efx_channel *channel ;
  struct efx_rx_queue *rx_queue ;
  struct efx_tx_queue *tx_queue ;
  int j ;
  void *tmp ;
  struct lock_class_key __key ;
  {
  tmp = kzalloc(1856UL, 208U);
  channel = (struct efx_channel *)tmp;
  if ((unsigned long )channel == (unsigned long )((struct efx_channel *)0)) {
    return (0);
  } else {
  }
  channel->efx = efx;
  channel->channel = i;
  channel->type = & efx_default_channel_type;
  j = 0;
  goto ldv_46663;
  ldv_46662:
  tx_queue = (struct efx_tx_queue *)(& channel->tx_queue) + (unsigned long )j;
  tx_queue->efx = efx;
  tx_queue->queue = (unsigned int )(i * 4 + j);
  tx_queue->channel = channel;
  j = j + 1;
  ldv_46663: ;
  if (j <= 3) {
    goto ldv_46662;
  } else {
  }
  rx_queue = & channel->rx_queue;
  rx_queue->efx = efx;
  init_timer_key(& rx_queue->slow_fill, 0U, "((&rx_queue->slow_fill))", & __key);
  rx_queue->slow_fill.function = & efx_rx_slow_fill;
  rx_queue->slow_fill.data = (unsigned long )rx_queue;
  return (channel);
}
}
static struct efx_channel *efx_copy_channel(struct efx_channel const *old_channel )
{
  struct efx_channel *channel ;
  struct efx_rx_queue *rx_queue ;
  struct efx_tx_queue *tx_queue ;
  int j ;
  void *tmp ;
  struct lock_class_key __key ;
  {
  tmp = kmalloc(1856UL, 208U);
  channel = (struct efx_channel *)tmp;
  if ((unsigned long )channel == (unsigned long )((struct efx_channel *)0)) {
    return (0);
  } else {
  }
  *channel = *old_channel;
  channel->napi_dev = 0;
  memset((void *)(& channel->eventq), 0, 32UL);
  j = 0;
  goto ldv_46674;
  ldv_46673:
  tx_queue = (struct efx_tx_queue *)(& channel->tx_queue) + (unsigned long )j;
  if ((unsigned long )tx_queue->channel != (unsigned long )((struct efx_channel *)0)) {
    tx_queue->channel = channel;
  } else {
  }
  tx_queue->buffer = 0;
  memset((void *)(& tx_queue->txd), 0, 32UL);
  j = j + 1;
  ldv_46674: ;
  if (j <= 3) {
    goto ldv_46673;
  } else {
  }
  rx_queue = & channel->rx_queue;
  rx_queue->buffer = 0;
  memset((void *)(& rx_queue->rxd), 0, 32UL);
  init_timer_key(& rx_queue->slow_fill, 0U, "((&rx_queue->slow_fill))", & __key);
  rx_queue->slow_fill.function = & efx_rx_slow_fill;
  rx_queue->slow_fill.data = (unsigned long )rx_queue;
  return (channel);
}
}
static int efx_probe_channel(struct efx_channel *channel )
{
  struct efx_tx_queue *tx_queue ;
  struct efx_rx_queue *rx_queue ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  {
  if (((channel->efx)->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_channel";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "creating channel %d\n";
    descriptor.lineno = 619U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "creating channel %d\n", channel->channel);
    } else {
    }
  } else {
  }
  rc = (*((channel->type)->pre_probe))(channel);
  if (rc != 0) {
    goto fail;
  } else {
  }
  rc = efx_probe_eventq(channel);
  if (rc != 0) {
    goto fail;
  } else {
  }
  tmp___1 = efx_channel_has_tx_queues(channel);
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46687;
    ldv_46686:
    rc = efx_probe_tx_queue(tx_queue);
    if (rc != 0) {
      goto fail;
    } else {
    }
    tx_queue = tx_queue + 1;
    ldv_46687: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___0 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___0) {
        goto ldv_46686;
      } else {
        goto ldv_46688;
      }
    } else {
    }
    ldv_46688: ;
  }
  tmp___3 = efx_channel_has_rx_queue(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_46690;
    ldv_46689:
    rc = efx_probe_rx_queue(rx_queue);
    if (rc != 0) {
      goto fail;
    } else {
    }
    rx_queue = 0;
    ldv_46690: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_46689;
    } else {
    }
  }
  channel->n_rx_frm_trunc = 0U;
  return (0);
  fail:
  efx_remove_channel(channel);
  return (rc);
}
}
static void efx_get_channel_name(struct efx_channel *channel , char *buf , size_t len )
{
  struct efx_nic *efx ;
  char const *type ;
  int number ;
  {
  efx = channel->efx;
  number = channel->channel;
  if (efx->tx_channel_offset == 0U) {
    type = "";
  } else
  if ((unsigned int )channel->channel < efx->tx_channel_offset) {
    type = "-rx";
  } else {
    type = "-tx";
    number = (int )((unsigned int )number - efx->tx_channel_offset);
  }
  snprintf(buf, len, "%s%s-%d", (char *)(& efx->name), type, number);
  return;
}
}
static void efx_set_channel_names(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_46705;
  ldv_46704:
  (*((channel->type)->get_name))(channel, (char *)(& efx->channel_name) + (unsigned long )channel->channel,
                                 22UL);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46705: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46704;
  } else {
  }
  return;
}
}
static int efx_probe_channels(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  int rc ;
  {
  efx->next_buffer_table = 0U;
  channel = efx->channel[efx->n_channels - 1U];
  goto ldv_46714;
  ldv_46713:
  rc = efx_probe_channel(channel);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to create channel %d\n",
                 channel->channel);
    } else {
    }
    goto fail;
  } else {
  }
  channel = channel->channel != 0 ? efx->channel[channel->channel + -1] : 0;
  ldv_46714: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46713;
  } else {
  }
  efx_set_channel_names(efx);
  return (0);
  fail:
  efx_remove_channels(efx);
  return (rc);
}
}
static void efx_start_datapath(struct efx_nic *efx )
{
  struct efx_tx_queue *tx_queue ;
  struct efx_rx_queue *rx_queue ;
  struct efx_channel *channel ;
  int _max1 ;
  int _max2 ;
  int tmp ;
  unsigned int tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  int tmp___5 ;
  int __ret_warn_on ;
  long tmp___6 ;
  bool tmp___7 ;
  {
  _max1 = 0;
  _max2 = 0;
  efx->rx_buffer_len = ((((unsigned int )(_max1 > _max2 ? _max1 : _max2) + (((efx->net_dev)->mtu + 29U) & 4294967288U)) + (unsigned int )(efx->type)->rx_buffer_hash_size) + (unsigned int )(efx->type)->rx_buffer_padding) + 16U;
  tmp = __get_order((unsigned long )efx->rx_buffer_len + 64UL);
  efx->rx_buffer_order = (unsigned int )tmp;
  tmp___0 = efx_tx_max_skb_descs(efx);
  efx->txq_stop_thresh = efx->txq_entries - tmp___0;
  efx->txq_wake_thresh = efx->txq_stop_thresh / 2U;
  channel = efx->channel[0];
  goto ldv_46734;
  ldv_46733:
  tmp___2 = efx_channel_has_tx_queues(channel);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46726;
    ldv_46725:
    efx_init_tx_queue(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_46726: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___1 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___1) {
        goto ldv_46725;
      } else {
        goto ldv_46727;
      }
    } else {
    }
    ldv_46727: ;
  }
  efx_rx_strategy(channel);
  tmp___4 = efx_channel_has_rx_queue(channel);
  if (tmp___4) {
    tmp___5 = 0;
  } else {
    tmp___5 = 1;
  }
  if (tmp___5) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_46729;
    ldv_46728:
    efx_init_rx_queue(rx_queue);
    efx_nic_generate_fill_event(rx_queue);
    rx_queue = 0;
    ldv_46729: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_46728;
    } else {
    }
  }
  __ret_warn_on = (unsigned long )channel->rx_pkt != (unsigned long )((struct efx_rx_buffer *)0);
  tmp___6 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___6 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
                       754);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  efx_rx_strategy(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46734: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46733;
  } else {
  }
  tmp___7 = netif_device_present(efx->net_dev);
  if ((int )tmp___7) {
    netif_tx_wake_all_queues(efx->net_dev);
  } else {
  }
  return;
}
}
static void efx_stop_datapath(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  struct efx_rx_queue *rx_queue ;
  struct pci_dev *dev ;
  int rc ;
  int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  struct _ddebug descriptor ;
  long tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  bool tmp___5 ;
  int tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  {
  dev = efx->pci_dev;
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             770);
      dump_stack();
    } else {
    }
  } else {
  }
  tmp___1 = ldv__builtin_expect((long )efx->port_enabled, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (771), "i" (12UL));
    ldv_46744: ;
    goto ldv_46744;
  } else {
  }
  if ((unsigned int )*((unsigned char *)dev + 2248UL) != 0U) {
    rc = efx_nic_flush_queues(efx);
    if (rc != 0) {
      tmp___3 = efx_nic_rev(efx);
      if (tmp___3 <= 2) {
        if ((int )efx->msg_enable & 1) {
          netdev_err((struct net_device const *)efx->net_dev, "Resetting to recover from flush failure\n");
        } else {
        }
        efx_schedule_reset(efx, 1);
      } else {
        goto _L;
      }
    } else
    _L:
    if (rc != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "failed to flush queues\n");
      } else
      if ((int )efx->msg_enable & 1) {
        descriptor.modname = "sfc";
        descriptor.function = "efx_stop_datapath";
        descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
        descriptor.format = "successfully flushed all queues\n";
        descriptor.lineno = 789U;
        descriptor.flags = 0U;
        tmp___2 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
        if (tmp___2 != 0L) {
          __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                               "successfully flushed all queues\n");
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  channel = efx->channel[0];
  goto ldv_46754;
  ldv_46753:
  tmp___4 = efx_channel_has_rx_queue(channel);
  if ((int )tmp___4) {
    efx_stop_eventq(channel);
    efx_start_eventq(channel);
  } else {
  }
  tmp___5 = efx_channel_has_rx_queue(channel);
  if (tmp___5) {
    tmp___6 = 0;
  } else {
    tmp___6 = 1;
  }
  if (tmp___6) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_46748;
    ldv_46747:
    efx_fini_rx_queue(rx_queue);
    rx_queue = 0;
    ldv_46748: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_46747;
    } else {
    }
  }
  tmp___7 = efx_channel_has_tx_queues(channel);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46751;
    ldv_46750:
    efx_fini_tx_queue(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_46751: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      goto ldv_46750;
    } else {
    }
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46754: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46753;
  } else {
  }
  return;
}
}
static void efx_remove_channel(struct efx_channel *channel )
{
  struct efx_tx_queue *tx_queue ;
  struct efx_rx_queue *rx_queue ;
  struct _ddebug descriptor ;
  long tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  if ((int )(channel->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_channel";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "destroy chan %d\n";
    descriptor.lineno = 818U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(channel->efx)->net_dev,
                           "destroy chan %d\n", channel->channel);
    } else {
    }
  } else {
  }
  tmp___0 = efx_channel_has_rx_queue(channel);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_46764;
    ldv_46763:
    efx_remove_rx_queue(rx_queue);
    rx_queue = 0;
    ldv_46764: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_46763;
    } else {
    }
  }
  tmp___2 = efx_channel_has_tx_queues(channel);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46767;
    ldv_46766:
    efx_remove_tx_queue(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_46767: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      goto ldv_46766;
    } else {
    }
  }
  efx_remove_eventq(channel);
  (*((channel->type)->post_remove))(channel);
  return;
}
}
static void efx_remove_channels(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_46774;
  ldv_46773:
  efx_remove_channel(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46774: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46773;
  } else {
  }
  return;
}
}
int efx_realloc_channels(struct efx_nic *efx , u32 rxq_entries , u32 txq_entries )
{
  struct efx_channel *other_channel[32U] ;
  struct efx_channel *channel ;
  u32 old_rxq_entries ;
  u32 old_txq_entries ;
  unsigned int i ;
  unsigned int next_buffer_table ;
  int rc ;
  struct efx_rx_queue *rx_queue ;
  struct efx_tx_queue *tx_queue ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  unsigned int _max1___0 ;
  unsigned int _max2___0 ;
  bool tmp ;
  int tmp___0 ;
  unsigned int _max1___1 ;
  unsigned int _max2___1 ;
  bool tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  next_buffer_table = 0U;
  rc = efx_check_disabled(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  channel = efx->channel[0];
  goto ldv_46807;
  ldv_46806: ;
  if ((unsigned long )(channel->type)->copy != (unsigned long )((struct efx_channel *(* )(struct efx_channel const * ))0)) {
    goto ldv_46790;
  } else {
  }
  _max1 = next_buffer_table;
  _max2 = channel->eventq.index + channel->eventq.entries;
  next_buffer_table = _max1 > _max2 ? _max1 : _max2;
  tmp = efx_channel_has_rx_queue(channel);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_46798;
    ldv_46797:
    _max1___0 = next_buffer_table;
    _max2___0 = rx_queue->rxd.index + rx_queue->rxd.entries;
    next_buffer_table = _max1___0 > _max2___0 ? _max1___0 : _max2___0;
    rx_queue = 0;
    ldv_46798: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_46797;
    } else {
    }
  }
  tmp___2 = efx_channel_has_tx_queues(channel);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46804;
    ldv_46803:
    _max1___1 = next_buffer_table;
    _max2___1 = tx_queue->txd.index + tx_queue->txd.entries;
    next_buffer_table = _max1___1 > _max2___1 ? _max1___1 : _max2___1;
    tx_queue = tx_queue + 1;
    ldv_46804: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___1 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___1) {
        goto ldv_46803;
      } else {
        goto ldv_46805;
      }
    } else {
    }
    ldv_46805: ;
  }
  ldv_46790:
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46807: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46806;
  } else {
  }
  efx_stop_all(efx);
  efx_stop_interrupts(efx, 1);
  memset((void *)(& other_channel), 0, 256UL);
  i = 0U;
  goto ldv_46811;
  ldv_46810:
  channel = efx->channel[i];
  if ((unsigned long )(channel->type)->copy != (unsigned long )((struct efx_channel *(* )(struct efx_channel const * ))0)) {
    channel = (*((channel->type)->copy))((struct efx_channel const *)channel);
  } else {
  }
  if ((unsigned long )channel == (unsigned long )((struct efx_channel *)0)) {
    rc = -12;
    goto out;
  } else {
  }
  other_channel[i] = channel;
  i = i + 1U;
  ldv_46811: ;
  if (efx->n_channels > i) {
    goto ldv_46810;
  } else {
  }
  old_rxq_entries = efx->rxq_entries;
  old_txq_entries = efx->txq_entries;
  efx->rxq_entries = rxq_entries;
  efx->txq_entries = txq_entries;
  i = 0U;
  goto ldv_46814;
  ldv_46813:
  channel = efx->channel[i];
  efx->channel[i] = other_channel[i];
  other_channel[i] = channel;
  i = i + 1U;
  ldv_46814: ;
  if (efx->n_channels > i) {
    goto ldv_46813;
  } else {
  }
  efx->next_buffer_table = next_buffer_table;
  i = 0U;
  goto ldv_46819;
  ldv_46818:
  channel = efx->channel[i];
  if ((unsigned long )(channel->type)->copy == (unsigned long )((struct efx_channel *(* )(struct efx_channel const * ))0)) {
    goto ldv_46816;
  } else {
  }
  rc = efx_probe_channel(channel);
  if (rc != 0) {
    goto rollback;
  } else {
  }
  efx_init_napi_channel(efx->channel[i]);
  ldv_46816:
  i = i + 1U;
  ldv_46819: ;
  if (efx->n_channels > i) {
    goto ldv_46818;
  } else {
  }
  out:
  i = 0U;
  goto ldv_46822;
  ldv_46821:
  channel = other_channel[i];
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0) && (unsigned long )(channel->type)->copy != (unsigned long )((struct efx_channel *(* )(struct efx_channel const * ))0)) {
    efx_fini_napi_channel(channel);
    efx_remove_channel(channel);
    kfree((void const *)channel);
  } else {
  }
  i = i + 1U;
  ldv_46822: ;
  if (efx->n_channels > i) {
    goto ldv_46821;
  } else {
  }
  efx_start_interrupts(efx, 1);
  efx_start_all(efx);
  return (rc);
  rollback:
  efx->rxq_entries = old_rxq_entries;
  efx->txq_entries = old_txq_entries;
  i = 0U;
  goto ldv_46825;
  ldv_46824:
  channel = efx->channel[i];
  efx->channel[i] = other_channel[i];
  other_channel[i] = channel;
  i = i + 1U;
  ldv_46825: ;
  if (efx->n_channels > i) {
    goto ldv_46824;
  } else {
  }
  goto out;
}
}
void efx_schedule_slow_fill(struct efx_rx_queue *rx_queue )
{
  unsigned long tmp ;
  {
  tmp = msecs_to_jiffies(100U);
  mod_timer(& rx_queue->slow_fill, tmp + (unsigned long )jiffies);
  return;
}
}
static struct efx_channel_type const efx_default_channel_type = {0, & efx_channel_dummy_op_int, & efx_channel_dummy_op_void, & efx_get_channel_name,
    & efx_copy_channel, 0, 0};
int efx_channel_dummy_op_int(struct efx_channel *channel )
{
  {
  return (0);
}
}
void efx_channel_dummy_op_void(struct efx_channel *channel )
{
  {
  return;
}
}
void efx_link_status_changed(struct efx_nic *efx )
{
  struct efx_link_state *link_state ;
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  link_state = & efx->link_state;
  tmp = netif_running((struct net_device const *)efx->net_dev);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  tmp___1 = netif_carrier_ok((struct net_device const *)efx->net_dev);
  if ((int )link_state->up != (int )tmp___1) {
    efx->n_link_state_changes = efx->n_link_state_changes + 1U;
    if ((int )link_state->up) {
      netif_carrier_on(efx->net_dev);
    } else {
      netif_carrier_off(efx->net_dev);
    }
  } else {
  }
  if ((int )link_state->up) {
    if ((efx->msg_enable & 4U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "link up at %uMbps %s-duplex (MTU %d)%s\n",
                  link_state->speed, (int )link_state->fd ? (char *)"full" : (char *)"half",
                  (efx->net_dev)->mtu, (int )efx->promiscuous ? (char *)" [PROMISC]" : (char *)"");
    } else
    if ((efx->msg_enable & 4U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "link down\n");
    } else {
    }
  } else {
  }
  return;
}
}
void efx_link_set_advertising(struct efx_nic *efx , u32 advertising )
{
  {
  efx->link_advertising = advertising;
  if (advertising != 0U) {
    if ((advertising & 8192U) != 0U) {
      efx->wanted_fc = (u8 )((unsigned int )efx->wanted_fc | 3U);
    } else {
      efx->wanted_fc = (unsigned int )efx->wanted_fc & 252U;
    }
    if ((advertising & 16384U) != 0U) {
      efx->wanted_fc = (u8 )((unsigned int )efx->wanted_fc ^ 1U);
    } else {
    }
  } else {
  }
  return;
}
}
void efx_link_set_wanted_fc(struct efx_nic *efx , u8 wanted_fc )
{
  {
  efx->wanted_fc = wanted_fc;
  if (efx->link_advertising != 0U) {
    if (((int )wanted_fc & 2) != 0) {
      efx->link_advertising = efx->link_advertising | 24576U;
    } else {
      efx->link_advertising = efx->link_advertising & 4294942719U;
    }
    if ((int )wanted_fc & 1) {
      efx->link_advertising = efx->link_advertising ^ 16384U;
    } else {
    }
  } else {
  }
  return;
}
}
static void efx_fini_port(struct efx_nic *efx ) ;
int __efx_reconfigure_port(struct efx_nic *efx )
{
  enum efx_phy_mode phy_mode ;
  int rc ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  {
  tmp = ldv_mutex_is_locked_8(& efx->mac_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
                       1042);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  netif_addr_lock_bh(efx->net_dev);
  netif_addr_unlock_bh(efx->net_dev);
  phy_mode = efx->phy_mode;
  if ((66600958 >> (int )efx->loopback_mode) & 1) {
    efx->phy_mode = (enum efx_phy_mode )((unsigned int )efx->phy_mode | 1U);
  } else {
    efx->phy_mode = (enum efx_phy_mode )((unsigned int )efx->phy_mode & 4294967294U);
  }
  rc = (*((efx->type)->reconfigure_port))(efx);
  if (rc != 0) {
    efx->phy_mode = phy_mode;
  } else {
  }
  return (rc);
}
}
int efx_reconfigure_port(struct efx_nic *efx )
{
  int rc ;
  int tmp ;
  long tmp___0 ;
  {
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             1069);
      dump_stack();
    } else {
    }
  } else {
  }
  ldv_mutex_lock_9(& efx->mac_lock);
  rc = __efx_reconfigure_port(efx);
  ldv_mutex_unlock_10(& efx->mac_lock);
  return (rc);
}
}
static void efx_mac_work(struct work_struct *data )
{
  struct efx_nic *efx ;
  struct work_struct const *__mptr ;
  {
  __mptr = (struct work_struct const *)data;
  efx = (struct efx_nic *)__mptr + 0xfffffffffffff748UL;
  ldv_mutex_lock_11(& efx->mac_lock);
  if ((int )efx->port_enabled) {
    (*((efx->type)->reconfigure_mac))(efx);
  } else {
  }
  ldv_mutex_unlock_12(& efx->mac_lock);
  return;
}
}
static int efx_probe_port(struct efx_nic *efx )
{
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  size_t __len ;
  void *__ret ;
  {
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "create port\n";
    descriptor.lineno = 1095U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "create port\n");
    } else {
    }
  } else {
  }
  if ((int )phy_flash_cfg) {
    efx->phy_mode = 8;
  } else {
  }
  rc = (*((efx->type)->probe_port))(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(efx->net_dev)->dev_addr, (void const *)(& (efx->net_dev)->perm_addr),
                     __len);
  } else {
    __ret = memcpy((void *)(efx->net_dev)->dev_addr, (void const *)(& (efx->net_dev)->perm_addr),
                             __len);
  }
  return (0);
}
}
static int efx_init_port(struct efx_nic *efx )
{
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "init port\n";
    descriptor.lineno = 1115U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "init port\n");
    } else {
    }
  } else {
  }
  ldv_mutex_lock_13(& efx->mac_lock);
  rc = (*((efx->phy_op)->init))(efx);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  efx->port_initialized = 1;
  (*((efx->type)->reconfigure_mac))(efx);
  rc = (*((efx->phy_op)->reconfigure))(efx);
  if (rc != 0) {
    goto fail2;
  } else {
  }
  ldv_mutex_unlock_14(& efx->mac_lock);
  return (0);
  fail2:
  (*((efx->phy_op)->fini))(efx);
  fail1:
  ldv_mutex_unlock_15(& efx->mac_lock);
  return (rc);
}
}
static void efx_start_port(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  long tmp___0 ;
  {
  if ((efx->msg_enable & 32U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_start_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "start port\n";
    descriptor.lineno = 1146U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "start port\n");
    } else {
    }
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )efx->port_enabled, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (1147), "i" (12UL));
    ldv_46890: ;
    goto ldv_46890;
  } else {
  }
  ldv_mutex_lock_16(& efx->mac_lock);
  efx->port_enabled = 1;
  (*((efx->type)->reconfigure_mac))(efx);
  ldv_mutex_unlock_17(& efx->mac_lock);
  return;
}
}
static void efx_stop_port(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((efx->msg_enable & 16U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_stop_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "stop port\n";
    descriptor.lineno = 1162U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "stop port\n");
    } else {
    }
  } else {
  }
  ldv_mutex_lock_18(& efx->mac_lock);
  efx->port_enabled = 0;
  ldv_mutex_unlock_19(& efx->mac_lock);
  netif_addr_lock_bh(efx->net_dev);
  netif_addr_unlock_bh(efx->net_dev);
  return;
}
}
static void efx_fini_port(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "shut down port\n";
    descriptor.lineno = 1175U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "shut down port\n");
    } else {
    }
  } else {
  }
  if (! efx->port_initialized) {
    return;
  } else {
  }
  (*((efx->phy_op)->fini))(efx);
  efx->port_initialized = 0;
  efx->link_state.up = 0;
  efx_link_status_changed(efx);
  return;
}
}
static void efx_remove_port(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "destroying port\n";
    descriptor.lineno = 1189U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "destroying port\n");
    } else {
    }
  } else {
  }
  (*((efx->type)->remove_port))(efx);
  return;
}
}
static int efx_init_io(struct efx_nic *efx )
{
  struct pci_dev *pci_dev ;
  dma_addr_t dma_mask ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  int tmp___0 ;
  struct _ddebug descriptor___0 ;
  long tmp___1 ;
  struct _ddebug descriptor___1 ;
  long tmp___2 ;
  {
  pci_dev = efx->pci_dev;
  dma_mask = (efx->type)->max_dma_mask;
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_io";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "initialising I/O\n";
    descriptor.lineno = 1207U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "initialising I/O\n");
    } else {
    }
  } else {
  }
  rc = pci_enable_device(pci_dev);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to enable PCI device\n");
    } else {
    }
    goto fail1;
  } else {
  }
  pci_set_master(pci_dev);
  goto ldv_46917;
  ldv_46916:
  tmp___0 = dma_supported(& pci_dev->dev, dma_mask);
  if (tmp___0 != 0) {
    rc = dma_set_mask(& pci_dev->dev, dma_mask);
    if (rc == 0) {
      goto ldv_46915;
    } else {
    }
  } else {
  }
  dma_mask = dma_mask >> 1;
  ldv_46917: ;
  if (dma_mask > 2147483647ULL) {
    goto ldv_46916;
  } else {
  }
  ldv_46915: ;
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "could not find a suitable DMA mask\n");
    } else {
    }
    goto fail2;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "efx_init_io";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor___0.format = "using DMA mask %llx\n";
    descriptor___0.lineno = 1237U;
    descriptor___0.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "using DMA mask %llx\n", dma_mask);
    } else {
    }
  } else {
  }
  rc = dma_set_coherent_mask(& pci_dev->dev, dma_mask);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to set consistent DMA mask\n");
    } else {
    }
    goto fail2;
  } else {
  }
  efx->membase_phys = (efx->pci_dev)->resource[2].start;
  rc = pci_request_region(pci_dev, 2, "sfc");
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "request for memory BAR failed\n");
    } else {
    }
    rc = -5;
    goto fail3;
  } else {
  }
  efx->membase = ioremap_nocache(efx->membase_phys, (unsigned long )(efx->type)->mem_map_size);
  if ((unsigned long )efx->membase == (unsigned long )((void *)0)) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "could not map memory BAR at %llx+%x\n",
                 efx->membase_phys, (efx->type)->mem_map_size);
    } else {
    }
    rc = -12;
    goto fail4;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor___1.modname = "sfc";
    descriptor___1.function = "efx_init_io";
    descriptor___1.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor___1.format = "memory BAR at %llx+%x (virtual %p)\n";
    descriptor___1.lineno = 1270U;
    descriptor___1.flags = 0U;
    tmp___2 = ldv__builtin_expect((long )descriptor___1.flags & 1L, 0L);
    if (tmp___2 != 0L) {
      __dynamic_netdev_dbg(& descriptor___1, (struct net_device const *)efx->net_dev,
                           "memory BAR at %llx+%x (virtual %p)\n", efx->membase_phys,
                           (efx->type)->mem_map_size, efx->membase);
    } else {
    }
  } else {
  }
  return (0);
  fail4:
  pci_release_region(efx->pci_dev, 2);
  fail3:
  efx->membase_phys = 0ULL;
  fail2:
  pci_disable_device(efx->pci_dev);
  fail1: ;
  return (rc);
}
}
static void efx_fini_io(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_io";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "shutting down I/O\n";
    descriptor.lineno = 1286U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "shutting down I/O\n");
    } else {
    }
  } else {
  }
  if ((unsigned long )efx->membase != (unsigned long )((void *)0)) {
    iounmap((void volatile *)efx->membase);
    efx->membase = 0;
  } else {
  }
  if (efx->membase_phys != 0ULL) {
    pci_release_region(efx->pci_dev, 2);
    efx->membase_phys = 0ULL;
  } else {
  }
  pci_disable_device(efx->pci_dev);
  return;
}
}
static unsigned int efx_wanted_parallelism(struct efx_nic *efx )
{
  cpumask_var_t thread_mask ;
  unsigned int count ;
  int cpu ;
  bool tmp ;
  int tmp___0 ;
  long tmp___1 ;
  void const *__vpp_verify ;
  unsigned long __ptr ;
  unsigned int tmp___2 ;
  int tmp___3 ;
  unsigned int tmp___4 ;
  unsigned int tmp___5 ;
  bool tmp___6 ;
  unsigned int tmp___7 ;
  unsigned int tmp___8 ;
  {
  if (rss_cpus != 0U) {
    count = rss_cpus;
  } else {
    tmp = zalloc_cpumask_var(& thread_mask, 208U);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    tmp___1 = ldv__builtin_expect((long )tmp___0, 0L);
    if (tmp___1 != 0L) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_warn((struct net_device const *)efx->net_dev, "RSS disabled due to allocation failure\n");
      } else {
      }
      return (1U);
    } else {
    }
    count = 0U;
    cpu = -1;
    goto ldv_46939;
    ldv_46938:
    tmp___2 = cpumask_check((unsigned int )cpu);
    tmp___3 = variable_test_bit((int )tmp___2, (unsigned long const volatile *)(& thread_mask->bits));
    if (tmp___3 == 0) {
      count = count + 1U;
      __vpp_verify = 0;
      __asm__ ("": "=r" (__ptr): "0" (& cpu_sibling_map));
      cpumask_or(thread_mask, (struct cpumask const *)thread_mask, (struct cpumask const *)*((cpumask_var_t **)(__per_cpu_offset[cpu] + __ptr)));
    } else {
    }
    ldv_46939:
    tmp___4 = cpumask_next(cpu, cpu_online_mask);
    cpu = (int )tmp___4;
    if (cpu < nr_cpu_ids) {
      goto ldv_46938;
    } else {
    }
    free_cpumask_var(thread_mask);
  }
  tmp___6 = efx_sriov_wanted(efx);
  if ((int )tmp___6) {
    tmp___7 = efx_vf_size(efx);
    if (tmp___7 > 1U) {
      tmp___8 = efx_vf_size(efx);
      if (tmp___8 < count) {
        if ((efx->msg_enable & 2U) != 0U) {
          tmp___5 = efx_vf_size(efx);
          netdev_warn((struct net_device const *)efx->net_dev, "Reducing number of RSS channels from %u to %u for VF support. Increase vf-msix-limit to use more channels on the PF.\n",
                      count, tmp___5);
        } else {
        }
        count = efx_vf_size(efx);
      } else {
      }
    } else {
    }
  } else {
  }
  return (count);
}
}
static int efx_init_rx_cpu_rmap(struct efx_nic *efx , struct msix_entry *xentries )
{
  unsigned int i ;
  int rc ;
  {
  (efx->net_dev)->rx_cpu_rmap = alloc_irq_cpu_rmap(efx->n_rx_channels);
  if ((unsigned long )(efx->net_dev)->rx_cpu_rmap == (unsigned long )((struct cpu_rmap *)0)) {
    return (-12);
  } else {
  }
  i = 0U;
  goto ldv_46948;
  ldv_46947:
  rc = irq_cpu_rmap_add((efx->net_dev)->rx_cpu_rmap, (int )(xentries + (unsigned long )i)->vector);
  if (rc != 0) {
    free_irq_cpu_rmap((efx->net_dev)->rx_cpu_rmap);
    (efx->net_dev)->rx_cpu_rmap = 0;
    return (rc);
  } else {
  }
  i = i + 1U;
  ldv_46948: ;
  if (efx->n_rx_channels > i) {
    goto ldv_46947;
  } else {
  }
  return (0);
}
}
static int efx_probe_interrupts(struct efx_nic *efx )
{
  unsigned int max_channels ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  unsigned int extra_channels ;
  unsigned int i ;
  unsigned int j ;
  int rc ;
  struct msix_entry xentries[32U] ;
  unsigned int n_channels ;
  unsigned int _min1___0 ;
  unsigned int _min2___0 ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  unsigned int _max1___0 ;
  unsigned int _max2___0 ;
  struct efx_channel *tmp ;
  struct efx_channel *tmp___0 ;
  struct efx_channel *tmp___1 ;
  unsigned int tmp___4 ;
  bool tmp___5 ;
  int tmp___6 ;
  {
  _min1 = (efx->type)->phys_addr_channels;
  _min2 = 32U;
  max_channels = _min1 < (unsigned int )((unsigned int const )_min2) ? _min1 : (unsigned int const )_min2;
  extra_channels = 0U;
  i = 0U;
  goto ldv_46962;
  ldv_46961: ;
  if ((unsigned long )efx->extra_channel_type[i] != (unsigned long )((struct efx_channel_type const *)0)) {
    extra_channels = extra_channels + 1U;
  } else {
  }
  i = i + 1U;
  ldv_46962: ;
  if (i <= 1U) {
    goto ldv_46961;
  } else {
  }
  if ((unsigned int )efx->interrupt_mode == 0U) {
    n_channels = efx_wanted_parallelism(efx);
    if ((int )separate_tx_channels) {
      n_channels = n_channels * 2U;
    } else {
    }
    n_channels = n_channels + extra_channels;
    _min1___0 = n_channels;
    _min2___0 = max_channels;
    n_channels = _min1___0 < _min2___0 ? _min1___0 : _min2___0;
    i = 0U;
    goto ldv_46970;
    ldv_46969:
    xentries[i].entry = (u16 )i;
    i = i + 1U;
    ldv_46970: ;
    if (i < n_channels) {
      goto ldv_46969;
    } else {
    }
    rc = pci_enable_msix(efx->pci_dev, (struct msix_entry *)(& xentries), (int )n_channels);
    if (rc > 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "WARNING: Insufficient MSI-X vectors available (%d < %u).\n",
                   rc, n_channels);
      } else {
      }
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "WARNING: Performance may be reduced.\n");
      } else {
      }
      n_channels = (unsigned int )rc;
      rc = pci_enable_msix(efx->pci_dev, (struct msix_entry *)(& xentries), (int )n_channels);
    } else {
    }
    if (rc == 0) {
      efx->n_channels = n_channels;
      if (n_channels > extra_channels) {
        n_channels = n_channels - extra_channels;
      } else {
      }
      if ((int )separate_tx_channels) {
        _max1 = n_channels / 2U;
        _max2 = 1U;
        efx->n_tx_channels = _max1 > _max2 ? _max1 : _max2;
        _max1___0 = n_channels - efx->n_tx_channels;
        _max2___0 = 1U;
        efx->n_rx_channels = _max1___0 > _max2___0 ? _max1___0 : _max2___0;
      } else {
        efx->n_tx_channels = n_channels;
        efx->n_rx_channels = n_channels;
      }
      rc = efx_init_rx_cpu_rmap(efx, (struct msix_entry *)(& xentries));
      if (rc != 0) {
        pci_disable_msix(efx->pci_dev);
        return (rc);
      } else {
      }
      i = 0U;
      goto ldv_46979;
      ldv_46978:
      tmp = efx_get_channel(efx, i);
      tmp->irq = (int )xentries[i].vector;
      i = i + 1U;
      ldv_46979: ;
      if (efx->n_channels > i) {
        goto ldv_46978;
      } else {
      }
    } else {
      efx->interrupt_mode = 1;
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "could not enable MSI-X\n");
      } else {
      }
    }
  } else {
  }
  if ((unsigned int )efx->interrupt_mode == 1U) {
    efx->n_channels = 1U;
    efx->n_rx_channels = 1U;
    efx->n_tx_channels = 1U;
    rc = pci_enable_msi_block(efx->pci_dev, 1U);
    if (rc == 0) {
      tmp___0 = efx_get_channel(efx, 0U);
      tmp___0->irq = (int )(efx->pci_dev)->irq;
    } else {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "could not enable MSI\n");
      } else {
      }
      efx->interrupt_mode = 2;
    }
  } else {
  }
  if ((unsigned int )efx->interrupt_mode == 2U) {
    efx->n_channels = (unsigned int )((int )separate_tx_channels + 1);
    efx->n_rx_channels = 1U;
    efx->n_tx_channels = 1U;
    efx->legacy_irq = (int )(efx->pci_dev)->irq;
  } else {
  }
  j = efx->n_channels;
  i = 0U;
  goto ldv_46983;
  ldv_46982: ;
  if ((unsigned long )efx->extra_channel_type[i] == (unsigned long )((struct efx_channel_type const *)0)) {
    goto ldv_46981;
  } else {
  }
  if ((unsigned int )efx->interrupt_mode != 0U || efx->n_channels <= extra_channels) {
    (*((efx->extra_channel_type[i])->handle_no_channel))(efx);
  } else {
    j = j - 1U;
    tmp___1 = efx_get_channel(efx, j);
    tmp___1->type = efx->extra_channel_type[i];
  }
  ldv_46981:
  i = i + 1U;
  ldv_46983: ;
  if (i <= 1U) {
    goto ldv_46982;
  } else {
  }
  if (efx->n_rx_channels > 1U) {
    efx->rss_spread = efx->n_rx_channels;
  } else {
    tmp___5 = efx_sriov_wanted(efx);
    if (tmp___5) {
      tmp___6 = 0;
    } else {
      tmp___6 = 1;
    }
    if (tmp___6) {
      efx->rss_spread = efx->n_rx_channels;
    } else {
      tmp___4 = efx_vf_size(efx);
      efx->rss_spread = tmp___4;
    }
  }
  return (0);
}
}
static void efx_start_interrupts(struct efx_nic *efx , bool may_keep_eventq )
{
  struct efx_channel *channel ;
  long tmp ;
  {
  tmp = ldv__builtin_expect((unsigned int )efx->state == 2U, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (1486), "i" (12UL));
    ldv_46990: ;
    goto ldv_46990;
  } else {
  }
  if (efx->legacy_irq != 0) {
    efx->legacy_irq_enabled = 1;
  } else {
  }
  efx_nic_enable_interrupts(efx);
  channel = efx->channel[0];
  goto ldv_46992;
  ldv_46991: ;
  if (! ((_Bool )(channel->type)->keep_eventq) || ! may_keep_eventq) {
    efx_init_eventq(channel);
  } else {
  }
  efx_start_eventq(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46992: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46991;
  } else {
  }
  efx_mcdi_mode_event(efx);
  return;
}
}
static void efx_stop_interrupts(struct efx_nic *efx , bool may_keep_eventq )
{
  struct efx_channel *channel ;
  {
  if ((unsigned int )efx->state == 2U) {
    return;
  } else {
  }
  efx_mcdi_mode_poll(efx);
  efx_nic_disable_interrupts(efx);
  if (efx->legacy_irq != 0) {
    synchronize_irq((unsigned int )efx->legacy_irq);
    efx->legacy_irq_enabled = 0;
  } else {
  }
  channel = efx->channel[0];
  goto ldv_47000;
  ldv_46999: ;
  if (channel->irq != 0) {
    synchronize_irq((unsigned int )channel->irq);
  } else {
  }
  efx_stop_eventq(channel);
  if (! ((_Bool )(channel->type)->keep_eventq) || ! may_keep_eventq) {
    efx_fini_eventq(channel);
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47000: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46999;
  } else {
  }
  return;
}
}
static void efx_remove_interrupts(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_47007;
  ldv_47006:
  channel->irq = 0;
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47007: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47006;
  } else {
  }
  pci_disable_msi(efx->pci_dev);
  pci_disable_msix(efx->pci_dev);
  efx->legacy_irq = 0;
  return;
}
}
static void efx_set_channels(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  bool tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  efx->tx_channel_offset = (int )separate_tx_channels ? efx->n_channels - efx->n_tx_channels : 0U;
  channel = efx->channel[0];
  goto ldv_47018;
  ldv_47017: ;
  if ((unsigned int )channel->channel < efx->n_rx_channels) {
    channel->rx_queue.core_index = channel->channel;
  } else {
    channel->rx_queue.core_index = -1;
  }
  tmp___0 = efx_channel_has_tx_queues(channel);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_47015;
    ldv_47014:
    tx_queue->queue = tx_queue->queue - efx->tx_channel_offset * 4U;
    tx_queue = tx_queue + 1;
    ldv_47015: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp = efx_tx_queue_used(tx_queue);
      if ((int )tmp) {
        goto ldv_47014;
      } else {
        goto ldv_47016;
      }
    } else {
    }
    ldv_47016: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47018: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47017;
  } else {
  }
  return;
}
}
static int efx_probe_nic(struct efx_nic *efx )
{
  size_t i ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_nic";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "creating NIC\n";
    descriptor.lineno = 1569U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "creating NIC\n");
    } else {
    }
  } else {
  }
  rc = (*((efx->type)->probe))(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_probe_interrupts(efx);
  if (rc != 0) {
    goto fail;
  } else {
  }
  (*((efx->type)->dimension_resources))(efx);
  if (efx->n_channels > 1U) {
    get_random_bytes((void *)(& efx->rx_hash_key), 40);
  } else {
  }
  i = 0UL;
  goto ldv_47031;
  ldv_47030:
  efx->rx_indir_table[i] = ethtool_rxfh_indir_default((u32 )i, efx->rss_spread);
  i = i + 1UL;
  ldv_47031: ;
  if (i <= 127UL) {
    goto ldv_47030;
  } else {
  }
  efx_set_channels(efx);
  netif_set_real_num_tx_queues(efx->net_dev, efx->n_tx_channels);
  netif_set_real_num_rx_queues(efx->net_dev, efx->n_rx_channels);
  efx_init_irq_moderation(efx, tx_irq_mod_usec, rx_irq_mod_usec, 1, 1);
  return (0);
  fail:
  (*((efx->type)->remove))(efx);
  return (rc);
}
}
static void efx_remove_nic(struct efx_nic *efx )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_nic";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "destroying NIC\n";
    descriptor.lineno = 1607U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "destroying NIC\n");
    } else {
    }
  } else {
  }
  efx_remove_interrupts(efx);
  (*((efx->type)->remove))(efx);
  return;
}
}
static int efx_probe_all(struct efx_nic *efx )
{
  int rc ;
  int __ret_warn_on ;
  unsigned int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  unsigned int tmp___2 ;
  {
  rc = efx_probe_nic(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to create NIC\n");
    } else {
    }
    goto fail1;
  } else {
  }
  rc = efx_probe_port(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to create port\n");
    } else {
    }
    goto fail2;
  } else {
  }
  tmp = efx_tx_max_skb_descs(efx);
  __ret_warn_on = tmp * 2U > 1024U;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
                       1636);
  } else {
  }
  tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___1 != 0L) {
    rc = -22;
    goto fail3;
  } else {
  }
  tmp___2 = 1024U;
  efx->txq_entries = tmp___2;
  efx->rxq_entries = tmp___2;
  rc = efx_probe_filters(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to create filter tables\n");
    } else {
    }
    goto fail3;
  } else {
  }
  rc = efx_probe_channels(efx);
  if (rc != 0) {
    goto fail4;
  } else {
  }
  return (0);
  fail4:
  efx_remove_filters(efx);
  fail3:
  efx_remove_port(efx);
  fail2:
  efx_remove_nic(efx);
  fail1: ;
  return (rc);
}
}
static void efx_start_all(struct efx_nic *efx )
{
  int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  bool tmp___4 ;
  {
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             1674);
      dump_stack();
    } else {
    }
  } else {
  }
  tmp___1 = ldv__builtin_expect((unsigned int )efx->state == 2U, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (1675), "i" (12UL));
    ldv_47051: ;
    goto ldv_47051;
  } else {
  }
  if ((int )efx->port_enabled) {
    return;
  } else {
    tmp___2 = netif_running((struct net_device const *)efx->net_dev);
    if (tmp___2) {
      tmp___3 = 0;
    } else {
      tmp___3 = 1;
    }
    if (tmp___3) {
      return;
    } else {
    }
  }
  efx_start_port(efx);
  efx_start_datapath(efx);
  if ((unsigned long )(efx->type)->monitor != (unsigned long )((void (* )(struct efx_nic * ))0)) {
    queue_delayed_work(efx->workqueue, & efx->monitor_work, (unsigned long )efx_monitor_interval);
  } else {
    ldv_mutex_lock_20(& efx->mac_lock);
    tmp___4 = (*((efx->phy_op)->poll))(efx);
    if ((int )tmp___4) {
      efx_link_status_changed(efx);
    } else {
    }
    ldv_mutex_unlock_21(& efx->mac_lock);
  }
  (*((efx->type)->start_stats))(efx);
  return;
}
}
static void efx_flush_all(struct efx_nic *efx )
{
  {
  cancel_delayed_work_sync(& efx->monitor_work);
  efx_selftest_async_cancel(efx);
  cancel_work_sync(& efx->mac_work);
  return;
}
}
static void efx_stop_all(struct efx_nic *efx )
{
  int tmp ;
  long tmp___0 ;
  {
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             1720);
      dump_stack();
    } else {
    }
  } else {
  }
  if (! efx->port_enabled) {
    return;
  } else {
  }
  (*((efx->type)->stop_stats))(efx);
  efx_stop_port(efx);
  efx_flush_all(efx);
  netif_tx_disable(efx->net_dev);
  efx_stop_datapath(efx);
  return;
}
}
static void efx_remove_all(struct efx_nic *efx )
{
  {
  efx_remove_channels(efx);
  efx_remove_filters(efx);
  efx_remove_port(efx);
  efx_remove_nic(efx);
  return;
}
}
static unsigned int irq_mod_ticks(unsigned int usecs , unsigned int quantum_ns )
{
  {
  if (usecs == 0U) {
    return (0U);
  } else {
  }
  if (usecs * 1000U < quantum_ns) {
    return (1U);
  } else {
  }
  return ((usecs * 1000U) / quantum_ns);
}
}
int efx_init_irq_moderation(struct efx_nic *efx , unsigned int tx_usecs , unsigned int rx_usecs ,
                            bool rx_adaptive , bool rx_may_override_tx )
{
  struct efx_channel *channel ;
  unsigned int irq_mod_max ;
  unsigned int tx_ticks ;
  unsigned int rx_ticks ;
  int tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  {
  irq_mod_max = ((unsigned int )(efx->type)->timer_period_max * efx->timer_quantum_ns + 999U) / 1000U;
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             1774);
      dump_stack();
    } else {
    }
  } else {
  }
  if (tx_usecs > irq_mod_max || rx_usecs > irq_mod_max) {
    return (-22);
  } else {
  }
  tx_ticks = irq_mod_ticks(tx_usecs, efx->timer_quantum_ns);
  rx_ticks = irq_mod_ticks(rx_usecs, efx->timer_quantum_ns);
  if ((tx_ticks != rx_ticks && efx->tx_channel_offset == 0U) && ! rx_may_override_tx) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "Channels are shared. RX and TX IRQ moderation must be equal\n");
    } else {
    }
    return (-22);
  } else {
  }
  efx->irq_rx_adaptive = rx_adaptive;
  efx->irq_rx_moderation = rx_ticks;
  channel = efx->channel[0];
  goto ldv_47077;
  ldv_47076:
  tmp___2 = efx_channel_has_rx_queue(channel);
  if ((int )tmp___2) {
    channel->irq_moderation = rx_ticks;
  } else {
    tmp___1 = efx_channel_has_tx_queues(channel);
    if ((int )tmp___1) {
      channel->irq_moderation = tx_ticks;
    } else {
    }
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47077: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47076;
  } else {
  }
  return (0);
}
}
void efx_get_irq_moderation(struct efx_nic *efx , unsigned int *tx_usecs , unsigned int *rx_usecs ,
                            bool *rx_adaptive )
{
  {
  *rx_adaptive = efx->irq_rx_adaptive;
  *rx_usecs = (efx->irq_rx_moderation * efx->timer_quantum_ns + 999U) / 1000U;
  if (efx->tx_channel_offset == 0U) {
    *tx_usecs = *rx_usecs;
  } else {
    *tx_usecs = ((efx->channel[efx->tx_channel_offset])->irq_moderation * efx->timer_quantum_ns + 999U) / 1000U;
  }
  return;
}
}
static void efx_monitor(struct work_struct *data )
{
  struct efx_nic *efx ;
  struct work_struct const *__mptr ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  long tmp ;
  int tmp___0 ;
  {
  __mptr = (struct work_struct const *)data;
  efx = (struct efx_nic *)__mptr + 0xfffffffffffff440UL;
  if (0) {
    if ((efx->msg_enable & 8U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_47096;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47096;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47096;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47096;
      default:
      __bad_percpu_size();
      }
      ldv_47096:
      pscr_ret__ = pfo_ret__;
      goto ldv_47102;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47106;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47106;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47106;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47106;
      default:
      __bad_percpu_size();
      }
      ldv_47106:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_47102;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47115;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47115;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47115;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47115;
      default:
      __bad_percpu_size();
      }
      ldv_47115:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_47102;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47124;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47124;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47124;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47124;
      default:
      __bad_percpu_size();
      }
      ldv_47124:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_47102;
      default:
      __bad_size_call_parameter();
      goto ldv_47102;
      }
      ldv_47102:
      netdev_printk("\017", (struct net_device const *)efx->net_dev, "hardware monitor executing on CPU %d\n",
                    pscr_ret__);
    } else {
    }
  } else {
  }
  tmp = ldv__builtin_expect((unsigned long )(efx->type)->monitor == (unsigned long )((void (* )(struct efx_nic * ))0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (1841), "i" (12UL));
    ldv_47133: ;
    goto ldv_47133;
  } else {
  }
  tmp___0 = ldv_mutex_trylock_22(& efx->mac_lock);
  if (tmp___0 != 0) {
    if ((int )efx->port_enabled) {
      (*((efx->type)->monitor))(efx);
    } else {
    }
    ldv_mutex_unlock_23(& efx->mac_lock);
  } else {
  }
  queue_delayed_work(efx->workqueue, & efx->monitor_work, (unsigned long )efx_monitor_interval);
  return;
}
}
static int efx_ioctl(struct net_device *net_dev , struct ifreq *ifr , int cmd )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct mii_ioctl_data *data ;
  struct mii_ioctl_data *tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = if_mii(ifr);
  data = tmp___0;
  if (cmd == 35248) {
    tmp___1 = efx_ptp_ioctl(efx, ifr, cmd);
    return (tmp___1);
  } else {
  }
  if ((cmd == 35144 || cmd == 35145) && ((int )data->phy_id & 64512) == 1024) {
    data->phy_id = (__u16 )((unsigned int )data->phy_id ^ 33792U);
  } else {
  }
  tmp___2 = mdio_mii_ioctl((struct mdio_if_info const *)(& efx->mdio), data, cmd);
  return (tmp___2);
}
}
static void efx_init_napi_channel(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  {
  efx = channel->efx;
  channel->napi_dev = efx->net_dev;
  netif_napi_add(channel->napi_dev, & channel->napi_str, & efx_poll, napi_weight);
  return;
}
}
static void efx_init_napi(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_47150;
  ldv_47149:
  efx_init_napi_channel(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47150: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47149;
  } else {
  }
  return;
}
}
static void efx_fini_napi_channel(struct efx_channel *channel )
{
  {
  if ((unsigned long )channel->napi_dev != (unsigned long )((struct net_device *)0)) {
    netif_napi_del(& channel->napi_str);
  } else {
  }
  channel->napi_dev = 0;
  return;
}
}
static void efx_fini_napi(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_47160;
  ldv_47159:
  efx_fini_napi_channel(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47160: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47159;
  } else {
  }
  return;
}
}
static void efx_netpoll(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_channel *channel ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  channel = efx->channel[0];
  goto ldv_47168;
  ldv_47167:
  efx_schedule_channel(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47168: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47167;
  } else {
  }
  return;
}
}
static int efx_net_open(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  int rc ;
  struct _ddebug descriptor ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  long tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((efx->msg_enable & 32U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_net_open";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "opening device on CPU %d\n";
    descriptor.lineno = 1955U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_47182;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47182;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47182;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47182;
      default:
      __bad_percpu_size();
      }
      ldv_47182:
      pscr_ret__ = pfo_ret__;
      goto ldv_47188;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47192;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47192;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47192;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47192;
      default:
      __bad_percpu_size();
      }
      ldv_47192:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_47188;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47201;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47201;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47201;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47201;
      default:
      __bad_percpu_size();
      }
      ldv_47201:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_47188;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47210;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47210;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47210;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47210;
      default:
      __bad_percpu_size();
      }
      ldv_47210:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_47188;
      default:
      __bad_size_call_parameter();
      goto ldv_47188;
      }
      ldv_47188:
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "opening device on CPU %d\n", pscr_ret__);
    } else {
    }
  } else {
  }
  rc = efx_check_disabled(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (((unsigned int )efx->phy_mode & 8U) != 0U) {
    return (-16);
  } else {
  }
  tmp___1 = efx_mcdi_poll_reboot(efx);
  if (tmp___1 != 0) {
    tmp___2 = efx_reset(efx, 1);
    if (tmp___2 != 0) {
      return (-5);
    } else {
    }
  } else {
  }
  efx_link_status_changed(efx);
  efx_start_all(efx);
  efx_selftest_async_start(efx);
  return (0);
}
}
static int efx_net_stop(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct _ddebug descriptor ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  long tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((efx->msg_enable & 16U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_net_stop";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "closing on CPU %d\n";
    descriptor.lineno = 1983U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_47229;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47229;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47229;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_47229;
      default:
      __bad_percpu_size();
      }
      ldv_47229:
      pscr_ret__ = pfo_ret__;
      goto ldv_47235;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47239;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47239;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47239;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_47239;
      default:
      __bad_percpu_size();
      }
      ldv_47239:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_47235;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47248;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47248;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47248;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_47248;
      default:
      __bad_percpu_size();
      }
      ldv_47248:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_47235;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47257;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47257;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47257;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_47257;
      default:
      __bad_percpu_size();
      }
      ldv_47257:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_47235;
      default:
      __bad_size_call_parameter();
      goto ldv_47235;
      }
      ldv_47235:
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "closing on CPU %d\n", pscr_ret__);
    } else {
    }
  } else {
  }
  efx_stop_all(efx);
  return (0);
}
}
static struct rtnl_link_stats64 *efx_net_stats(struct net_device *net_dev , struct rtnl_link_stats64 *stats )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_mac_stats *mac_stats ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  mac_stats = & efx->mac_stats;
  spin_lock_bh(& efx->stats_lock);
  (*((efx->type)->update_stats))(efx);
  stats->rx_packets = mac_stats->rx_packets;
  stats->tx_packets = mac_stats->tx_packets;
  stats->rx_bytes = mac_stats->rx_bytes;
  stats->tx_bytes = mac_stats->tx_bytes;
  stats->rx_dropped = (__u64 )efx->n_rx_nodesc_drop_cnt;
  stats->multicast = mac_stats->rx_multicast;
  stats->collisions = mac_stats->tx_collision;
  stats->rx_length_errors = mac_stats->rx_gtjumbo + mac_stats->rx_length_error;
  stats->rx_crc_errors = mac_stats->rx_bad;
  stats->rx_frame_errors = mac_stats->rx_align_error;
  stats->rx_fifo_errors = mac_stats->rx_overflow;
  stats->rx_missed_errors = mac_stats->rx_missed;
  stats->tx_window_errors = mac_stats->tx_late_collision;
  stats->rx_errors = ((stats->rx_length_errors + stats->rx_crc_errors) + stats->rx_frame_errors) + mac_stats->rx_symbol_error;
  stats->tx_errors = stats->tx_window_errors + mac_stats->tx_bad;
  spin_unlock_bh(& efx->stats_lock);
  return (stats);
}
}
static void efx_watchdog(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((efx->msg_enable & 128U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "TX stuck with port_enabled=%d: resetting channels\n",
               (int )efx->port_enabled);
  } else {
  }
  efx_schedule_reset(efx, 5);
  return;
}
}
static int efx_change_mtu(struct net_device *net_dev , int new_mtu )
{
  struct efx_nic *efx ;
  void *tmp ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = efx_check_disabled(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (new_mtu > 9216) {
    return (-22);
  } else {
  }
  efx_stop_all(efx);
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_change_mtu";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "changing MTU to %d\n";
    descriptor.lineno = 2056U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "changing MTU to %d\n", new_mtu);
    } else {
    }
  } else {
  }
  ldv_mutex_lock_24(& efx->mac_lock);
  net_dev->mtu = (unsigned int )new_mtu;
  (*((efx->type)->reconfigure_mac))(efx);
  ldv_mutex_unlock_25(& efx->mac_lock);
  efx_start_all(efx);
  return (0);
}
}
static int efx_set_mac_address(struct net_device *net_dev , void *data )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct sockaddr *addr ;
  char *new_addr ;
  bool tmp___0 ;
  int tmp___1 ;
  size_t __len ;
  void *__ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  addr = (struct sockaddr *)data;
  new_addr = (char *)(& addr->sa_data);
  tmp___0 = is_valid_ether_addr((u8 const *)new_addr);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "invalid ethernet MAC address requested: %pM\n",
                 new_addr);
    } else {
    }
    return (-99);
  } else {
  }
  __len = (size_t )net_dev->addr_len;
  __ret = memcpy((void *)net_dev->dev_addr, (void const *)new_addr, __len);
  efx_sriov_mac_address_changed(efx);
  ldv_mutex_lock_26(& efx->mac_lock);
  (*((efx->type)->reconfigure_mac))(efx);
  ldv_mutex_unlock_27(& efx->mac_lock);
  return (0);
}
}
static void efx_set_rx_mode(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct netdev_hw_addr *ha ;
  union efx_multicast_hash *mc_hash ;
  u32 crc ;
  int bit ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  mc_hash = & efx->multicast_hash;
  efx->promiscuous = (net_dev->flags & 256U) != 0U;
  if ((int )efx->promiscuous || (net_dev->flags & 512U) != 0U) {
    memset((void *)mc_hash, 255, 32UL);
  } else {
    memset((void *)mc_hash, 0, 32UL);
    __mptr = (struct list_head const *)net_dev->mc.list.next;
    ha = (struct netdev_hw_addr *)__mptr;
    goto ldv_47306;
    ldv_47305:
    crc = crc32_le(4294967295U, (unsigned char const *)(& ha->addr), 6UL);
    bit = (int )crc & 255;
    __set_bit_le(bit, (void *)mc_hash);
    __mptr___0 = (struct list_head const *)ha->list.next;
    ha = (struct netdev_hw_addr *)__mptr___0;
    ldv_47306: ;
    if ((unsigned long )(& ha->list) != (unsigned long )(& net_dev->mc.list)) {
      goto ldv_47305;
    } else {
    }
    __set_bit_le(255, (void *)mc_hash);
  }
  if ((int )efx->port_enabled) {
    queue_work(efx->workqueue, & efx->mac_work);
  } else {
  }
  return;
}
}
static int efx_set_features(struct net_device *net_dev , netdev_features_t data )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if (((net_dev->features & ~ data) & 134217728ULL) != 0ULL) {
    efx_filter_clear_rx(efx, 1);
  } else {
  }
  return (0);
}
}
static struct net_device_ops const efx_netdev_ops =
     {0, 0, & efx_net_open, & efx_net_stop, & efx_hard_start_xmit, 0, 0, & efx_set_rx_mode,
    & efx_set_mac_address, & eth_validate_addr, & efx_ioctl, 0, & efx_change_mtu,
    0, & efx_watchdog, & efx_net_stats, 0, 0, 0, & efx_netpoll, 0, 0, & efx_sriov_set_vf_mac,
    & efx_sriov_set_vf_vlan, 0, & efx_sriov_set_vf_spoofchk, & efx_sriov_get_vf_config,
    0, 0, & efx_setup_tc, 0, 0, 0, 0, 0, 0, 0, & efx_filter_rfs, 0, 0, 0, & efx_set_features,
    0, 0, 0, 0, 0, 0, 0};
static void efx_update_name(struct efx_nic *efx )
{
  {
  strcpy((char *)(& efx->name), (char const *)(& (efx->net_dev)->name));
  efx_mtd_rename(efx);
  efx_set_channel_names(efx);
  return;
}
}
static int efx_netdev_event(struct notifier_block *this , unsigned long event , void *ptr )
{
  struct net_device *net_dev ;
  void *tmp ;
  {
  net_dev = (struct net_device *)ptr;
  if ((unsigned long )net_dev->netdev_ops == (unsigned long )(& efx_netdev_ops) && event == 10UL) {
    tmp = netdev_priv((struct net_device const *)net_dev);
    efx_update_name((struct efx_nic *)tmp);
  } else {
  }
  return (0);
}
}
static struct notifier_block efx_netdev_notifier = {& efx_netdev_event, 0, 0};
static ssize_t show_phy_type(struct device *dev , struct device_attribute *attr ,
                             char *buf )
{
  struct efx_nic *efx ;
  struct device const *__mptr ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  tmp = pci_get_drvdata((struct pci_dev *)__mptr + 0xffffffffffffff68UL);
  efx = (struct efx_nic *)tmp;
  tmp___0 = sprintf(buf, "%d\n", efx->phy_type);
  return ((ssize_t )tmp___0);
}
}
static struct device_attribute dev_attr_phy_type = {{"phy_type", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                      {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
    & show_phy_type, 0};
static int efx_register_netdev(struct efx_nic *efx )
{
  struct net_device *net_dev ;
  struct efx_channel *channel ;
  int rc ;
  struct efx_tx_queue *tx_queue ;
  bool tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  net_dev = efx->net_dev;
  net_dev->watchdog_timeo = 1250;
  net_dev->irq = (efx->pci_dev)->irq;
  net_dev->netdev_ops = & efx_netdev_ops;
  net_dev->ethtool_ops = & efx_ethtool_ops;
  net_dev->gso_max_segs = 100U;
  rtnl_lock();
  efx->state = 1;
  __asm__ volatile ("mfence": : : "memory");
  if (efx->reset_pending != 0UL) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "aborting probe due to scheduled reset\n");
    } else {
    }
    rc = -5;
    goto fail_locked;
  } else {
  }
  rc = dev_alloc_name(net_dev, (char const *)(& net_dev->name));
  if (rc < 0) {
    goto fail_locked;
  } else {
  }
  efx_update_name(efx);
  netif_carrier_off(net_dev);
  rc = register_netdevice(net_dev);
  if (rc != 0) {
    goto fail_locked;
  } else {
  }
  channel = efx->channel[0];
  goto ldv_47345;
  ldv_47344:
  tmp___0 = efx_channel_has_tx_queues(channel);
  if (tmp___0) {
    tmp___1 = 0;
  } else {
    tmp___1 = 1;
  }
  if (tmp___1) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_47342;
    ldv_47341:
    efx_init_tx_queue_core_txq(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_47342: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp = efx_tx_queue_used(tx_queue);
      if ((int )tmp) {
        goto ldv_47341;
      } else {
        goto ldv_47343;
      }
    } else {
    }
    ldv_47343: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47345: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47344;
  } else {
  }
  rtnl_unlock();
  rc = device_create_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& dev_attr_phy_type));
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to init net dev attributes\n");
    } else {
    }
    goto fail_registered;
  } else {
  }
  return (0);
  fail_registered:
  rtnl_lock();
  unregister_netdevice(net_dev);
  fail_locked:
  efx->state = 0;
  rtnl_unlock();
  if ((int )efx->msg_enable & 1) {
    netdev_err((struct net_device const *)efx->net_dev, "could not register net dev\n");
  } else {
  }
  return (rc);
}
}
static void efx_unregister_netdev(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  void *tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  char const *tmp___4 ;
  {
  if ((unsigned long )efx->net_dev == (unsigned long )((struct net_device *)0)) {
    return;
  } else {
  }
  tmp = netdev_priv((struct net_device const *)efx->net_dev);
  tmp___0 = ldv__builtin_expect((unsigned long )tmp != (unsigned long )((void *)efx),
                             0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (2268), "i" (12UL));
    ldv_47353: ;
    goto ldv_47353;
  } else {
  }
  channel = efx->channel[0];
  goto ldv_47358;
  ldv_47357:
  tmp___2 = efx_channel_has_tx_queues(channel);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_47355;
    ldv_47354:
    efx_release_tx_buffers(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_47355: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___1 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___1) {
        goto ldv_47354;
      } else {
        goto ldv_47356;
      }
    } else {
    }
    ldv_47356: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_47358: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_47357;
  } else {
  }
  tmp___4 = pci_name((struct pci_dev const *)efx->pci_dev);
  strlcpy((char *)(& efx->name), tmp___4, 16UL);
  device_remove_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& dev_attr_phy_type));
  rtnl_lock();
  unregister_netdevice(efx->net_dev);
  efx->state = 0;
  rtnl_unlock();
  return;
}
}
void efx_reset_down(struct efx_nic *efx , enum reset_type method )
{
  int tmp ;
  long tmp___0 ;
  {
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             2297);
      dump_stack();
    } else {
    }
  } else {
  }
  efx_stop_all(efx);
  efx_stop_interrupts(efx, 0);
  ldv_mutex_lock_28(& efx->mac_lock);
  if ((int )efx->port_initialized && (unsigned int )method != 0U) {
    (*((efx->phy_op)->fini))(efx);
  } else {
  }
  (*((efx->type)->fini))(efx);
  return;
}
}
int efx_reset_up(struct efx_nic *efx , enum reset_type method , bool ok )
{
  int rc ;
  int tmp ;
  long tmp___0 ;
  int tmp___1 ;
  {
  if ((unsigned int )efx->state == 1U || (unsigned int )efx->state == 2U) {
    tmp = rtnl_is_locked();
    tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
    if (tmp___0 != 0L) {
      printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
             2317);
      dump_stack();
    } else {
    }
  } else {
  }
  rc = (*((efx->type)->init))(efx);
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to initialise NIC\n");
    } else {
    }
    goto fail;
  } else {
  }
  if (! ok) {
    goto fail;
  } else {
  }
  if ((int )efx->port_initialized && (unsigned int )method != 0U) {
    rc = (*((efx->phy_op)->init))(efx);
    if (rc != 0) {
      goto fail;
    } else {
    }
    tmp___1 = (*((efx->phy_op)->reconfigure))(efx);
    if (tmp___1 != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "could not restore PHY settings\n");
      } else {
      }
    } else {
    }
  } else {
  }
  (*((efx->type)->reconfigure_mac))(efx);
  efx_start_interrupts(efx, 0);
  efx_restore_filters(efx);
  efx_sriov_reset(efx);
  ldv_mutex_unlock_29(& efx->mac_lock);
  efx_start_all(efx);
  return (0);
  fail:
  efx->port_initialized = 0;
  ldv_mutex_unlock_30(& efx->mac_lock);
  return (rc);
}
}
int efx_reset(struct efx_nic *efx , enum reset_type method )
{
  int rc ;
  int rc2 ;
  bool disabled ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )efx->msg_enable & 1) {
    netdev_info((struct net_device const *)efx->net_dev, "resetting (%s)\n", (unsigned int )method < (unsigned int )efx_reset_type_max ? efx_reset_type_names[(unsigned int )method] : (char const * )"(invalid)");
  } else {
  }
  efx_device_detach_sync(efx);
  efx_reset_down(efx, method);
  rc = (*((efx->type)->reset))(efx, method);
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to reset hardware\n");
    } else {
    }
    goto out;
  } else {
  }
  efx->reset_pending = efx->reset_pending & (unsigned long )(- (1 << (int )((unsigned int )method + 1U)));
  pci_set_master(efx->pci_dev);
  out:
  disabled = (bool )(rc != 0 || (unsigned int )method == 3U);
  rc2 = efx_reset_up(efx, method, (int )((bool )(! ((int )disabled != 0))));
  if (rc2 != 0) {
    disabled = 1;
    if (rc == 0) {
      rc = rc2;
    } else {
    }
  } else {
  }
  if ((int )disabled) {
    dev_close(efx->net_dev);
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "has been disabled\n");
    } else {
    }
    efx->state = 2;
  } else {
    if ((int )efx->msg_enable & 1) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_reset";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
      descriptor.format = "reset complete\n";
      descriptor.lineno = 2405U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "reset complete\n");
      } else {
      }
    } else {
    }
    netif_device_attach(efx->net_dev);
  }
  return (rc);
}
}
static void efx_reset_work(struct work_struct *data )
{
  struct efx_nic *efx ;
  struct work_struct const *__mptr ;
  unsigned long pending ;
  int tmp ;
  {
  __mptr = (struct work_struct const *)data;
  efx = (struct efx_nic *)__mptr + 0xffffffffffffffc0UL;
  pending = *((unsigned long volatile *)(& efx->reset_pending));
  if (pending == 0UL) {
    return;
  } else {
  }
  rtnl_lock();
  if ((unsigned int )efx->state == 1U) {
    tmp = fls((int )pending);
    efx_reset(efx, (enum reset_type )(tmp + -1));
  } else {
  }
  rtnl_unlock();
  return;
}
}
void efx_schedule_reset(struct efx_nic *efx , enum reset_type type )
{
  enum reset_type method ;
  struct _ddebug descriptor ;
  long tmp ;
  struct _ddebug descriptor___0 ;
  long tmp___0 ;
  {
  switch ((unsigned int )type) {
  case 0U: ;
  case 1U: ;
  case 2U: ;
  case 3U:
  method = type;
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_schedule_reset";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "scheduling %s reset\n";
    descriptor.lineno = 2445U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "scheduling %s reset\n", (unsigned int )method < (unsigned int )efx_reset_type_max ? efx_reset_type_names[(unsigned int )method] : (char const * )"(invalid)");
    } else {
    }
  } else {
  }
  goto ldv_47399;
  default:
  method = (*((efx->type)->map_reset_reason))(type);
  if ((int )efx->msg_enable & 1) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "efx_schedule_reset";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor___0.format = "scheduling %s reset for %s\n";
    descriptor___0.lineno = 2451U;
    descriptor___0.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "scheduling %s reset for %s\n", (unsigned int )method < (unsigned int )efx_reset_type_max ? efx_reset_type_names[(unsigned int )method] : (char const * )"(invalid)",
                           (unsigned int )type < (unsigned int )efx_reset_type_max ? efx_reset_type_names[(unsigned int )type] : (char const * )"(invalid)");
    } else {
    }
  } else {
  }
  goto ldv_47399;
  }
  ldv_47399:
  set_bit((unsigned int )method, (unsigned long volatile *)(& efx->reset_pending));
  __asm__ volatile ("mfence": : : "memory");
  if ((unsigned int )*((enum nic_state volatile *)(& efx->state)) != 1U) {
    return;
  } else {
  }
  efx_mcdi_mode_poll(efx);
  queue_work(reset_workqueue, & efx->reset_work);
  return;
}
}
static struct pci_device_id const efx_pci_table[5U] = { {6436U, 1795U, 4294967295U, 4294967295U, 0U, 0U, (unsigned long )(& falcon_a1_nic_type)},
        {6436U,
      1808U, 4294967295U, 4294967295U, 0U, 0U, (unsigned long )(& falcon_b0_nic_type)},
        {6436U,
      2051U, 4294967295U, 4294967295U, 0U, 0U, (unsigned long )(& siena_a0_nic_type)},
        {6436U,
      2067U, 4294967295U, 4294967295U, 0U, 0U, (unsigned long )(& siena_a0_nic_type)},
        {0U,
      0U, 0U, 0U, 0U, 0U, 0UL}};
int efx_port_dummy_op_int(struct efx_nic *efx )
{
  {
  return (0);
}
}
void efx_port_dummy_op_void(struct efx_nic *efx )
{
  {
  return;
}
}
static bool efx_port_dummy_op_poll(struct efx_nic *efx )
{
  {
  return (0);
}
}
static struct efx_phy_operations const efx_dummy_phy_operations =
     {0, & efx_port_dummy_op_int, & efx_port_dummy_op_void, 0, & efx_port_dummy_op_int,
    & efx_port_dummy_op_poll, 0, 0, 0, 0, 0, 0, 0, 0};
static int efx_init_struct(struct efx_nic *efx , struct pci_dev *pci_dev , struct net_device *net_dev )
{
  int i ;
  struct lock_class_key __key ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___1 ;
  atomic_long_t __constr_expr_1 ;
  struct lock_class_key __key___2 ;
  struct lock_class_key __key___3 ;
  atomic_long_t __constr_expr_2 ;
  struct lock_class_key __key___4 ;
  char const *tmp ;
  struct lock_class_key __key___5 ;
  struct lock_class_key __key___6 ;
  struct lock_class_key __key___7 ;
  atomic_long_t __constr_expr_3 ;
  struct lock_class_key __key___8 ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  char const *tmp___0 ;
  struct lock_class_key __key___9 ;
  char const *__lock_name ;
  struct workqueue_struct *tmp___1 ;
  {
  spinlock_check(& efx->biu_lock);
  __raw_spin_lock_init(& efx->biu_lock.ldv_5961.rlock, "&(&efx->biu_lock)->rlock",
                       & __key);
  INIT_LIST_HEAD(& efx->mtd_list);
  __init_work(& efx->reset_work, 0);
  __constr_expr_0.counter = 4195328L;
  efx->reset_work.data = __constr_expr_0;
  lockdep_init_map(& efx->reset_work.lockdep_map, "(&efx->reset_work)", & __key___0,
                   0);
  INIT_LIST_HEAD(& efx->reset_work.entry);
  efx->reset_work.func = & efx_reset_work;
  __init_work(& efx->monitor_work.work, 0);
  __constr_expr_1.counter = 4195328L;
  efx->monitor_work.work.data = __constr_expr_1;
  lockdep_init_map(& efx->monitor_work.work.lockdep_map, "(&(&efx->monitor_work)->work)",
                   & __key___1, 0);
  INIT_LIST_HEAD(& efx->monitor_work.work.entry);
  efx->monitor_work.work.func = & efx_monitor;
  init_timer_key(& efx->monitor_work.timer, 2U, "(&(&efx->monitor_work)->timer)",
                 & __key___2);
  efx->monitor_work.timer.function = & delayed_work_timer_fn;
  efx->monitor_work.timer.data = (unsigned long )(& efx->monitor_work);
  __init_work(& efx->selftest_work.work, 0);
  __constr_expr_2.counter = 4195328L;
  efx->selftest_work.work.data = __constr_expr_2;
  lockdep_init_map(& efx->selftest_work.work.lockdep_map, "(&(&efx->selftest_work)->work)",
                   & __key___3, 0);
  INIT_LIST_HEAD(& efx->selftest_work.work.entry);
  efx->selftest_work.work.func = & efx_selftest_async_work;
  init_timer_key(& efx->selftest_work.timer, 2U, "(&(&efx->selftest_work)->timer)",
                 & __key___4);
  efx->selftest_work.timer.function = & delayed_work_timer_fn;
  efx->selftest_work.timer.data = (unsigned long )(& efx->selftest_work);
  efx->pci_dev = pci_dev;
  efx->msg_enable = debug;
  efx->state = 0;
  tmp = pci_name((struct pci_dev const *)pci_dev);
  strlcpy((char *)(& efx->name), tmp, 16UL);
  efx->net_dev = net_dev;
  spinlock_check(& efx->stats_lock);
  __raw_spin_lock_init(& efx->stats_lock.ldv_5961.rlock, "&(&efx->stats_lock)->rlock",
                       & __key___5);
  __mutex_init(& efx->mac_lock, "&efx->mac_lock", & __key___6);
  efx->phy_op = & efx_dummy_phy_operations;
  efx->mdio.dev = net_dev;
  __init_work(& efx->mac_work, 0);
  __constr_expr_3.counter = 4195328L;
  efx->mac_work.data = __constr_expr_3;
  lockdep_init_map(& efx->mac_work.lockdep_map, "(&efx->mac_work)", & __key___7, 0);
  INIT_LIST_HEAD(& efx->mac_work.entry);
  efx->mac_work.func = & efx_mac_work;
  __init_waitqueue_head(& efx->flush_wq, "&efx->flush_wq", & __key___8);
  i = 0;
  goto ldv_47435;
  ldv_47434:
  efx->channel[i] = efx_alloc_channel(efx, i, 0);
  if ((unsigned long )efx->channel[i] == (unsigned long )((struct efx_channel *)0)) {
    goto fail;
  } else {
  }
  i = i + 1;
  ldv_47435: ;
  if ((unsigned int )i <= 31U) {
    goto ldv_47434;
  } else {
  }
  _max1 = (efx->type)->max_interrupt_mode;
  _max2 = interrupt_mode;
  efx->interrupt_mode = (enum efx_int_mode )(_max1 > (unsigned int )((unsigned int const )_max2) ? _max1 : (unsigned int const )_max2);
  tmp___0 = pci_name((struct pci_dev const *)pci_dev);
  snprintf((char *)(& efx->workqueue_name), 16UL, "sfc%s", tmp___0);
  __lock_name = "(efx->workqueue_name)";
  tmp___1 = __alloc_workqueue_key((char const *)(& efx->workqueue_name), 10U, 1,
                                  & __key___9, __lock_name);
  efx->workqueue = tmp___1;
  if ((unsigned long )efx->workqueue == (unsigned long )((struct workqueue_struct *)0)) {
    goto fail;
  } else {
  }
  return (0);
  fail:
  efx_fini_struct(efx);
  return (-12);
}
}
static void efx_fini_struct(struct efx_nic *efx )
{
  int i ;
  {
  i = 0;
  goto ldv_47448;
  ldv_47447:
  kfree((void const *)efx->channel[i]);
  i = i + 1;
  ldv_47448: ;
  if ((unsigned int )i <= 31U) {
    goto ldv_47447;
  } else {
  }
  if ((unsigned long )efx->workqueue != (unsigned long )((struct workqueue_struct *)0)) {
    destroy_workqueue(efx->workqueue);
    efx->workqueue = 0;
  } else {
  }
  return;
}
}
static void efx_pci_remove_main(struct efx_nic *efx )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((unsigned int )efx->state == 1U, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared"),
                         "i" (2607), "i" (12UL));
    ldv_47453: ;
    goto ldv_47453;
  } else {
  }
  cancel_work_sync(& efx->reset_work);
  free_irq_cpu_rmap((efx->net_dev)->rx_cpu_rmap);
  (efx->net_dev)->rx_cpu_rmap = 0;
  efx_stop_interrupts(efx, 0);
  efx_nic_fini_interrupt(efx);
  efx_fini_port(efx);
  (*((efx->type)->fini))(efx);
  efx_fini_napi(efx);
  efx_remove_all(efx);
  return;
}
}
static void efx_pci_remove(struct pci_dev *pci_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  tmp = pci_get_drvdata(pci_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned long )efx == (unsigned long )((struct efx_nic *)0)) {
    return;
  } else {
  }
  rtnl_lock();
  dev_close(efx->net_dev);
  efx_stop_interrupts(efx, 0);
  rtnl_unlock();
  efx_sriov_fini(efx);
  efx_unregister_netdev(efx);
  efx_mtd_remove(efx);
  efx_pci_remove_main(efx);
  efx_fini_io(efx);
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_pci_remove";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "shutdown successful\n";
    descriptor.lineno = 2647U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "shutdown successful\n");
    } else {
    }
  } else {
  }
  efx_fini_struct(efx);
  pci_set_drvdata(pci_dev, 0);
  free_netdev(efx->net_dev);
  return;
}
}
static void efx_print_product_vpd(struct efx_nic *efx )
{
  struct pci_dev *dev ;
  char vpd_data[512U] ;
  ssize_t vpd_size ;
  int i ;
  int j ;
  u16 tmp ;
  u8 tmp___0 ;
  {
  dev = efx->pci_dev;
  vpd_size = pci_read_vpd(dev, 0LL, 512UL, (void *)(& vpd_data));
  if (vpd_size <= 0L) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "Unable to read VPD\n");
    } else {
    }
    return;
  } else {
  }
  i = pci_vpd_find_tag((u8 const *)(& vpd_data), 0U, (unsigned int )vpd_size, 144);
  if (i < 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "VPD Read-only not found\n");
    } else {
    }
    return;
  } else {
  }
  tmp = pci_vpd_lrdt_size((u8 const *)(& vpd_data) + (unsigned long )i);
  j = (int )tmp;
  i = i + 3;
  if ((ssize_t )(i + j) > vpd_size) {
    j = (int )((unsigned int )vpd_size - (unsigned int )i);
  } else {
  }
  i = pci_vpd_find_info_keyword((u8 const *)(& vpd_data), (unsigned int )i, (unsigned int )j,
                                "PN");
  if (i < 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "Part number not found\n");
    } else {
    }
    return;
  } else {
  }
  tmp___0 = pci_vpd_info_field_size((u8 const *)(& vpd_data) + (unsigned long )i);
  j = (int )tmp___0;
  i = i + 3;
  if ((ssize_t )(i + j) > vpd_size) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "Incomplete part number\n");
    } else {
    }
    return;
  } else {
  }
  if ((int )efx->msg_enable & 1) {
    netdev_info((struct net_device const *)efx->net_dev, "Part Number : %.*s\n",
                j, (char *)(& vpd_data) + (unsigned long )i);
  } else {
  }
  return;
}
}
static int efx_pci_probe_main(struct efx_nic *efx )
{
  int rc ;
  {
  rc = efx_probe_all(efx);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  efx_init_napi(efx);
  rc = (*((efx->type)->init))(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to initialise NIC\n");
    } else {
    }
    goto fail3;
  } else {
  }
  rc = efx_init_port(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to initialise port\n");
    } else {
    }
    goto fail4;
  } else {
  }
  rc = efx_nic_init_interrupt(efx);
  if (rc != 0) {
    goto fail5;
  } else {
  }
  efx_start_interrupts(efx, 0);
  return (0);
  fail5:
  efx_fini_port(efx);
  fail4:
  (*((efx->type)->fini))(efx);
  fail3:
  efx_fini_napi(efx);
  efx_remove_all(efx);
  fail1: ;
  return (rc);
}
}
static int efx_pci_probe(struct pci_dev *pci_dev , struct pci_device_id const *entry )
{
  struct net_device *net_dev ;
  struct efx_nic *efx ;
  int rc ;
  void *tmp ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  int __ret_warn_on ;
  long tmp___1 ;
  struct _ddebug descriptor___0 ;
  long tmp___2 ;
  {
  net_dev = alloc_etherdev_mqs(3904, 64U, 32U);
  if ((unsigned long )net_dev == (unsigned long )((struct net_device *)0)) {
    return (-12);
  } else {
  }
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  efx->type = (struct efx_nic_type const *)entry->driver_data;
  net_dev->features = (net_dev->features | (unsigned long long )(efx->type)->offload_features) | 536936481ULL;
  if (((unsigned long long )(efx->type)->offload_features & 24ULL) != 0ULL) {
    net_dev->features = net_dev->features | 1048576ULL;
  } else {
  }
  net_dev->vlan_features = net_dev->vlan_features | 538509371ULL;
  net_dev->hw_features = net_dev->features & 0xffffffffffffffdfULL;
  pci_set_drvdata(pci_dev, (void *)efx);
  net_dev->dev.parent = & pci_dev->dev;
  rc = efx_init_struct(efx, pci_dev, net_dev);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "Solarflare NIC detected\n");
  } else {
  }
  efx_print_product_vpd(efx);
  rc = efx_init_io(efx);
  if (rc != 0) {
    goto fail2;
  } else {
  }
  rc = efx_pci_probe_main(efx);
  if (rc != 0) {
    goto fail3;
  } else {
  }
  rc = efx_register_netdev(efx);
  if (rc != 0) {
    goto fail4;
  } else {
  }
  rc = efx_sriov_init(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "SR-IOV can\'t be enabled rc %d\n",
                 rc);
    } else {
    }
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_pci_probe";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor.format = "initialisation successful\n";
    descriptor.lineno = 2814U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "initialisation successful\n");
    } else {
    }
  } else {
  }
  rtnl_lock();
  rc = efx_mtd_probe(efx);
  rtnl_unlock();
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_warn((struct net_device const *)efx->net_dev, "failed to create MTDs (%d)\n",
                  rc);
    } else {
    }
  } else {
  }
  return (0);
  fail4:
  efx_pci_remove_main(efx);
  fail3:
  efx_fini_io(efx);
  fail2:
  efx_fini_struct(efx);
  fail1:
  pci_set_drvdata(pci_dev, 0);
  __ret_warn_on = rc > 0;
  tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared",
                       2834);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if ((int )efx->msg_enable & 1) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "efx_pci_probe";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/efx.c.prepared";
    descriptor___0.format = "initialisation failed. rc=%d\n";
    descriptor___0.lineno = 2835U;
    descriptor___0.flags = 0U;
    tmp___2 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___2 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "initialisation failed. rc=%d\n", rc);
    } else {
    }
  } else {
  }
  free_netdev(net_dev);
  return (rc);
}
}
static int efx_pm_freeze(struct device *dev )
{
  struct efx_nic *efx ;
  struct device const *__mptr ;
  void *tmp ;
  {
  __mptr = (struct device const *)dev;
  tmp = pci_get_drvdata((struct pci_dev *)__mptr + 0xffffffffffffff68UL);
  efx = (struct efx_nic *)tmp;
  rtnl_lock();
  if ((unsigned int )efx->state != 2U) {
    efx->state = 0;
    efx_device_detach_sync(efx);
    efx_stop_all(efx);
    efx_stop_interrupts(efx, 0);
  } else {
  }
  rtnl_unlock();
  return (0);
}
}
static int efx_pm_thaw(struct device *dev )
{
  struct efx_nic *efx ;
  struct device const *__mptr ;
  void *tmp ;
  {
  __mptr = (struct device const *)dev;
  tmp = pci_get_drvdata((struct pci_dev *)__mptr + 0xffffffffffffff68UL);
  efx = (struct efx_nic *)tmp;
  rtnl_lock();
  if ((unsigned int )efx->state != 2U) {
    efx_start_interrupts(efx, 0);
    ldv_mutex_lock_31(& efx->mac_lock);
    (*((efx->phy_op)->reconfigure))(efx);
    ldv_mutex_unlock_32(& efx->mac_lock);
    efx_start_all(efx);
    netif_device_attach(efx->net_dev);
    efx->state = 1;
    (*((efx->type)->resume_wol))(efx);
  } else {
  }
  rtnl_unlock();
  queue_work(reset_workqueue, & efx->reset_work);
  return (0);
}
}
static int efx_pm_poweroff(struct device *dev )
{
  struct pci_dev *pci_dev ;
  struct device const *__mptr ;
  struct efx_nic *efx ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  pci_dev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pci_dev);
  efx = (struct efx_nic *)tmp;
  (*((efx->type)->fini))(efx);
  efx->reset_pending = 0UL;
  pci_save_state(pci_dev);
  tmp___0 = pci_set_power_state(pci_dev, 3);
  return (tmp___0);
}
}
static int efx_pm_resume(struct device *dev )
{
  struct pci_dev *pci_dev ;
  struct device const *__mptr ;
  struct efx_nic *efx ;
  void *tmp ;
  int rc ;
  {
  __mptr = (struct device const *)dev;
  pci_dev = (struct pci_dev *)__mptr + 0xffffffffffffff68UL;
  tmp = pci_get_drvdata(pci_dev);
  efx = (struct efx_nic *)tmp;
  rc = pci_set_power_state(pci_dev, 0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  pci_restore_state(pci_dev);
  rc = pci_enable_device(pci_dev);
  if (rc != 0) {
    return (rc);
  } else {
  }
  pci_set_master(efx->pci_dev);
  rc = (*((efx->type)->reset))(efx, 1);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = (*((efx->type)->init))(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  efx_pm_thaw(dev);
  return (0);
}
}
static int efx_pm_suspend(struct device *dev )
{
  int rc ;
  {
  efx_pm_freeze(dev);
  rc = efx_pm_poweroff(dev);
  if (rc != 0) {
    efx_pm_resume(dev);
  } else {
  }
  return (rc);
}
}
static struct dev_pm_ops const efx_pm_ops =
     {0, 0, & efx_pm_suspend, & efx_pm_resume, & efx_pm_freeze, & efx_pm_thaw, & efx_pm_poweroff,
    & efx_pm_resume, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};
static struct pci_driver efx_pci_driver =
     {{0, 0}, "sfc", (struct pci_device_id const *)(& efx_pci_table), & efx_pci_probe,
    & efx_pci_remove, 0, 0, 0, 0, 0, 0, 0, {0, 0, 0, 0, (_Bool)0, 0, 0, 0, 0, 0, 0,
                                            0, 0, & efx_pm_ops, 0}, {{{{{{0U}}, 0U,
                                                                        0U, 0, {0,
                                                                                {0,
                                                                                 0},
                                                                                0,
                                                                                0,
                                                                                0UL}}}},
                                                                     {0, 0}}};
static int efx_init_module(void)
{
  int rc ;
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp ;
  {
  printk("\016Solarflare NET driver v3.2\n");
  rc = register_netdevice_notifier(& efx_netdev_notifier);
  if (rc != 0) {
    goto err_notifier;
  } else {
  }
  rc = efx_init_sriov();
  if (rc != 0) {
    goto err_sriov;
  } else {
  }
  __lock_name = "sfc_reset";
  tmp = __alloc_workqueue_key("sfc_reset", 10U, 1, & __key, __lock_name);
  reset_workqueue = tmp;
  if ((unsigned long )reset_workqueue == (unsigned long )((struct workqueue_struct *)0)) {
    rc = -12;
    goto err_reset;
  } else {
  }
  rc = __pci_register_driver(& efx_pci_driver, & __this_module, "sfc");
  if (rc < 0) {
    goto err_pci;
  } else {
  }
  return (0);
  err_pci:
  destroy_workqueue(reset_workqueue);
  err_reset:
  efx_fini_sriov();
  err_sriov:
  unregister_netdevice_notifier(& efx_netdev_notifier);
  err_notifier: ;
  return (rc);
}
}
static void efx_exit_module(void)
{
  {
  printk("\016Solarflare NET driver unloading\n");
  pci_unregister_driver(& efx_pci_driver);
  destroy_workqueue(reset_workqueue);
  efx_fini_sriov();
  unregister_netdevice_notifier(& efx_netdev_notifier);
  return;
}
}
struct pci_device_id const __mod_pci_device_table ;
void ldv_check_final_state(void) ;
extern void ldv_check_return_value(int ) ;
extern void ldv_check_return_value_probe(int ) ;
void ldv_initialize(void) ;
extern void ldv_handler_precall(void) ;
extern int __VERIFIER_nondet_int(void) ;
int LDV_IN_INTERRUPT ;
void ldv_main0_sequence_infinite_withcheck_stateful(void)
{
  struct efx_channel *var_group1 ;
  char *var_efx_get_channel_name_14_p1 ;
  size_t var_efx_get_channel_name_14_p2 ;
  struct efx_channel const *var_efx_copy_channel_12_p0 ;
  struct net_device *var_group2 ;
  int res_efx_net_open_63 ;
  int res_efx_net_stop_64 ;
  struct rtnl_link_stats64 *var_group3 ;
  struct ifreq *var_group4 ;
  int var_efx_ioctl_57_p2 ;
  int var_efx_change_mtu_67_p1 ;
  void *var_efx_set_mac_address_68_p1 ;
  netdev_features_t var_efx_set_features_70_p1 ;
  struct notifier_block *var_group5 ;
  unsigned long var_efx_netdev_event_72_p1 ;
  void *var_efx_netdev_event_72_p2 ;
  struct efx_nic *var_group6 ;
  struct device *var_group7 ;
  struct pci_dev *var_group8 ;
  struct pci_device_id const *var_efx_pci_probe_90_p1 ;
  int res_efx_pci_probe_90 ;
  int ldv_s_efx_netdev_ops_net_device_ops ;
  int ldv_s_efx_pci_driver_pci_driver ;
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  ldv_s_efx_netdev_ops_net_device_ops = 0;
  ldv_s_efx_pci_driver_pci_driver = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  ldv_handler_precall();
  tmp = efx_init_module();
  if (tmp != 0) {
    goto ldv_final;
  } else {
  }
  goto ldv_47634;
  ldv_47633:
  tmp___0 = __VERIFIER_nondet_int();
  switch (tmp___0) {
  case 0:
  ldv_handler_precall();
  efx_channel_dummy_op_int(var_group1);
  goto ldv_47606;
  case 1:
  ldv_handler_precall();
  efx_channel_dummy_op_void(var_group1);
  goto ldv_47606;
  case 2:
  ldv_handler_precall();
  efx_get_channel_name(var_group1, var_efx_get_channel_name_14_p1, var_efx_get_channel_name_14_p2);
  goto ldv_47606;
  case 3:
  ldv_handler_precall();
  efx_copy_channel(var_efx_copy_channel_12_p0);
  goto ldv_47606;
  case 4: ;
  if (ldv_s_efx_netdev_ops_net_device_ops == 0) {
    ldv_handler_precall();
    res_efx_net_open_63 = efx_net_open(var_group2);
    ldv_check_return_value(res_efx_net_open_63);
    if (res_efx_net_open_63 < 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_efx_netdev_ops_net_device_ops = ldv_s_efx_netdev_ops_net_device_ops + 1;
  } else {
  }
  goto ldv_47606;
  case 5: ;
  if (ldv_s_efx_netdev_ops_net_device_ops == 1) {
    ldv_handler_precall();
    res_efx_net_stop_64 = efx_net_stop(var_group2);
    ldv_check_return_value(res_efx_net_stop_64);
    if (res_efx_net_stop_64 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_efx_netdev_ops_net_device_ops = 0;
  } else {
  }
  goto ldv_47606;
  case 6:
  ldv_handler_precall();
  efx_net_stats(var_group2, var_group3);
  goto ldv_47606;
  case 7:
  ldv_handler_precall();
  efx_watchdog(var_group2);
  goto ldv_47606;
  case 8:
  ldv_handler_precall();
  efx_ioctl(var_group2, var_group4, var_efx_ioctl_57_p2);
  goto ldv_47606;
  case 9:
  ldv_handler_precall();
  efx_change_mtu(var_group2, var_efx_change_mtu_67_p1);
  goto ldv_47606;
  case 10:
  ldv_handler_precall();
  efx_set_mac_address(var_group2, var_efx_set_mac_address_68_p1);
  goto ldv_47606;
  case 11:
  ldv_handler_precall();
  efx_set_rx_mode(var_group2);
  goto ldv_47606;
  case 12:
  ldv_handler_precall();
  efx_set_features(var_group2, var_efx_set_features_70_p1);
  goto ldv_47606;
  case 13:
  ldv_handler_precall();
  efx_netpoll(var_group2);
  goto ldv_47606;
  case 14:
  ldv_handler_precall();
  efx_netdev_event(var_group5, var_efx_netdev_event_72_p1, var_efx_netdev_event_72_p2);
  goto ldv_47606;
  case 15:
  ldv_handler_precall();
  efx_port_dummy_op_int(var_group6);
  goto ldv_47606;
  case 16:
  ldv_handler_precall();
  efx_port_dummy_op_poll(var_group6);
  goto ldv_47606;
  case 17:
  ldv_handler_precall();
  efx_port_dummy_op_void(var_group6);
  goto ldv_47606;
  case 18:
  ldv_handler_precall();
  efx_pm_suspend(var_group7);
  goto ldv_47606;
  case 19:
  ldv_handler_precall();
  efx_pm_resume(var_group7);
  goto ldv_47606;
  case 20:
  ldv_handler_precall();
  efx_pm_freeze(var_group7);
  goto ldv_47606;
  case 21:
  ldv_handler_precall();
  efx_pm_thaw(var_group7);
  goto ldv_47606;
  case 22:
  ldv_handler_precall();
  efx_pm_poweroff(var_group7);
  goto ldv_47606;
  case 23: ;
  if (ldv_s_efx_pci_driver_pci_driver == 0) {
    res_efx_pci_probe_90 = efx_pci_probe(var_group8, var_efx_pci_probe_90_p1);
    ldv_check_return_value(res_efx_pci_probe_90);
    ldv_check_return_value_probe(res_efx_pci_probe_90);
    if (res_efx_pci_probe_90 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_efx_pci_driver_pci_driver = ldv_s_efx_pci_driver_pci_driver + 1;
  } else {
  }
  goto ldv_47606;
  case 24: ;
  if (ldv_s_efx_pci_driver_pci_driver == 1) {
    ldv_handler_precall();
    efx_pci_remove(var_group8);
    ldv_s_efx_pci_driver_pci_driver = 0;
  } else {
  }
  goto ldv_47606;
  default: ;
  goto ldv_47606;
  }
  ldv_47606: ;
  ldv_47634:
  tmp___1 = __VERIFIER_nondet_int();
  if ((tmp___1 != 0 || ldv_s_efx_netdev_ops_net_device_ops != 0) || ldv_s_efx_pci_driver_pci_driver != 0) {
    goto ldv_47633;
  } else {
  }
  ldv_module_exit:
  ldv_handler_precall();
  efx_exit_module();
  ldv_final:
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_1(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_2(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_3(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_4(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_5(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_6(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_7(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int ldv_mutex_is_locked_8(struct mutex *lock )
{
  ldv_func_ret_type___6 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_is_locked(lock);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_is_locked_mac_lock(lock);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_lock_9(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_10(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_11(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_12(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_13(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_14(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_15(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_16(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_17(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_18(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_19(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_20(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_21(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_22(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___20 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mac_lock(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_23(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_24(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_25(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_26(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_27(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_28(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_29(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_30(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_31(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_32(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static void __clear_bit(int nr , unsigned long volatile *addr )
{
  {
  __asm__ volatile ("btr %1,%0": "+m" (*((long volatile *)addr)): "Ir" (nr));
  return;
}
}
__inline static unsigned long __ffs(unsigned long word )
{
  {
  __asm__ ("rep; bsf %1,%0": "=r" (word): "rm" (word));
  return (word);
}
}
__inline static void __clear_bit_le(int nr , void *addr )
{
  {
  __clear_bit(nr, (unsigned long volatile *)addr);
  return;
}
}
extern unsigned long __phys_addr(unsigned long ) ;
extern struct pv_irq_ops pv_irq_ops ;
extern struct task_struct *current_task ;
__inline static struct task_struct *get_current(void)
{
  struct task_struct *pfo_ret__ ;
  {
  switch (8UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "p" (& current_task));
  goto ldv_2861;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_2861;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_2861;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "p" (& current_task));
  goto ldv_2861;
  default:
  __bad_percpu_size();
  }
  ldv_2861: ;
  return (pfo_ret__);
}
}
__inline static unsigned long arch_local_save_flags(void)
{
  unsigned long __ret ;
  unsigned long __edi ;
  unsigned long __esi ;
  unsigned long __edx ;
  unsigned long __ecx ;
  unsigned long __eax ;
  long tmp ;
  {
  __edi = __edi;
  __esi = __esi;
  __edx = __edx;
  __ecx = __ecx;
  __eax = __eax;
  tmp = ldv__builtin_expect((unsigned long )pv_irq_ops.save_fl.func == (unsigned long )((void *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/inst/current/envs/linux-3.8-rc1/linux-3.8-rc1/arch/x86/include/asm/paravirt.h"),
                         "i" (825), "i" (12UL));
    ldv_4725: ;
    goto ldv_4725;
  } else {
  }
  __asm__ volatile ("771:\n\tcall *%c2;\n772:\n.pushsection .parainstructions,\"a\"\n .balign 8 \n .quad  771b\n  .byte %c1\n  .byte 772b-771b\n  .short %c3\n.popsection\n": "=a" (__eax): [paravirt_typenum] "i" (45UL),
                       [paravirt_opptr] "i" (& pv_irq_ops.save_fl.func), [paravirt_clobber] "i" (1): "memory",
                       "cc");
  __ret = __eax;
  return (__ret);
}
}
__inline static int arch_irqs_disabled_flags(unsigned long flags )
{
  {
  return ((flags & 512UL) == 0UL);
}
}
extern void __cmpxchg_wrong_size(void) ;
__inline static void atomic_set(atomic_t *v , int i )
{
  {
  v->counter = i;
  return;
}
}
__inline static void atomic_inc(atomic_t *v )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; incl %0": "+m" (v->counter));
  return;
}
}
__inline static void atomic_dec(atomic_t *v )
{
  {
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; decl %0": "+m" (v->counter));
  return;
}
}
__inline static int atomic_cmpxchg(atomic_t *v , int old , int new )
{
  int __ret ;
  int __old ;
  int __new ;
  u8 volatile *__ptr ;
  u16 volatile *__ptr___0 ;
  u32 volatile *__ptr___1 ;
  u64 volatile *__ptr___2 ;
  {
  __old = old;
  __new = new;
  switch (4UL) {
  case 1UL:
  __ptr = (u8 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgb %2,%1": "=a" (__ret),
                       "+m" (*__ptr): "q" (__new), "0" (__old): "memory");
  goto ldv_5494;
  case 2UL:
  __ptr___0 = (u16 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgw %2,%1": "=a" (__ret),
                       "+m" (*__ptr___0): "r" (__new), "0" (__old): "memory");
  goto ldv_5494;
  case 4UL:
  __ptr___1 = (u32 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgl %2,%1": "=a" (__ret),
                       "+m" (*__ptr___1): "r" (__new), "0" (__old): "memory");
  goto ldv_5494;
  case 8UL:
  __ptr___2 = (u64 volatile *)(& v->counter);
  __asm__ volatile (".pushsection .smp_locks,\"a\"\n.balign 4\n.long 671f - .\n.popsection\n671:\n\tlock; cmpxchgq %2,%1": "=a" (__ret),
                       "+m" (*__ptr___2): "r" (__new), "0" (__old): "memory");
  goto ldv_5494;
  default:
  __cmpxchg_wrong_size();
  }
  ldv_5494: ;
  return (__ret);
}
}
int ldv_mutex_trylock_68(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_66(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_69(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_71(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_65(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_67(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_70(struct mutex *ldv_func_arg1 ) ;
extern unsigned long _raw_spin_lock_irqsave(raw_spinlock_t * ) ;
extern void _raw_spin_unlock_irqrestore(raw_spinlock_t * , unsigned long ) ;
__inline static void spin_unlock_irqrestore(spinlock_t *lock , unsigned long flags )
{
  {
  _raw_spin_unlock_irqrestore(& lock->ldv_5961.rlock, flags);
  return;
}
}
extern void __wake_up(wait_queue_head_t * , unsigned int , int , void * ) ;
extern void prepare_to_wait(wait_queue_head_t * , wait_queue_t * , int ) ;
extern void finish_wait(wait_queue_head_t * , wait_queue_t * ) ;
extern int autoremove_wake_function(wait_queue_t * , unsigned int , int , void * ) ;
__inline static unsigned int __readl(void const volatile *addr )
{
  unsigned int ret ;
  {
  __asm__ volatile ("movl %1,%0": "=r" (ret): "m" (*((unsigned int volatile *)addr)));
  return (ret);
}
}
__inline static void __writel(unsigned int val , void volatile *addr )
{
  {
  __asm__ volatile ("movl %0,%1": : "r" (val), "m" (*((unsigned int volatile *)addr)));
  return;
}
}
__inline static unsigned long readq(void const volatile *addr )
{
  unsigned long ret ;
  {
  __asm__ volatile ("movq %1,%0": "=r" (ret): "m" (*((unsigned long volatile *)addr)): "memory");
  return (ret);
}
}
__inline static void writeq(unsigned long val , void volatile *addr )
{
  {
  __asm__ volatile ("movq %0,%1": : "r" (val), "m" (*((unsigned long volatile *)addr)): "memory");
  return;
}
}
__inline static phys_addr_t virt_to_phys(void volatile *address )
{
  unsigned long tmp ;
  {
  tmp = __phys_addr((unsigned long )address);
  return ((phys_addr_t )tmp);
}
}
extern int request_threaded_irq(unsigned int , irqreturn_t (*)(int , void * ) ,
                                irqreturn_t (*)(int , void * ) , unsigned long ,
                                char const * , void * ) ;
__inline static int request_irq(unsigned int irq , irqreturn_t (*handler)(int , void * ) ,
                                unsigned long flags , char const *name , void *dev )
{
  int tmp ;
  {
  tmp = request_threaded_irq(irq, handler, 0, flags, name, dev);
  return (tmp);
}
}
extern void free_irq(unsigned int , void * ) ;
extern void pci_clear_master(struct pci_dev * ) ;
__inline static int is_device_dma_capable(struct device *dev )
{
  {
  return ((unsigned long )dev->dma_mask != (unsigned long )((u64 *)0) && *(dev->dma_mask) != 0ULL);
}
}
extern void debug_dma_alloc_coherent(struct device * , size_t , dma_addr_t , void * ) ;
extern void debug_dma_free_coherent(struct device * , size_t , void * , dma_addr_t ) ;
extern struct device x86_dma_fallback_dev ;
extern struct dma_map_ops *dma_ops ;
__inline static struct dma_map_ops *get_dma_ops(struct device *dev )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((unsigned long )dev == (unsigned long )((struct device *)0),
                         0L);
  if (tmp != 0L || (unsigned long )dev->archdata.dma_ops == (unsigned long )((struct dma_map_ops *)0)) {
    return (dma_ops);
  } else {
    return (dev->archdata.dma_ops);
  }
}
}
__inline static unsigned long dma_alloc_coherent_mask(struct device *dev , gfp_t gfp )
{
  unsigned long dma_mask ;
  {
  dma_mask = 0UL;
  dma_mask = (unsigned long )dev->coherent_dma_mask;
  if (dma_mask == 0UL) {
    dma_mask = (int )gfp & 1 ? 16777215UL : 4294967295UL;
  } else {
  }
  return (dma_mask);
}
}
__inline static gfp_t dma_alloc_coherent_gfp_flags(struct device *dev , gfp_t gfp )
{
  unsigned long dma_mask ;
  unsigned long tmp ;
  {
  tmp = dma_alloc_coherent_mask(dev, gfp);
  dma_mask = tmp;
  if ((unsigned long long )dma_mask <= 16777215ULL) {
    gfp = gfp | 1U;
  } else {
  }
  if ((unsigned long long )dma_mask <= 4294967295ULL && (gfp & 1U) == 0U) {
    gfp = gfp | 4U;
  } else {
  }
  return (gfp);
}
}
__inline static void *dma_alloc_attrs(struct device *dev , size_t size , dma_addr_t *dma_handle ,
                                      gfp_t gfp , struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  void *memory ;
  int tmp___0 ;
  gfp_t tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  gfp = gfp & 4294967288U;
  if ((unsigned long )dev == (unsigned long )((struct device *)0)) {
    dev = & x86_dma_fallback_dev;
  } else {
  }
  tmp___0 = is_device_dma_capable(dev);
  if (tmp___0 == 0) {
    return (0);
  } else {
  }
  if ((unsigned long )ops->alloc == (unsigned long )((void *(*)(struct device * ,
                                                                size_t , dma_addr_t * ,
                                                                gfp_t , struct dma_attrs * ))0)) {
    return (0);
  } else {
  }
  tmp___1 = dma_alloc_coherent_gfp_flags(dev, gfp);
  memory = (*(ops->alloc))(dev, size, dma_handle, tmp___1, attrs);
  debug_dma_alloc_coherent(dev, size, *dma_handle, memory);
  return (memory);
}
}
__inline static void dma_free_attrs(struct device *dev , size_t size , void *vaddr ,
                                    dma_addr_t bus , struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int __ret_warn_on ;
  unsigned long _flags ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  _flags = arch_local_save_flags();
  tmp___0 = arch_irqs_disabled_flags(_flags);
  __ret_warn_on = tmp___0 != 0;
  tmp___1 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/inst/current/envs/linux-3.8-rc1/linux-3.8-rc1/arch/x86/include/asm/dma-mapping.h",
                       166);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  debug_dma_free_coherent(dev, size, vaddr, bus);
  if ((unsigned long )ops->free != (unsigned long )((void (*)(struct device * , size_t ,
                                                              void * , dma_addr_t ,
                                                              struct dma_attrs * ))0)) {
    (*(ops->free))(dev, size, vaddr, bus, attrs);
  } else {
  }
  return;
}
}
extern long schedule_timeout(long ) ;
__inline static void netif_tx_lock___0(struct net_device *dev )
{
  unsigned int i ;
  int cpu ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  spin_lock(& dev->tx_global_lock);
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_39543;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39543;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39543;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_39543;
  default:
  __bad_percpu_size();
  }
  ldv_39543:
  pscr_ret__ = pfo_ret__;
  goto ldv_39549;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39553;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39553;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39553;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_39553;
  default:
  __bad_percpu_size();
  }
  ldv_39553:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_39549;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39562;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39562;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39562;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_39562;
  default:
  __bad_percpu_size();
  }
  ldv_39562:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_39549;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39571;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39571;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39571;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_39571;
  default:
  __bad_percpu_size();
  }
  ldv_39571:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_39549;
  default:
  __bad_size_call_parameter();
  goto ldv_39549;
  }
  ldv_39549:
  cpu = pscr_ret__;
  i = 0U;
  goto ldv_39581;
  ldv_39580:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  __netif_tx_lock(txq, cpu);
  set_bit(2U, (unsigned long volatile *)(& txq->state));
  __netif_tx_unlock(txq);
  i = i + 1U;
  ldv_39581: ;
  if (dev->num_tx_queues > i) {
    goto ldv_39580;
  } else {
  }
  return;
}
}
__inline static void netif_tx_unlock___0(struct net_device *dev )
{
  unsigned int i ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  i = 0U;
  goto ldv_39592;
  ldv_39591:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  clear_bit(2, (unsigned long volatile *)(& txq->state));
  netif_schedule_queue(txq);
  i = i + 1U;
  ldv_39592: ;
  if (dev->num_tx_queues > i) {
    goto ldv_39591;
  } else {
  }
  spin_unlock(& dev->tx_global_lock);
  return;
}
}
__inline static struct efx_tx_queue *efx_get_tx_queue(struct efx_nic *efx , unsigned int index ,
                                                      unsigned int type )
{
  {
  return ((struct efx_tx_queue *)(& (efx->channel[efx->tx_channel_offset + index])->tx_queue) + (unsigned long )type);
}
}
__inline static struct efx_tx_queue *efx_channel_get_tx_queue(struct efx_channel *channel ,
                                                              unsigned int type )
{
  {
  return ((struct efx_tx_queue *)(& channel->tx_queue) + (unsigned long )type);
}
}
__inline static struct efx_channel *efx_rx_queue_channel(struct efx_rx_queue *rx_queue )
{
  struct efx_rx_queue const *__mptr ;
  {
  __mptr = (struct efx_rx_queue const *)rx_queue;
  return ((struct efx_channel *)__mptr + 0xfffffffffffffec0UL);
}
}
__inline static int efx_rx_queue_index(struct efx_rx_queue *rx_queue )
{
  struct efx_channel *tmp ;
  {
  tmp = efx_rx_queue_channel(rx_queue);
  return (tmp->channel);
}
}
__inline static struct efx_rx_buffer *efx_rx_buffer(struct efx_rx_queue *rx_queue ,
                                                    unsigned int index )
{
  {
  return (rx_queue->buffer + (unsigned long )index);
}
}
void efx_xmit_done(struct efx_tx_queue *tx_queue , unsigned int index ) ;
void efx_rx_packet(struct efx_rx_queue *rx_queue , unsigned int index , unsigned int len ,
                   u16 flags ) ;
__inline static void efx_schedule_channel___0(struct efx_channel *channel )
{
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  {
  if (0) {
    if (((channel->efx)->msg_enable & 512U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_42090;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_42090;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_42090;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_42090;
      default:
      __bad_percpu_size();
      }
      ldv_42090:
      pscr_ret__ = pfo_ret__;
      goto ldv_42096;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_42100;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_42100;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_42100;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_42100;
      default:
      __bad_percpu_size();
      }
      ldv_42100:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_42096;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_42109;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_42109;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_42109;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_42109;
      default:
      __bad_percpu_size();
      }
      ldv_42109:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_42096;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_42118;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_42118;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_42118;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_42118;
      default:
      __bad_percpu_size();
      }
      ldv_42118:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_42096;
      default:
      __bad_size_call_parameter();
      goto ldv_42096;
      }
      ldv_42096:
      netdev_printk("\017", (struct net_device const *)(channel->efx)->net_dev,
                    "channel %d scheduling NAPI poll on CPU%d\n", channel->channel,
                    pscr_ret__);
    } else {
    }
  } else {
  }
  channel->work_pending = 1;
  napi_schedule(& channel->napi_str);
  return;
}
}
__inline static void efx_schedule_channel_irq(struct efx_channel *channel )
{
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  {
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_42135;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_42135;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_42135;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_42135;
  default:
  __bad_percpu_size();
  }
  ldv_42135:
  pscr_ret__ = pfo_ret__;
  goto ldv_42141;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_42145;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_42145;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_42145;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_42145;
  default:
  __bad_percpu_size();
  }
  ldv_42145:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_42141;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_42154;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_42154;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_42154;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_42154;
  default:
  __bad_percpu_size();
  }
  ldv_42154:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_42141;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_42163;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_42163;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_42163;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_42163;
  default:
  __bad_percpu_size();
  }
  ldv_42163:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_42141;
  default:
  __bad_size_call_parameter();
  goto ldv_42141;
  }
  ldv_42141:
  channel->event_test_cpu = pscr_ret__;
  efx_schedule_channel___0(channel);
  return;
}
}
void efx_mcdi_process_event(struct efx_channel *channel , efx_qword_t *event ) ;
int efx_mcdi_flush_rxqs(struct efx_nic *efx ) ;
u32 efx_nic_fpga_ver(struct efx_nic *efx ) ;
__inline static bool efx_nic_is_dual_func(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_nic_rev(efx);
  return (tmp <= 1);
}
}
__inline static bool efx_sriov_enabled(struct efx_nic *efx )
{
  {
  return (efx->vf_init_count != 0U);
}
}
void efx_sriov_tx_flush_done(struct efx_nic *efx , efx_qword_t *event ) ;
void efx_sriov_rx_flush_done(struct efx_nic *efx , efx_qword_t *event ) ;
void efx_sriov_event(struct efx_channel *channel , efx_qword_t *event ) ;
void efx_sriov_desc_fetch_err(struct efx_nic *efx , unsigned int dmaq ) ;
int efx_nic_probe_tx(struct efx_tx_queue *tx_queue ) ;
void efx_nic_init_tx(struct efx_tx_queue *tx_queue ) ;
void efx_nic_fini_tx(struct efx_tx_queue *tx_queue ) ;
void efx_nic_remove_tx(struct efx_tx_queue *tx_queue ) ;
void efx_nic_push_buffers(struct efx_tx_queue *tx_queue ) ;
int efx_nic_probe_rx(struct efx_rx_queue *rx_queue ) ;
void efx_nic_init_rx(struct efx_rx_queue *rx_queue ) ;
void efx_nic_fini_rx(struct efx_rx_queue *rx_queue ) ;
void efx_nic_remove_rx(struct efx_rx_queue *rx_queue ) ;
void efx_nic_notify_rx_desc(struct efx_rx_queue *rx_queue ) ;
bool efx_nic_event_present(struct efx_channel *channel ) ;
void efx_nic_event_test_start(struct efx_channel *channel ) ;
void efx_nic_irq_test_start(struct efx_nic *efx ) ;
irqreturn_t efx_nic_fatal_interrupt(struct efx_nic *efx ) ;
irqreturn_t falcon_legacy_interrupt_a1(int irq , void *dev_id ) ;
void falcon_irq_ack_a1(struct efx_nic *efx ) ;
void efx_nic_dimension_resources(struct efx_nic *efx , unsigned int sram_lim_qw ) ;
void efx_nic_init_common(struct efx_nic *efx ) ;
void efx_nic_push_rx_indir_table(struct efx_nic *efx ) ;
int efx_nic_alloc_buffer(struct efx_nic *efx , struct efx_buffer *buffer , unsigned int len ) ;
void efx_nic_free_buffer(struct efx_nic *efx , struct efx_buffer *buffer ) ;
int efx_nic_test_registers(struct efx_nic *efx , struct efx_nic_register_test const *regs ,
                           size_t n_regs ) ;
size_t efx_nic_get_regs_len(struct efx_nic *efx ) ;
void efx_nic_get_regs(struct efx_nic *efx , void *buf ) ;
void efx_generate_event(struct efx_nic *efx , unsigned int evq , efx_qword_t *event ) ;
__inline static void _efx_writeq(struct efx_nic *efx , __le64 value , unsigned int reg )
{
  {
  writeq((unsigned long )value, (void volatile *)efx->membase + (unsigned long )reg);
  return;
}
}
__inline static void _efx_writed(struct efx_nic *efx , __le32 value , unsigned int reg )
{
  {
  __writel(value, (void volatile *)efx->membase + (unsigned long )reg);
  return;
}
}
__inline static __le32 _efx_readd(struct efx_nic *efx , unsigned int reg )
{
  unsigned int tmp ;
  {
  tmp = __readl((void const volatile *)efx->membase + (unsigned long )reg);
  return (tmp);
}
}
__inline static void efx_writeo(struct efx_nic *efx , efx_oword_t *value , unsigned int reg )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& efx->biu_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  _efx_writeq(efx, value->u64[0], reg);
  _efx_writeq(efx, value->u64[1], reg + 8U);
  __asm__ volatile ("": : : "memory");
  spin_unlock_irqrestore(& efx->biu_lock, flags);
  return;
}
}
__inline static void efx_sram_writeq(struct efx_nic *efx , void *membase , efx_qword_t *value ,
                                     unsigned int index )
{
  unsigned int addr ;
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  addr = index * 8U;
  tmp = spinlock_check(& efx->biu_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  writeq((unsigned long )value->u64[0], (void volatile *)membase + (unsigned long )addr);
  __asm__ volatile ("": : : "memory");
  spin_unlock_irqrestore(& efx->biu_lock, flags);
  return;
}
}
__inline static void efx_writed(struct efx_nic *efx , efx_dword_t *value , unsigned int reg )
{
  {
  _efx_writed(efx, value->u32[0], reg);
  return;
}
}
__inline static void efx_reado(struct efx_nic *efx , efx_oword_t *value , unsigned int reg )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  tmp = spinlock_check(& efx->biu_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  value->u32[0] = _efx_readd(efx, reg);
  value->u32[1] = _efx_readd(efx, reg + 4U);
  value->u32[2] = _efx_readd(efx, reg + 8U);
  value->u32[3] = _efx_readd(efx, reg + 12U);
  spin_unlock_irqrestore(& efx->biu_lock, flags);
  return;
}
}
__inline static void efx_sram_readq(struct efx_nic *efx , void *membase , efx_qword_t *value ,
                                    unsigned int index )
{
  unsigned int addr ;
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  unsigned long tmp___0 ;
  {
  addr = index * 8U;
  tmp = spinlock_check(& efx->biu_lock);
  flags = _raw_spin_lock_irqsave(tmp);
  tmp___0 = readq((void const volatile *)membase + (unsigned long )addr);
  value->u64[0] = (unsigned long long )tmp___0;
  spin_unlock_irqrestore(& efx->biu_lock, flags);
  return;
}
}
__inline static void efx_readd(struct efx_nic *efx , efx_dword_t *value , unsigned int reg )
{
  {
  value->u32[0] = _efx_readd(efx, reg);
  return;
}
}
__inline static void efx_writeo_table(struct efx_nic *efx , efx_oword_t *value , unsigned int reg ,
                                      unsigned int index )
{
  {
  efx_writeo(efx, value, index * 16U + reg);
  return;
}
}
__inline static void efx_reado_table(struct efx_nic *efx , efx_oword_t *value , unsigned int reg ,
                                     unsigned int index )
{
  {
  efx_reado(efx, value, index * 16U + reg);
  return;
}
}
__inline static void _efx_writeo_page(struct efx_nic *efx , efx_oword_t *value , unsigned int reg ,
                                      unsigned int page )
{
  {
  reg = page * 8192U + reg;
  _efx_writeq(efx, value->u64[0], reg);
  _efx_writeq(efx, value->u64[1], reg + 8U);
  return;
}
}
__inline static void _efx_writed_page(struct efx_nic *efx , efx_dword_t *value , unsigned int reg ,
                                      unsigned int page )
{
  {
  efx_writed(efx, value, page * 8192U + reg);
  return;
}
}
static void efx_magic_event(struct efx_channel *channel , u32 magic ) ;
__inline static void efx_write_buf_tbl(struct efx_nic *efx , efx_qword_t *value ,
                                       unsigned int index )
{
  {
  efx_sram_writeq(efx, efx->membase + (unsigned long )(efx->type)->buf_tbl_base, value,
                  index);
  return;
}
}
__inline static efx_qword_t *efx_event(struct efx_channel *channel , unsigned int index )
{
  {
  return ((efx_qword_t *)channel->eventq.addr + (unsigned long )(channel->eventq_mask & index));
}
}
__inline static int efx_event_present(efx_qword_t *event )
{
  {
  return (event->dword[0].u32[0] != 4294967295U && event->dword[1].u32[0] != 4294967295U);
}
}
static bool efx_masked_compare_oword(efx_oword_t const *a , efx_oword_t const *b ,
                                     efx_oword_t const *mask )
{
  {
  return ((bool )(((a->u64[0] ^ b->u64[0]) & mask->u64[0]) != 0ULL || ((a->u64[1] ^ b->u64[1]) & mask->u64[1]) != 0ULL));
}
}
int efx_nic_test_registers(struct efx_nic *efx , struct efx_nic_register_test const *regs ,
                           size_t n_regs )
{
  unsigned int address ;
  unsigned int i ;
  unsigned int j ;
  efx_oword_t mask ;
  efx_oword_t imask ;
  efx_oword_t original ;
  efx_oword_t reg ;
  efx_oword_t buf ;
  bool tmp ;
  bool tmp___0 ;
  {
  address = 0U;
  i = 0U;
  goto ldv_42868;
  ldv_42867:
  address = (regs + (unsigned long )i)->address;
  imask = (regs + (unsigned long )i)->mask;
  mask = imask;
  imask.u64[0] = ~ imask.u64[0];
  imask.u64[1] = ~ imask.u64[1];
  efx_reado(efx, & original, address);
  j = 0U;
  goto ldv_42865;
  ldv_42864: ;
  if ((((((j <= 31U ? (j != 0U ? mask.u32[0] >> (int )j : mask.u32[0] << (int )(- j)) : 0U) | (j <= 63U && j > 31U ? (j > 32U ? mask.u32[1] >> (int )(j - 32U) : mask.u32[1] << (int )(32U - j)) : 0U)) | (j <= 95U && j > 63U ? (j > 64U ? mask.u32[2] >> (int )(j - 64U) : mask.u32[2] << (int )(64U - j)) : 0U)) | (j <= 127U && j > 95U ? (j > 96U ? mask.u32[3] >> (int )(j - 96U) : mask.u32[3] << (int )(96U - j)) : 0U)) & 1U) == 0U) {
    goto ldv_42862;
  } else {
  }
  reg.u64[0] = original.u64[0] & mask.u64[0];
  reg.u64[1] = original.u64[1] & mask.u64[1];
  reg.u32[0] = (reg.u32[0] & (j <= 31U ? (j != 0U ? ~ (1U << (int )j) : ~ (1U >> (int )(- j))) : 4294967295U)) | (j <= 31U ? (j != 0U ? 1U << (int )j : 1U >> (int )(- j)) : 0U);
  reg.u32[1] = (reg.u32[1] & (j <= 63U && j > 31U ? (j > 32U ? ~ (1U << (int )(j - 32U)) : ~ (1U >> (int )(32U - j))) : 4294967295U)) | (j <= 63U && j > 31U ? (j > 32U ? 1U << (int )(j - 32U) : 1U >> (int )(32U - j)) : 0U);
  reg.u32[2] = (reg.u32[2] & (j <= 95U && j > 63U ? (j > 64U ? ~ (1U << (int )(j - 64U)) : ~ (1U >> (int )(64U - j))) : 4294967295U)) | (j <= 95U && j > 63U ? (j > 64U ? 1U << (int )(j - 64U) : 1U >> (int )(64U - j)) : 0U);
  reg.u32[3] = (reg.u32[3] & (j <= 127U && j > 95U ? (j > 96U ? ~ (1U << (int )(j - 96U)) : ~ (1U >> (int )(96U - j))) : 4294967295U)) | (j <= 127U && j > 95U ? (j > 96U ? 1U << (int )(j - 96U) : 1U >> (int )(96U - j)) : 0U);
  efx_writeo(efx, & reg, address);
  efx_reado(efx, & buf, address);
  tmp = efx_masked_compare_oword((efx_oword_t const *)(& reg), (efx_oword_t const *)(& buf),
                                 (efx_oword_t const *)(& mask));
  if ((int )tmp) {
    goto fail;
  } else {
  }
  reg.u64[0] = original.u64[0] | mask.u64[0];
  reg.u64[1] = original.u64[1] | mask.u64[1];
  reg.u32[0] = reg.u32[0] & (j <= 31U ? (j != 0U ? ~ (1U << (int )j) : ~ (1U >> (int )(- j))) : 4294967295U);
  reg.u32[1] = reg.u32[1] & (j <= 63U && j > 31U ? (j > 32U ? ~ (1U << (int )(j - 32U)) : ~ (1U >> (int )(32U - j))) : 4294967295U);
  reg.u32[2] = reg.u32[2] & (j <= 95U && j > 63U ? (j > 64U ? ~ (1U << (int )(j - 64U)) : ~ (1U >> (int )(64U - j))) : 4294967295U);
  reg.u32[3] = reg.u32[3] & (j <= 127U && j > 95U ? (j > 96U ? ~ (1U << (int )(j - 96U)) : ~ (1U >> (int )(96U - j))) : 4294967295U);
  efx_writeo(efx, & reg, address);
  efx_reado(efx, & buf, address);
  tmp___0 = efx_masked_compare_oword((efx_oword_t const *)(& reg), (efx_oword_t const *)(& buf),
                                     (efx_oword_t const *)(& mask));
  if ((int )tmp___0) {
    goto fail;
  } else {
  }
  ldv_42862:
  j = j + 1U;
  ldv_42865: ;
  if (j <= 127U) {
    goto ldv_42864;
  } else {
  }
  efx_writeo(efx, & original, address);
  i = i + 1U;
  ldv_42868: ;
  if ((size_t )i < n_regs) {
    goto ldv_42867;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "wrote %08x:%08x:%08x:%08x read %08x:%08x:%08x:%08x at address 0x%x mask %08x:%08x:%08x:%08x\n",
               reg.u32[3], reg.u32[2], reg.u32[1], reg.u32[0], buf.u32[3], buf.u32[2],
               buf.u32[1], buf.u32[0], address, mask.u32[3], mask.u32[2], mask.u32[1],
               mask.u32[0]);
  } else {
  }
  return (-5);
}
}
static void efx_init_special_buffer(struct efx_nic *efx , struct efx_special_buffer *buffer )
{
  efx_qword_t buf_desc ;
  unsigned int index ;
  dma_addr_t dma_addr ;
  int i ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  i = 0;
  goto ldv_42881;
  ldv_42880:
  index = buffer->index + (unsigned int )i;
  dma_addr = buffer->dma_addr + (dma_addr_t )(i * 4096);
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_special_buffer";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "mapping special buffer %d at %llx\n";
    descriptor.lineno = 293U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "mapping special buffer %d at %llx\n", index, dma_addr);
    } else {
    }
  } else {
  }
  buf_desc.u64[0] = (dma_addr >> 12) << 14;
  efx_write_buf_tbl(efx, & buf_desc, index);
  i = i + 1;
  ldv_42881: ;
  if ((unsigned int )i < buffer->entries) {
    goto ldv_42880;
  } else {
  }
  return;
}
}
static void efx_fini_special_buffer(struct efx_nic *efx , struct efx_special_buffer *buffer )
{
  efx_oword_t buf_tbl_upd ;
  unsigned int start ;
  unsigned int end ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  start = buffer->index;
  end = (buffer->index + buffer->entries) - 1U;
  if (buffer->entries == 0U) {
    return;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_special_buffer";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "unmapping special buffers %d-%d\n";
    descriptor.lineno = 314U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "unmapping special buffers %d-%d\n", buffer->index, (buffer->index + buffer->entries) - 1U);
    } else {
    }
  } else {
  }
  buf_tbl_upd.u64[0] = (((unsigned long long )end << 32) | (unsigned long long )start) | 4611686018427387904ULL;
  buf_tbl_upd.u64[1] = 0ULL;
  efx_writeo(efx, & buf_tbl_upd, 1616U);
  return;
}
}
static int efx_alloc_special_buffer(struct efx_nic *efx , struct efx_special_buffer *buffer ,
                                    unsigned int len )
{
  long tmp ;
  bool tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  struct _ddebug descriptor ;
  phys_addr_t tmp___3 ;
  long tmp___4 ;
  {
  len = (len + 4095U) & 4294963200U;
  buffer->addr = dma_alloc_attrs(& (efx->pci_dev)->dev, (size_t )len, & buffer->dma_addr,
                                 208U, 0);
  if ((unsigned long )buffer->addr == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  buffer->len = len;
  buffer->entries = len / 4096U;
  tmp = ldv__builtin_expect((buffer->dma_addr & 4095ULL) != 0ULL, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared"),
                         "i" (345), "i" (12UL));
    ldv_42897: ;
    goto ldv_42897;
  } else {
  }
  buffer->index = efx->next_buffer_table;
  efx->next_buffer_table = efx->next_buffer_table + buffer->entries;
  tmp___0 = efx_sriov_enabled(efx);
  tmp___1 = ldv__builtin_expect((long )tmp___0, 0L);
  if (tmp___1 != 0L) {
    tmp___2 = ldv__builtin_expect(efx->vf_buftbl_base < efx->next_buffer_table, 0L);
    if (tmp___2 != 0L) {
      __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared"),
                           "i" (352), "i" (12UL));
      ldv_42898: ;
      goto ldv_42898;
    } else {
    }
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_alloc_special_buffer";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "allocating special buffers %d-%d at %llx+%x (virt %p phys %llx)\n";
    descriptor.lineno = 360U;
    descriptor.flags = 0U;
    tmp___4 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___4 != 0L) {
      tmp___3 = virt_to_phys((void volatile *)buffer->addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "allocating special buffers %d-%d at %llx+%x (virt %p phys %llx)\n",
                           buffer->index, (buffer->index + buffer->entries) - 1U,
                           buffer->dma_addr, len, buffer->addr, tmp___3);
    } else {
    }
  } else {
  }
  return (0);
}
}
static void efx_free_special_buffer(struct efx_nic *efx , struct efx_special_buffer *buffer )
{
  struct _ddebug descriptor ;
  phys_addr_t tmp ;
  long tmp___0 ;
  {
  if ((unsigned long )buffer->addr == (unsigned long )((void *)0)) {
    return;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_free_special_buffer";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "deallocating special buffers %d-%d at %llx+%x (virt %p phys %llx)\n";
    descriptor.lineno = 376U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      tmp = virt_to_phys((void volatile *)buffer->addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "deallocating special buffers %d-%d at %llx+%x (virt %p phys %llx)\n",
                           buffer->index, (buffer->index + buffer->entries) - 1U,
                           buffer->dma_addr, buffer->len, buffer->addr, tmp);
    } else {
    }
  } else {
  }
  dma_free_attrs(& (efx->pci_dev)->dev, (size_t )buffer->len, buffer->addr, buffer->dma_addr,
                 0);
  buffer->addr = 0;
  buffer->entries = 0U;
  return;
}
}
int efx_nic_alloc_buffer(struct efx_nic *efx , struct efx_buffer *buffer , unsigned int len )
{
  {
  buffer->addr = dma_alloc_attrs(& (efx->pci_dev)->dev, (size_t )len, & buffer->dma_addr,
                                 32U, 0);
  if ((unsigned long )buffer->addr == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  buffer->len = len;
  memset(buffer->addr, 0, (size_t )len);
  return (0);
}
}
void efx_nic_free_buffer(struct efx_nic *efx , struct efx_buffer *buffer )
{
  {
  if ((unsigned long )buffer->addr != (unsigned long )((void *)0)) {
    dma_free_attrs(& (efx->pci_dev)->dev, (size_t )buffer->len, buffer->addr, buffer->dma_addr,
                   0);
    buffer->addr = 0;
  } else {
  }
  return;
}
}
__inline static efx_qword_t *efx_tx_desc(struct efx_tx_queue *tx_queue , unsigned int index )
{
  {
  return ((efx_qword_t *)tx_queue->txd.addr + (unsigned long )index);
}
}
__inline static void efx_notify_tx_desc(struct efx_tx_queue *tx_queue )
{
  unsigned int write_ptr ;
  efx_dword_t reg ;
  {
  write_ptr = tx_queue->write_count & tx_queue->ptr_mask;
  reg.u32[0] = write_ptr;
  _efx_writed_page(tx_queue->efx, & reg, 2588U, tx_queue->queue);
  return;
}
}
__inline static void efx_push_tx_desc(struct efx_tx_queue *tx_queue , efx_qword_t const *txd )
{
  unsigned int write_ptr ;
  efx_oword_t reg ;
  {
  write_ptr = tx_queue->write_count & tx_queue->ptr_mask;
  reg.u64[0] = 0ULL;
  reg.u64[1] = ((unsigned long long )write_ptr << 32) | 2147483648ULL;
  reg.qword[0] = *txd;
  _efx_writeo_page(tx_queue->efx, & reg, 2576U, tx_queue->queue);
  return;
}
}
__inline static bool efx_may_push_tx_desc(struct efx_tx_queue *tx_queue , unsigned int write_count )
{
  unsigned int empty_read_count ;
  {
  empty_read_count = *((unsigned int volatile *)(& tx_queue->empty_read_count));
  if (empty_read_count == 0U) {
    return (0);
  } else {
  }
  tx_queue->empty_read_count = 0U;
  return (((empty_read_count ^ write_count) & 2147483647U) == 0U);
}
}
void efx_nic_push_buffers(struct efx_tx_queue *tx_queue )
{
  struct efx_tx_buffer *buffer ;
  efx_qword_t *txd ;
  unsigned int write_ptr ;
  unsigned int old_write_count ;
  long tmp ;
  bool tmp___0 ;
  {
  old_write_count = tx_queue->write_count;
  tmp = ldv__builtin_expect(tx_queue->write_count == tx_queue->insert_count, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared"),
                         "i" (481), "i" (12UL));
    ldv_42955: ;
    goto ldv_42955;
  } else {
  }
  ldv_42956:
  write_ptr = tx_queue->write_count & tx_queue->ptr_mask;
  buffer = tx_queue->buffer + (unsigned long )write_ptr;
  txd = efx_tx_desc(tx_queue, write_ptr);
  tx_queue->write_count = tx_queue->write_count + 1U;
  txd->u64[0] = ((((unsigned long long )buffer->flags & 1ULL) << 62) | ((unsigned long long )buffer->len << 48)) | buffer->dma_addr;
  if (tx_queue->write_count != tx_queue->insert_count) {
    goto ldv_42956;
  } else {
  }
  __asm__ volatile ("sfence": : : "memory");
  tmp___0 = efx_may_push_tx_desc(tx_queue, old_write_count);
  if ((int )tmp___0) {
    txd = efx_tx_desc(tx_queue, tx_queue->ptr_mask & old_write_count);
    efx_push_tx_desc(tx_queue, (efx_qword_t const *)txd);
    tx_queue->pushes = tx_queue->pushes + 1U;
  } else {
    efx_notify_tx_desc(tx_queue);
  }
  return;
}
}
int efx_nic_probe_tx(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  unsigned int entries ;
  int tmp ;
  {
  efx = tx_queue->efx;
  entries = tx_queue->ptr_mask + 1U;
  tmp = efx_alloc_special_buffer(efx, & tx_queue->txd, entries * 8U);
  return (tmp);
}
}
void efx_nic_init_tx(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  unsigned long tmp ;
  int csum ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  efx = tx_queue->efx;
  efx_init_special_buffer(efx, & tx_queue->txd);
  tmp = __ffs((unsigned long )tx_queue->txd.entries);
  reg.u64[0] = ((((unsigned long long )tx_queue->txd.index << 36) | ((unsigned long long )(tx_queue->channel)->channel << 24)) | ((unsigned long long )tx_queue->queue << 5)) | ((unsigned long long )tmp << 3);
  reg.u64[1] = 150994944ULL;
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 > 1) {
    csum = (int )tx_queue->queue & 1;
    reg.u64[0] = reg.u64[0];
    reg.u64[1] = (reg.u64[1] & 0xfffffffffbffffffULL) | (csum == 0 ? 67108864ULL : 0ULL);
    reg.u64[0] = reg.u64[0];
    reg.u64[1] = (reg.u64[1] & 0xfffffffffdffffffULL) | (csum == 0 ? 33554432ULL : 0ULL);
  } else {
  }
  efx_writeo_table(efx, & reg, (efx->type)->txd_ptr_tbl_base, tx_queue->queue);
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 <= 1) {
    efx_reado(efx, & reg, 2608U);
    if ((int )tx_queue->queue & 1) {
      __clear_bit_le((int )tx_queue->queue, (void *)(& reg));
    } else {
      __set_bit_le((int )tx_queue->queue, (void *)(& reg));
    }
    efx_writeo(efx, & reg, 2608U);
  } else {
  }
  tmp___2 = efx_nic_rev(efx);
  if (tmp___2 > 1) {
    reg.u64[0] = (tx_queue->queue & 2U) != 0U ? 0ULL : 21ULL;
    reg.u64[1] = 0ULL;
    efx_writeo_table(efx, & reg, 16252928U, tx_queue->queue);
  } else {
  }
  return;
}
}
static void efx_flush_tx_queue(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  efx_oword_t tx_flush_descq ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  {
  efx = tx_queue->efx;
  tmp = atomic_read((atomic_t const *)(& tx_queue->flush_outstanding));
  __ret_warn_on = tmp != 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       583);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  atomic_set(& tx_queue->flush_outstanding, 1);
  tx_flush_descq.u64[0] = (unsigned long long )tx_queue->queue | 4096ULL;
  tx_flush_descq.u64[1] = 0ULL;
  efx_writeo(efx, & tx_flush_descq, 2560U);
  return;
}
}
void efx_nic_fini_tx(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  efx_oword_t tx_desc_ptr ;
  {
  efx = tx_queue->efx;
  tx_desc_ptr.u64[0] = 0ULL;
  tx_desc_ptr.u64[1] = 0ULL;
  efx_writeo_table(efx, & tx_desc_ptr, (efx->type)->txd_ptr_tbl_base, tx_queue->queue);
  efx_fini_special_buffer(efx, & tx_queue->txd);
  return;
}
}
void efx_nic_remove_tx(struct efx_tx_queue *tx_queue )
{
  {
  efx_free_special_buffer(tx_queue->efx, & tx_queue->txd);
  return;
}
}
__inline static efx_qword_t *efx_rx_desc(struct efx_rx_queue *rx_queue , unsigned int index )
{
  {
  return ((efx_qword_t *)rx_queue->rxd.addr + (unsigned long )index);
}
}
__inline static void efx_build_rx_desc(struct efx_rx_queue *rx_queue , unsigned int index )
{
  struct efx_rx_buffer *rx_buf ;
  efx_qword_t *rxd ;
  {
  rxd = efx_rx_desc(rx_queue, index);
  rx_buf = efx_rx_buffer(rx_queue, index);
  rxd->u64[0] = ((unsigned long long )(rx_buf->len - (unsigned int )((rx_queue->efx)->type)->rx_buffer_padding) << 48) | rx_buf->dma_addr;
  return;
}
}
void efx_nic_notify_rx_desc(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  efx_dword_t reg ;
  unsigned int write_ptr ;
  int tmp ;
  {
  efx = rx_queue->efx;
  goto ldv_43001;
  ldv_43000:
  efx_build_rx_desc(rx_queue, (unsigned int )rx_queue->notified_count & rx_queue->ptr_mask);
  rx_queue->notified_count = rx_queue->notified_count + 1;
  ldv_43001: ;
  if (rx_queue->notified_count != rx_queue->added_count) {
    goto ldv_43000;
  } else {
  }
  __asm__ volatile ("sfence": : : "memory");
  write_ptr = (unsigned int )rx_queue->added_count & rx_queue->ptr_mask;
  reg.u32[0] = write_ptr;
  tmp = efx_rx_queue_index(rx_queue);
  _efx_writed_page(efx, & reg, 2108U, (unsigned int )tmp);
  return;
}
}
int efx_nic_probe_rx(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  unsigned int entries ;
  int tmp ;
  {
  efx = rx_queue->efx;
  entries = rx_queue->ptr_mask + 1U;
  tmp = efx_alloc_special_buffer(efx, & rx_queue->rxd, entries * 8U);
  return (tmp);
}
}
void efx_nic_init_rx(struct efx_rx_queue *rx_queue )
{
  efx_oword_t rx_desc_ptr ;
  struct efx_nic *efx ;
  bool is_b0 ;
  int tmp ;
  bool iscsi_digest_en ;
  struct _ddebug descriptor ;
  int tmp___0 ;
  long tmp___1 ;
  struct efx_channel *tmp___2 ;
  int tmp___3 ;
  unsigned long tmp___4 ;
  int tmp___5 ;
  {
  efx = rx_queue->efx;
  tmp = efx_nic_rev(efx);
  is_b0 = tmp > 1;
  iscsi_digest_en = is_b0;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_nic_init_rx";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "RX queue %d ring in special buffers %d-%d\n";
    descriptor.lineno = 685U;
    descriptor.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      tmp___0 = efx_rx_queue_index(rx_queue);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "RX queue %d ring in special buffers %d-%d\n", tmp___0,
                           rx_queue->rxd.index, (rx_queue->rxd.index + rx_queue->rxd.entries) - 1U);
    } else {
    }
  } else {
  }
  efx_init_special_buffer(efx, & rx_queue->rxd);
  tmp___2 = efx_rx_queue_channel(rx_queue);
  tmp___3 = efx_rx_queue_index(rx_queue);
  tmp___4 = __ffs((unsigned long )rx_queue->rxd.entries);
  rx_desc_ptr.u64[0] = ((((((unsigned long long )rx_queue->rxd.index << 36) | ((unsigned long long )tmp___2->channel << 24)) | ((unsigned long long )tmp___3 << 5)) | ((unsigned long long )tmp___4 << 3)) | ((unsigned long long )(! is_b0) << 1)) | 1ULL;
  rx_desc_ptr.u64[1] = ((unsigned long long )iscsi_digest_en << 24) | ((unsigned long long )iscsi_digest_en << 23);
  tmp___5 = efx_rx_queue_index(rx_queue);
  efx_writeo_table(efx, & rx_desc_ptr, (efx->type)->rxd_ptr_tbl_base, (unsigned int )tmp___5);
  return;
}
}
static void efx_flush_rx_queue(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  efx_oword_t rx_flush_descq ;
  int tmp ;
  {
  efx = rx_queue->efx;
  tmp = efx_rx_queue_index(rx_queue);
  rx_flush_descq.u64[0] = (unsigned long long )tmp | 16777216ULL;
  rx_flush_descq.u64[1] = 0ULL;
  efx_writeo(efx, & rx_flush_descq, 2080U);
  return;
}
}
void efx_nic_fini_rx(struct efx_rx_queue *rx_queue )
{
  efx_oword_t rx_desc_ptr ;
  struct efx_nic *efx ;
  int tmp ;
  {
  efx = rx_queue->efx;
  rx_desc_ptr.u64[0] = 0ULL;
  rx_desc_ptr.u64[1] = 0ULL;
  tmp = efx_rx_queue_index(rx_queue);
  efx_writeo_table(efx, & rx_desc_ptr, (efx->type)->rxd_ptr_tbl_base, (unsigned int )tmp);
  efx_fini_special_buffer(efx, & rx_queue->rxd);
  return;
}
}
void efx_nic_remove_rx(struct efx_rx_queue *rx_queue )
{
  {
  efx_free_special_buffer(rx_queue->efx, & rx_queue->rxd);
  return;
}
}
static bool efx_flush_wake(struct efx_nic *efx )
{
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  __asm__ volatile ("mfence": : : "memory");
  tmp = atomic_read((atomic_t const *)(& efx->drain_pending));
  if (tmp == 0) {
    tmp___2 = 1;
  } else {
    tmp___0 = atomic_read((atomic_t const *)(& efx->rxq_flush_outstanding));
    if (tmp___0 <= 3) {
      tmp___1 = atomic_read((atomic_t const *)(& efx->rxq_flush_pending));
      if (tmp___1 > 0) {
        tmp___2 = 1;
      } else {
        tmp___2 = 0;
      }
    } else {
      tmp___2 = 0;
    }
  }
  return ((bool )tmp___2);
}
}
static bool efx_check_tx_flush_complete(struct efx_nic *efx )
{
  bool i ;
  efx_oword_t txd_ptr_tbl ;
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  struct _ddebug descriptor ;
  long tmp ;
  struct _ddebug descriptor___0 ;
  long tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  {
  i = 1;
  channel = efx->channel[0];
  goto ldv_43057;
  ldv_43056:
  tmp___3 = efx_channel_has_tx_queues(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_43054;
    ldv_43053:
    efx_reado_table(efx, & txd_ptr_tbl, 16056320U, tx_queue->queue);
    if ((int )txd_ptr_tbl.u64[0] & 1 || (int )(txd_ptr_tbl.u64[1] >> 24) & 1) {
      if ((efx->msg_enable & 8192U) != 0U) {
        descriptor.modname = "sfc";
        descriptor.function = "efx_check_tx_flush_complete";
        descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
        descriptor.format = "flush did not complete on TXQ %d\n";
        descriptor.lineno = 778U;
        descriptor.flags = 0U;
        tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
        if (tmp != 0L) {
          __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                               "flush did not complete on TXQ %d\n", tx_queue->queue);
        } else {
        }
      } else {
      }
      i = 0;
    } else {
      tmp___1 = atomic_cmpxchg(& tx_queue->flush_outstanding, 1, 0);
      if (tmp___1 != 0) {
        if ((efx->msg_enable & 8192U) != 0U) {
          descriptor___0.modname = "sfc";
          descriptor___0.function = "efx_check_tx_flush_complete";
          descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
          descriptor___0.format = "flush complete on TXQ %d, so drain the queue\n";
          descriptor___0.lineno = 787U;
          descriptor___0.flags = 0U;
          tmp___0 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
          if (tmp___0 != 0L) {
            __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                                 "flush complete on TXQ %d, so drain the queue\n",
                                 tx_queue->queue);
          } else {
          }
        } else {
        }
        efx_magic_event(channel, tx_queue->queue | 66560U);
      } else {
      }
    }
    tx_queue = tx_queue + 1;
    ldv_43054: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___2 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___2) {
        goto ldv_43053;
      } else {
        goto ldv_43055;
      }
    } else {
    }
    ldv_43055: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43057: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43056;
  } else {
  }
  return (i);
}
}
int efx_nic_flush_queues(struct efx_nic *efx )
{
  unsigned int timeout ;
  unsigned long tmp ;
  struct efx_channel *channel ;
  struct efx_rx_queue *rx_queue ;
  struct efx_tx_queue *tx_queue ;
  int rc ;
  bool tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  bool tmp___5 ;
  int tmp___6 ;
  bool tmp___7 ;
  int tmp___8 ;
  long __ret ;
  wait_queue_t __wait ;
  struct task_struct *tmp___9 ;
  bool tmp___10 ;
  bool tmp___11 ;
  int tmp___12 ;
  int tmp___13 ;
  int tmp___14 ;
  int tmp___15 ;
  int tmp___16 ;
  int tmp___17 ;
  bool tmp___18 ;
  int tmp___19 ;
  {
  tmp = msecs_to_jiffies(5000U);
  timeout = (unsigned int )tmp;
  rc = 0;
  (*((efx->type)->prepare_flush))(efx);
  channel = efx->channel[0];
  goto ldv_43074;
  ldv_43073:
  tmp___1 = efx_channel_has_tx_queues(channel);
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_43068;
    ldv_43067:
    atomic_inc(& efx->drain_pending);
    efx_flush_tx_queue(tx_queue);
    tx_queue = tx_queue + 1;
    ldv_43068: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___0 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___0) {
        goto ldv_43067;
      } else {
        goto ldv_43069;
      }
    } else {
    }
    ldv_43069: ;
  }
  tmp___3 = efx_channel_has_rx_queue(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_43071;
    ldv_43070:
    atomic_inc(& efx->drain_pending);
    rx_queue->flush_pending = 1;
    atomic_inc(& efx->rxq_flush_pending);
    rx_queue = 0;
    ldv_43071: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_43070;
    } else {
    }
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43074: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43073;
  } else {
  }
  goto ldv_43089;
  ldv_43088:
  tmp___5 = efx_sriov_enabled(efx);
  if ((int )tmp___5) {
    rc = efx_mcdi_flush_rxqs(efx);
    if (rc == 0) {
      goto wait;
    } else {
    }
  } else {
  }
  channel = efx->channel[0];
  goto ldv_43081;
  ldv_43080:
  tmp___7 = efx_channel_has_rx_queue(channel);
  if (tmp___7) {
    tmp___8 = 0;
  } else {
    tmp___8 = 1;
  }
  if (tmp___8) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_43079;
    ldv_43078:
    tmp___6 = atomic_read((atomic_t const *)(& efx->rxq_flush_outstanding));
    if (tmp___6 > 3) {
      goto ldv_43077;
    } else {
    }
    if ((int )rx_queue->flush_pending) {
      rx_queue->flush_pending = 0;
      atomic_dec(& efx->rxq_flush_pending);
      atomic_inc(& efx->rxq_flush_outstanding);
      efx_flush_rx_queue(rx_queue);
    } else {
    }
    rx_queue = 0;
    ldv_43079: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_43078;
    } else {
    }
    ldv_43077: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43081: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43080;
  } else {
  }
  wait:
  __ret = (long )timeout;
  tmp___11 = efx_flush_wake(efx);
  if (tmp___11) {
    tmp___12 = 0;
  } else {
    tmp___12 = 1;
  }
  if (tmp___12) {
    tmp___9 = get_current();
    __wait.flags = 0U;
    __wait.private = (void *)tmp___9;
    __wait.func = & autoremove_wake_function;
    __wait.task_list.next = & __wait.task_list;
    __wait.task_list.prev = & __wait.task_list;
    ldv_43086:
    prepare_to_wait(& efx->flush_wq, & __wait, 2);
    tmp___10 = efx_flush_wake(efx);
    if ((int )tmp___10) {
      goto ldv_43085;
    } else {
    }
    __ret = schedule_timeout(__ret);
    if (__ret == 0L) {
      goto ldv_43085;
    } else {
    }
    goto ldv_43086;
    ldv_43085:
    finish_wait(& efx->flush_wq, & __wait);
  } else {
  }
  timeout = (unsigned int )__ret;
  ldv_43089: ;
  if (timeout != 0U) {
    tmp___13 = atomic_read((atomic_t const *)(& efx->drain_pending));
    if (tmp___13 > 0) {
      goto ldv_43088;
    } else {
      goto ldv_43090;
    }
  } else {
  }
  ldv_43090:
  tmp___17 = atomic_read((atomic_t const *)(& efx->drain_pending));
  if (tmp___17 != 0) {
    tmp___18 = efx_check_tx_flush_complete(efx);
    if (tmp___18) {
      tmp___19 = 0;
    } else {
      tmp___19 = 1;
    }
    if (tmp___19) {
      if ((efx->msg_enable & 8192U) != 0U) {
        tmp___14 = atomic_read((atomic_t const *)(& efx->rxq_flush_pending));
        tmp___15 = atomic_read((atomic_t const *)(& efx->rxq_flush_outstanding));
        tmp___16 = atomic_read((atomic_t const *)(& efx->drain_pending));
        netdev_err((struct net_device const *)efx->net_dev, "failed to flush %d queues (rx %d+%d)\n",
                   tmp___16, tmp___15, tmp___14);
      } else {
      }
      rc = -110;
      atomic_set(& efx->drain_pending, 0);
      atomic_set(& efx->rxq_flush_pending, 0);
      atomic_set(& efx->rxq_flush_outstanding, 0);
    } else {
    }
  } else {
  }
  (*((efx->type)->finish_flush))(efx);
  return (rc);
}
}
void efx_nic_eventq_read_ack(struct efx_channel *channel )
{
  efx_dword_t reg ;
  struct efx_nic *efx ;
  {
  efx = channel->efx;
  reg.u32[0] = channel->eventq_read_ptr & channel->eventq_mask;
  efx_writed(efx, & reg, (unsigned int )(efx->type)->evq_rptr_tbl_base + (unsigned int )(channel->channel * 16));
  return;
}
}
void efx_generate_event(struct efx_nic *efx , unsigned int evq , efx_qword_t *event )
{
  efx_oword_t drv_ev_reg ;
  {
  drv_ev_reg.u32[0] = event->u32[0];
  drv_ev_reg.u32[1] = event->u32[1];
  drv_ev_reg.u32[2] = 0U;
  drv_ev_reg.u32[3] = 0U;
  drv_ev_reg.u64[0] = drv_ev_reg.u64[0];
  drv_ev_reg.u64[1] = (drv_ev_reg.u64[1] & 0xfffffffffffff000ULL) | (unsigned long long )evq;
  efx_writeo(efx, & drv_ev_reg, 1088U);
  return;
}
}
static void efx_magic_event(struct efx_channel *channel , u32 magic )
{
  efx_qword_t event ;
  {
  event.u64[0] = (unsigned long long )magic | 8070450532247928832ULL;
  efx_generate_event(channel->efx, (unsigned int )channel->channel, & event);
  return;
}
}
static int efx_handle_tx_event(struct efx_channel *channel , efx_qword_t *event )
{
  unsigned int tx_ev_desc_ptr ;
  unsigned int tx_ev_q_label ;
  struct efx_tx_queue *tx_queue ;
  struct efx_nic *efx ;
  int tx_packets ;
  long tmp ;
  long tmp___0 ;
  {
  efx = channel->efx;
  tx_packets = 0;
  tmp = ldv__builtin_expect((unsigned long )*((unsigned long volatile *)(& efx->reset_pending)) != 0UL,
                         0L);
  if (tmp != 0L) {
    return (0);
  } else {
  }
  tmp___0 = ldv__builtin_expect((long )((int )(event->u64[0] >> 12)) & 1L, 1L);
  if (tmp___0 != 0L) {
    tx_ev_desc_ptr = (unsigned int )event->u64[0] & 4095U;
    tx_ev_q_label = (unsigned int )(event->u64[0] >> 32) & 31U;
    tx_queue = efx_channel_get_tx_queue(channel, tx_ev_q_label & 3U);
    tx_packets = (int )((tx_ev_desc_ptr - tx_queue->read_count) & tx_queue->ptr_mask);
    efx_xmit_done(tx_queue, tx_ev_desc_ptr);
  } else
  if ((int )(event->u64[0] >> 15) & 1) {
    tx_ev_q_label = (unsigned int )(event->u64[0] >> 32) & 31U;
    tx_queue = efx_channel_get_tx_queue(channel, tx_ev_q_label & 3U);
    netif_tx_lock___0(efx->net_dev);
    efx_notify_tx_desc(tx_queue);
    netif_tx_unlock___0(efx->net_dev);
  } else
  if ((int )(event->u64[0] >> 38) & 1) {
    efx_schedule_reset(efx, 9);
  } else
  if ((efx->msg_enable & 128U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "channel %d unexpected TX event %08x:%08x\n",
               channel->channel, event->u32[1], event->u32[0]);
  } else {
  }
  return (tx_packets);
}
}
static u16 efx_handle_rx_not_ok(struct efx_rx_queue *rx_queue , efx_qword_t const *event )
{
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  struct efx_nic *efx ;
  bool rx_ev_buf_owner_id_err ;
  bool rx_ev_ip_hdr_chksum_err ;
  bool rx_ev_tcp_udp_chksum_err ;
  bool rx_ev_eth_crc_err ;
  bool rx_ev_frm_trunc ;
  bool rx_ev_drib_nib ;
  bool rx_ev_tobe_disc ;
  bool rx_ev_other_err ;
  bool rx_ev_pause_frm ;
  bool rx_ev_hdr_type ;
  bool rx_ev_mcast_pkt ;
  unsigned int rx_ev_pkt_type ;
  int tmp___0 ;
  {
  tmp = efx_rx_queue_channel(rx_queue);
  channel = tmp;
  efx = rx_queue->efx;
  rx_ev_hdr_type = ((event->u64[0] >> 42) & 3ULL) != 0ULL;
  rx_ev_mcast_pkt = ((event->u64[0] >> 39) & 1ULL) != 0ULL;
  rx_ev_tobe_disc = ((event->u64[0] >> 47) & 1ULL) != 0ULL;
  rx_ev_pkt_type = (unsigned int )(event->u64[0] >> 44) & 7U;
  rx_ev_buf_owner_id_err = ((event->u64[0] >> 54) & 1ULL) != 0ULL;
  rx_ev_ip_hdr_chksum_err = ((event->u64[0] >> 52) & 1ULL) != 0ULL;
  rx_ev_tcp_udp_chksum_err = ((event->u64[0] >> 51) & 1ULL) != 0ULL;
  rx_ev_eth_crc_err = ((event->u64[0] >> 50) & 1ULL) != 0ULL;
  rx_ev_frm_trunc = ((event->u64[0] >> 49) & 1ULL) != 0ULL;
  tmp___0 = efx_nic_rev(efx);
  rx_ev_drib_nib = tmp___0 <= 1 && ((event->u64[0] >> 49) & 1ULL) != 0ULL;
  rx_ev_pause_frm = ((event->u64[0] >> 55) & 1ULL) != 0ULL;
  rx_ev_other_err = ((((((int )rx_ev_drib_nib | (int )rx_ev_tcp_udp_chksum_err) | (int )rx_ev_buf_owner_id_err) | (int )rx_ev_eth_crc_err) | (int )rx_ev_frm_trunc) | (int )rx_ev_ip_hdr_chksum_err) != 0;
  if ((int )rx_ev_frm_trunc) {
    channel->n_rx_frm_trunc = channel->n_rx_frm_trunc + 1U;
  } else
  if ((int )rx_ev_tobe_disc) {
    channel->n_rx_tobe_disc = channel->n_rx_tobe_disc + 1U;
  } else
  if ((unsigned long )efx->loopback_selftest == (unsigned long )((void *)0)) {
    if ((int )rx_ev_ip_hdr_chksum_err) {
      channel->n_rx_ip_hdr_chksum_err = channel->n_rx_ip_hdr_chksum_err + 1U;
    } else
    if ((int )rx_ev_tcp_udp_chksum_err) {
      channel->n_rx_tcp_udp_chksum_err = channel->n_rx_tcp_udp_chksum_err + 1U;
    } else {
    }
  } else {
  }
  return ((((((int )rx_ev_eth_crc_err | (int )rx_ev_frm_trunc) | (int )rx_ev_drib_nib) | (int )rx_ev_tobe_disc) | (int )rx_ev_pause_frm) != 0 ? 4U : 0U);
}
}
static void efx_handle_rx_bad_index(struct efx_rx_queue *rx_queue , unsigned int index )
{
  struct efx_nic *efx ;
  unsigned int expected ;
  unsigned int dropped ;
  int tmp ;
  {
  efx = rx_queue->efx;
  expected = (unsigned int )rx_queue->removed_count & rx_queue->ptr_mask;
  dropped = (index - expected) & rx_queue->ptr_mask;
  if ((efx->msg_enable & 64U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "dropped %d events (index=%d expected=%d)\n",
                dropped, index, expected);
  } else {
  }
  tmp = efx_nic_rev(efx);
  efx_schedule_reset(efx, tmp <= 1 ? 7 : 3);
  return;
}
}
static void efx_handle_rx_event(struct efx_channel *channel , efx_qword_t const *event )
{
  unsigned int rx_ev_desc_ptr ;
  unsigned int rx_ev_byte_cnt ;
  unsigned int rx_ev_hdr_type ;
  unsigned int rx_ev_mcast_pkt ;
  unsigned int expected_ptr ;
  bool rx_ev_pkt_ok ;
  u16 flags ;
  struct efx_rx_queue *rx_queue ;
  struct efx_nic *efx ;
  long tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  int __ret_warn_on___0 ;
  long tmp___1 ;
  int __ret_warn_on___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  unsigned int rx_ev_mcast_hash_match ;
  long tmp___5 ;
  {
  efx = channel->efx;
  tmp = ldv__builtin_expect((unsigned long )*((unsigned long volatile *)(& efx->reset_pending)) != 0UL,
                         0L);
  if (tmp != 0L) {
    return;
  } else {
  }
  rx_ev_byte_cnt = (unsigned int )(event->u64[0] >> 16) & 16383U;
  rx_ev_pkt_ok = ((event->u64[0] >> 56) & 1ULL) != 0ULL;
  rx_ev_hdr_type = (unsigned int )(event->u64[0] >> 42) & 3U;
  __ret_warn_on = (int )(event->u64[0] >> 31) & 1;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       1100);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __ret_warn_on___0 = ((event->u64[0] >> 15) & 1ULL) == 0ULL;
  tmp___1 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       1101);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  __ret_warn_on___1 = ((event->u64[0] >> 32) & 31ULL) != (unsigned long long )channel->channel;
  tmp___2 = ldv__builtin_expect(__ret_warn_on___1 != 0, 0L);
  if (tmp___2 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       1103);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___1 != 0, 0L);
  rx_queue = efx_channel_get_rx_queue(channel);
  rx_ev_desc_ptr = (unsigned int )event->u64[0] & 4095U;
  expected_ptr = (unsigned int )rx_queue->removed_count & rx_queue->ptr_mask;
  tmp___3 = ldv__builtin_expect(rx_ev_desc_ptr != expected_ptr, 0L);
  if (tmp___3 != 0L) {
    efx_handle_rx_bad_index(rx_queue, rx_ev_desc_ptr);
  } else {
  }
  tmp___4 = ldv__builtin_expect((long )rx_ev_pkt_ok, 1L);
  if (tmp___4 != 0L) {
    flags = rx_ev_hdr_type == 0U || rx_ev_hdr_type == 1U ? 2U : 0U;
  } else {
    flags = efx_handle_rx_not_ok(rx_queue, event);
  }
  rx_ev_mcast_pkt = (unsigned int )(event->u64[0] >> 39) & 1U;
  if (rx_ev_mcast_pkt != 0U) {
    rx_ev_mcast_hash_match = (unsigned int )(event->u64[0] >> 40) & 1U;
    tmp___5 = ldv__builtin_expect(rx_ev_mcast_hash_match == 0U, 0L);
    if (tmp___5 != 0L) {
      channel->n_rx_mcast_mismatch = channel->n_rx_mcast_mismatch + 1U;
      flags = (u16 )((unsigned int )flags | 4U);
    } else {
    }
  } else {
  }
  channel->irq_mod_score = channel->irq_mod_score + 2U;
  efx_rx_packet(rx_queue, rx_ev_desc_ptr, rx_ev_byte_cnt, (int )flags);
  return;
}
}
static void efx_handle_tx_flush_done(struct efx_nic *efx , efx_qword_t *event )
{
  struct efx_tx_queue *tx_queue ;
  int qid ;
  int tmp ;
  {
  qid = (int )event->u64[0] & 16383;
  if ((unsigned int )qid < efx->n_tx_channels * 4U) {
    tx_queue = efx_get_tx_queue(efx, (unsigned int )(qid / 4), (unsigned int )(qid % 4));
    tmp = atomic_cmpxchg(& tx_queue->flush_outstanding, 1, 0);
    if (tmp != 0) {
      efx_magic_event(tx_queue->channel, tx_queue->queue | 66560U);
    } else {
    }
  } else {
  }
  return;
}
}
static void efx_handle_rx_flush_done(struct efx_nic *efx , efx_qword_t *event )
{
  struct efx_channel *channel ;
  struct efx_rx_queue *rx_queue ;
  int qid ;
  bool failed ;
  bool tmp ;
  int tmp___0 ;
  int tmp___1 ;
  struct efx_channel *tmp___2 ;
  bool tmp___3 ;
  {
  qid = (int )event->u64[0] & 4095;
  failed = ((event->u64[0] >> 12) & 1ULL) != 0ULL;
  if ((unsigned int )qid >= efx->n_channels) {
    return;
  } else {
  }
  channel = efx_get_channel(efx, (unsigned int )qid);
  tmp = efx_channel_has_rx_queue(channel);
  if (tmp) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  if (tmp___0) {
    return;
  } else {
  }
  rx_queue = efx_channel_get_rx_queue(channel);
  if ((int )failed) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "RXQ %d flush retry\n",
                  qid);
    } else {
    }
    rx_queue->flush_pending = 1;
    atomic_inc(& efx->rxq_flush_pending);
  } else {
    tmp___1 = efx_rx_queue_index(rx_queue);
    tmp___2 = efx_rx_queue_channel(rx_queue);
    efx_magic_event(tmp___2, (u32 )(tmp___1 | 66304));
  }
  atomic_dec(& efx->rxq_flush_outstanding);
  tmp___3 = efx_flush_wake(efx);
  if ((int )tmp___3) {
    __wake_up(& efx->flush_wq, 3U, 1, 0);
  } else {
  }
  return;
}
}
static void efx_handle_drain_event(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  {
  efx = channel->efx;
  tmp = atomic_read((atomic_t const *)(& efx->drain_pending));
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       1202);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  atomic_dec(& efx->drain_pending);
  tmp___1 = efx_flush_wake(efx);
  if ((int )tmp___1) {
    __wake_up(& efx->flush_wq, 3U, 1, 0);
  } else {
  }
  return;
}
}
static void efx_handle_generated_event(struct efx_channel *channel , efx_qword_t *event )
{
  struct efx_nic *efx ;
  struct efx_rx_queue *rx_queue ;
  struct efx_rx_queue *tmp___0 ;
  struct efx_rx_queue *tmp___1 ;
  bool tmp___2 ;
  unsigned int magic ;
  unsigned int code ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  struct _ddebug descriptor ;
  long tmp___3 ;
  int tmp___4 ;
  int tmp___5 ;
  {
  efx = channel->efx;
  tmp___2 = efx_channel_has_rx_queue(channel);
  if ((int )tmp___2) {
    tmp___0 = efx_channel_get_rx_queue(channel);
    tmp___1 = tmp___0;
  } else {
    tmp___1 = 0;
  }
  rx_queue = tmp___1;
  magic = (unsigned int )event->u64[0];
  code = magic >> 8;
  if ((unsigned int )(channel->channel | 65792) == magic) {
    __vpp_verify = 0;
    switch (4UL) {
    case 1UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
    goto ldv_43194;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43194;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43194;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43194;
    default:
    __bad_percpu_size();
    }
    ldv_43194:
    pscr_ret__ = pfo_ret__;
    goto ldv_43200;
    case 2UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43204;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43204;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43204;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43204;
    default:
    __bad_percpu_size();
    }
    ldv_43204:
    pscr_ret__ = pfo_ret_____0;
    goto ldv_43200;
    case 4UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43213;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43213;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43213;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43213;
    default:
    __bad_percpu_size();
    }
    ldv_43213:
    pscr_ret__ = pfo_ret_____1;
    goto ldv_43200;
    case 8UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43222;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43222;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43222;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43222;
    default:
    __bad_percpu_size();
    }
    ldv_43222:
    pscr_ret__ = pfo_ret_____2;
    goto ldv_43200;
    default:
    __bad_size_call_parameter();
    goto ldv_43200;
    }
    ldv_43200:
    channel->event_test_cpu = pscr_ret__;
  } else
  if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
    tmp___5 = efx_rx_queue_index(rx_queue);
    if ((unsigned int )(tmp___5 | 66048) == magic) {
      efx_fast_push_rx_descriptors(rx_queue);
    } else {
      goto _L___0;
    }
  } else
  _L___0:
  if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
    tmp___4 = efx_rx_queue_index(rx_queue);
    if ((unsigned int )(tmp___4 | 66304) == magic) {
      rx_queue->enabled = 0;
      efx_handle_drain_event(channel);
    } else {
      goto _L;
    }
  } else
  _L:
  if (code == 260U) {
    efx_handle_drain_event(channel);
  } else
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_handle_generated_event";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "channel %d received generated event %08x:%08x\n";
    descriptor.lineno = 1235U;
    descriptor.flags = 0U;
    tmp___3 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___3 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "channel %d received generated event %08x:%08x\n", channel->channel,
                           event->u32[1], event->u32[0]);
    } else {
    }
  } else {
  }
  return;
}
}
static void efx_handle_driver_event(struct efx_channel *channel , efx_qword_t *event )
{
  struct efx_nic *efx ;
  unsigned int ev_sub_code ;
  unsigned int ev_sub_data ;
  struct _ddebug descriptor ;
  long tmp ;
  int tmp___0 ;
  {
  efx = channel->efx;
  ev_sub_code = (unsigned int )(event->u64[0] >> 56) & 15U;
  ev_sub_data = (unsigned int )event->u64[0] & 16383U;
  switch (ev_sub_code) {
  case 0U:
  efx_handle_tx_flush_done(efx, event);
  efx_sriov_tx_flush_done(efx, event);
  goto ldv_43241;
  case 1U:
  efx_handle_rx_flush_done(efx, event);
  efx_sriov_rx_flush_done(efx, event);
  goto ldv_43241;
  case 2U: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_handle_driver_event";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "channel %d EVQ %d initialised\n";
    descriptor.lineno = 1265U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "channel %d EVQ %d initialised\n", channel->channel, ev_sub_data);
    } else {
    }
  } else {
  }
  goto ldv_43241;
  case 5U: ;
  goto ldv_43241;
  case 6U: ;
  goto ldv_43241;
  case 10U: ;
  goto ldv_43241;
  case 11U: ;
  if ((efx->msg_enable & 64U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "channel %d seen DRIVER RX_RESET event. Resetting.\n",
               channel->channel);
  } else {
  }
  atomic_inc(& efx->rx_reset);
  tmp___0 = efx_nic_rev(efx);
  efx_schedule_reset(efx, tmp___0 <= 1 ? 7 : 3);
  goto ldv_43241;
  case 14U: ;
  if (ev_sub_data <= 127U) {
    if ((efx->msg_enable & 64U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "RX DMA Q %d reports descriptor fetch error. RX Q %d is disabled.\n",
                 ev_sub_data, ev_sub_data);
    } else {
    }
    efx_schedule_reset(efx, 8);
  } else {
    efx_sriov_desc_fetch_err(efx, ev_sub_data);
  }
  goto ldv_43241;
  case 15U: ;
  if (ev_sub_data <= 127U) {
    if ((efx->msg_enable & 128U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "TX DMA Q %d reports descriptor fetch error. TX Q %d is disabled.\n",
                 ev_sub_data, ev_sub_data);
    } else {
    }
    efx_schedule_reset(efx, 9);
  } else {
    efx_sriov_desc_fetch_err(efx, ev_sub_data);
  }
  goto ldv_43241;
  default: ;
  goto ldv_43241;
  }
  ldv_43241: ;
  return;
}
}
int efx_nic_process_eventq(struct efx_channel *channel , int budget )
{
  struct efx_nic *efx ;
  unsigned int read_ptr ;
  efx_qword_t event ;
  efx_qword_t *p_event ;
  int ev_code ;
  int tx_packets ;
  int spent ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  efx = channel->efx;
  tx_packets = 0;
  spent = 0;
  read_ptr = channel->eventq_read_ptr;
  ldv_43281:
  p_event = efx_event(channel, read_ptr);
  event = *p_event;
  tmp = efx_event_present(& event);
  if (tmp == 0) {
    goto ldv_43269;
  } else {
  }
  p_event->u64[0] = 0xffffffffffffffffULL;
  read_ptr = read_ptr + 1U;
  ev_code = (int )(event.u64[0] >> 60);
  switch (ev_code) {
  case 0:
  efx_handle_rx_event(channel, (efx_qword_t const *)(& event));
  spent = spent + 1;
  if (spent == budget) {
    goto out;
  } else {
  }
  goto ldv_43273;
  case 2:
  tmp___0 = efx_handle_tx_event(channel, & event);
  tx_packets = tmp___0 + tx_packets;
  if ((unsigned int )tx_packets > efx->txq_entries) {
    spent = budget;
    goto out;
  } else {
  }
  goto ldv_43273;
  case 7:
  efx_handle_generated_event(channel, & event);
  goto ldv_43273;
  case 5:
  efx_handle_driver_event(channel, & event);
  goto ldv_43273;
  case 8:
  efx_sriov_event(channel, & event);
  goto ldv_43273;
  case 12:
  efx_mcdi_process_event(channel, & event);
  goto ldv_43273;
  case 6: ;
  if ((unsigned long )(efx->type)->handle_global_event != (unsigned long )((bool (* )(struct efx_channel * ,
                                                                                                 efx_qword_t * ))0)) {
    tmp___1 = (*((efx->type)->handle_global_event))(channel, & event);
    if ((int )tmp___1) {
      goto ldv_43273;
    } else {
    }
  } else {
  }
  default: ;
  if (((channel->efx)->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)(channel->efx)->net_dev, "channel %d unknown event type %d (data %08x:%08x)\n",
               channel->channel, ev_code, event.u32[1], event.u32[0]);
  } else {
  }
  }
  ldv_43273: ;
  goto ldv_43281;
  ldv_43269: ;
  out:
  channel->eventq_read_ptr = read_ptr;
  return (spent);
}
}
bool efx_nic_event_present(struct efx_channel *channel )
{
  efx_qword_t *tmp ;
  int tmp___0 ;
  {
  tmp = efx_event(channel, channel->eventq_read_ptr);
  tmp___0 = efx_event_present(tmp);
  return (tmp___0 != 0);
}
}
int efx_nic_probe_eventq(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  unsigned int entries ;
  int tmp ;
  {
  efx = channel->efx;
  entries = channel->eventq_mask + 1U;
  tmp = efx_alloc_special_buffer(efx, & channel->eventq, entries * 8U);
  return (tmp);
}
}
void efx_nic_init_eventq(struct efx_channel *channel )
{
  efx_oword_t reg ;
  struct efx_nic *efx ;
  struct _ddebug descriptor ;
  long tmp ;
  int tmp___0 ;
  unsigned long tmp___1 ;
  {
  efx = channel->efx;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_nic_init_eventq";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared";
    descriptor.format = "channel %d event queue in special buffers %d-%d\n";
    descriptor.lineno = 1420U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "channel %d event queue in special buffers %d-%d\n", channel->channel,
                           channel->eventq.index, (channel->eventq.index + channel->eventq.entries) - 1U);
    } else {
    }
  } else {
  }
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 > 2) {
    reg.u64[0] = 8589934592ULL;
    reg.u64[1] = 0ULL;
    efx_writeo_table(efx, & reg, 16187392U, (unsigned int )channel->channel);
  } else {
  }
  efx_init_special_buffer(efx, & channel->eventq);
  memset(channel->eventq.addr, 255, (size_t )channel->eventq.len);
  tmp___1 = __ffs((unsigned long )channel->eventq.entries);
  reg.u64[0] = (((unsigned long long )tmp___1 << 20) | (unsigned long long )channel->eventq.index) | 8388608ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, (efx->type)->evq_ptr_tbl_base, (unsigned int )channel->channel);
  (*((efx->type)->push_irq_moderation))(channel);
  return;
}
}
void efx_nic_fini_eventq(struct efx_channel *channel )
{
  efx_oword_t reg ;
  struct efx_nic *efx ;
  int tmp ;
  {
  efx = channel->efx;
  reg.u64[0] = 0ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, (efx->type)->evq_ptr_tbl_base, (unsigned int )channel->channel);
  tmp = efx_nic_rev(efx);
  if (tmp > 2) {
    efx_writeo_table(efx, & reg, 16187392U, (unsigned int )channel->channel);
  } else {
  }
  efx_fini_special_buffer(efx, & channel->eventq);
  return;
}
}
void efx_nic_remove_eventq(struct efx_channel *channel )
{
  {
  efx_free_special_buffer(channel->efx, & channel->eventq);
  return;
}
}
void efx_nic_event_test_start(struct efx_channel *channel )
{
  {
  channel->event_test_cpu = -1;
  __asm__ volatile ("": : : "memory");
  efx_magic_event(channel, (u32 )(channel->channel | 65792));
  return;
}
}
void efx_nic_generate_fill_event(struct efx_rx_queue *rx_queue )
{
  int tmp ;
  struct efx_channel *tmp___0 ;
  {
  tmp = efx_rx_queue_index(rx_queue);
  tmp___0 = efx_rx_queue_channel(rx_queue);
  efx_magic_event(tmp___0, (u32 )(tmp | 66048));
  return;
}
}
__inline static void efx_nic_interrupts(struct efx_nic *efx , bool enabled , bool force )
{
  efx_oword_t int_en_reg_ker ;
  {
  int_en_reg_ker.u64[0] = (((unsigned long long )efx->irq_level << 8) | ((unsigned long long )force << 3)) | (unsigned long long )enabled;
  int_en_reg_ker.u64[1] = 0ULL;
  efx_writeo(efx, & int_en_reg_ker, 16U);
  return;
}
}
void efx_nic_enable_interrupts(struct efx_nic *efx )
{
  {
  ((efx_oword_t *)efx->irq_status.addr)->u64[0] = 0ULL;
  ((efx_oword_t *)efx->irq_status.addr)->u64[1] = 0ULL;
  __asm__ volatile ("sfence": : : "memory");
  efx_nic_interrupts(efx, 1, 0);
  return;
}
}
void efx_nic_disable_interrupts(struct efx_nic *efx )
{
  {
  efx_nic_interrupts(efx, 0, 0);
  return;
}
}
void efx_nic_irq_test_start(struct efx_nic *efx )
{
  {
  efx->last_irq_cpu = -1;
  __asm__ volatile ("": : : "memory");
  efx_nic_interrupts(efx, 1, 1);
  return;
}
}
irqreturn_t efx_nic_fatal_interrupt(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t *int_ker ;
  efx_oword_t fatal_intr ;
  int error ;
  int mem_perr ;
  efx_oword_t reg ;
  bool tmp ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  int_ker = (efx_oword_t *)efx->irq_status.addr;
  efx_reado(efx, & fatal_intr, 560U);
  error = (int )fatal_intr.u64[0] & 4095;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "SYSTEM ERROR %08x:%08x:%08x:%08x status %08x:%08x:%08x:%08x: %s\n",
               int_ker->u32[3], int_ker->u32[2], int_ker->u32[1], int_ker->u32[0],
               fatal_intr.u32[3], fatal_intr.u32[2], fatal_intr.u32[1], fatal_intr.u32[0],
               error != 0 ? (char *)"disabling bus mastering" : (char *)"no recognised error");
  } else {
  }
  mem_perr = (int )(fatal_intr.u64[0] >> 8) & 1 || (int )fatal_intr.u64[0] & 1;
  if (mem_perr != 0) {
    efx_reado(efx, & reg, 608U);
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "SYSTEM ERROR: memory parity error %08x:%08x:%08x:%08x\n",
                 reg.u32[3], reg.u32[2], reg.u32[1], reg.u32[0]);
    } else {
    }
  } else {
  }
  pci_clear_master(efx->pci_dev);
  tmp = efx_nic_is_dual_func(efx);
  if ((int )tmp) {
    pci_clear_master(nic_data->pci_dev2);
  } else {
  }
  efx_nic_disable_interrupts(efx);
  if (efx->int_error_count == 0U || (long )efx->int_error_expire - (long )jiffies < 0L) {
    efx->int_error_count = 0U;
    efx->int_error_expire = (unsigned long )jiffies + 900000UL;
  } else {
  }
  efx->int_error_count = efx->int_error_count + 1U;
  if (efx->int_error_count <= 4U) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "SYSTEM ERROR - reset scheduled\n");
    } else {
    }
    efx_schedule_reset(efx, 6);
  } else {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "SYSTEM ERROR - max number of errors seen.NIC will be disabled\n");
    } else {
    }
    efx_schedule_reset(efx, 3);
  }
  return (1);
}
}
static irqreturn_t efx_legacy_interrupt(int irq , void *dev_id )
{
  struct efx_nic *efx ;
  efx_oword_t *int_ker ;
  irqreturn_t result ;
  struct efx_channel *channel ;
  efx_dword_t reg ;
  u32 queues ;
  int syserr ;
  irqreturn_t tmp ;
  long tmp___0 ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  efx_qword_t *event ;
  unsigned int tmp___1 ;
  int tmp___2 ;
  int pscr_ret_____0 ;
  void const *__vpp_verify___0 ;
  int pfo_ret_____3 ;
  int pfo_ret_____4 ;
  int pfo_ret_____5 ;
  int pfo_ret_____6 ;
  {
  efx = (struct efx_nic *)dev_id;
  int_ker = (efx_oword_t *)efx->irq_status.addr;
  result = 0;
  if (! efx->legacy_irq_enabled) {
    return (result);
  } else {
  }
  efx_readd(efx, & reg, 144U);
  queues = reg.u32[0];
  if (((1U << (int )efx->irq_level) & queues) != 0U) {
    syserr = (int )int_ker->u64[1] & 1;
    tmp___0 = ldv__builtin_expect(syserr != 0, 0L);
    if (tmp___0 != 0L) {
      tmp = efx_nic_fatal_interrupt(efx);
      return (tmp);
    } else {
    }
    __vpp_verify = 0;
    switch (4UL) {
    case 1UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
    goto ldv_43357;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43357;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43357;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
    goto ldv_43357;
    default:
    __bad_percpu_size();
    }
    ldv_43357:
    pscr_ret__ = pfo_ret__;
    goto ldv_43363;
    case 2UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43367;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43367;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43367;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
    goto ldv_43367;
    default:
    __bad_percpu_size();
    }
    ldv_43367:
    pscr_ret__ = pfo_ret_____0;
    goto ldv_43363;
    case 4UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43376;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43376;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43376;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
    goto ldv_43376;
    default:
    __bad_percpu_size();
    }
    ldv_43376:
    pscr_ret__ = pfo_ret_____1;
    goto ldv_43363;
    case 8UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43385;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43385;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43385;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
    goto ldv_43385;
    default:
    __bad_percpu_size();
    }
    ldv_43385:
    pscr_ret__ = pfo_ret_____2;
    goto ldv_43363;
    default:
    __bad_size_call_parameter();
    goto ldv_43363;
    }
    ldv_43363:
    efx->last_irq_cpu = pscr_ret__;
  } else {
  }
  if (queues != 0U) {
    efx->irq_zero_count = 0U;
    channel = efx->channel[0];
    goto ldv_43394;
    ldv_43393: ;
    if ((int )queues & 1) {
      efx_schedule_channel_irq(channel);
    } else {
    }
    queues = queues >> 1;
    channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
    ldv_43394: ;
    if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
      goto ldv_43393;
    } else {
    }
    result = 1;
  } else {
    tmp___1 = efx->irq_zero_count;
    efx->irq_zero_count = efx->irq_zero_count + 1U;
    if (tmp___1 == 0U) {
      result = 1;
    } else {
    }
    channel = efx->channel[0];
    goto ldv_43398;
    ldv_43397:
    event = efx_event(channel, channel->eventq_read_ptr);
    tmp___2 = efx_event_present(event);
    if (tmp___2 != 0) {
      efx_schedule_channel_irq(channel);
    } else {
      efx_nic_eventq_read_ack(channel);
    }
    channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
    ldv_43398: ;
    if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
      goto ldv_43397;
    } else {
    }
  }
  if ((unsigned int )result == 1U) {
    if (0) {
      if ((efx->msg_enable & 512U) != 0U) {
        __vpp_verify___0 = 0;
        switch (4UL) {
        case 1UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____3): "m" (cpu_number));
        goto ldv_43405;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
        goto ldv_43405;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
        goto ldv_43405;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
        goto ldv_43405;
        default:
        __bad_percpu_size();
        }
        ldv_43405:
        pscr_ret_____0 = pfo_ret_____3;
        goto ldv_43411;
        case 2UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____4): "m" (cpu_number));
        goto ldv_43415;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
        goto ldv_43415;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
        goto ldv_43415;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
        goto ldv_43415;
        default:
        __bad_percpu_size();
        }
        ldv_43415:
        pscr_ret_____0 = pfo_ret_____4;
        goto ldv_43411;
        case 4UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____5): "m" (cpu_number));
        goto ldv_43424;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
        goto ldv_43424;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
        goto ldv_43424;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
        goto ldv_43424;
        default:
        __bad_percpu_size();
        }
        ldv_43424:
        pscr_ret_____0 = pfo_ret_____5;
        goto ldv_43411;
        case 8UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____6): "m" (cpu_number));
        goto ldv_43433;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
        goto ldv_43433;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
        goto ldv_43433;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
        goto ldv_43433;
        default:
        __bad_percpu_size();
        }
        ldv_43433:
        pscr_ret_____0 = pfo_ret_____6;
        goto ldv_43411;
        default:
        __bad_size_call_parameter();
        goto ldv_43411;
        }
        ldv_43411:
        netdev_printk("\017", (struct net_device const *)efx->net_dev, "IRQ %d on CPU %d status %08x\n",
                      irq, pscr_ret_____0, reg.u32[0]);
      } else {
      }
    } else {
    }
  } else {
  }
  return (result);
}
}
static irqreturn_t efx_msi_interrupt(int irq , void *dev_id )
{
  struct efx_channel *channel ;
  struct efx_nic *efx ;
  efx_oword_t *int_ker ;
  int syserr ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  irqreturn_t tmp ;
  long tmp___0 ;
  int pscr_ret_____0 ;
  void const *__vpp_verify___0 ;
  int pfo_ret_____3 ;
  int pfo_ret_____4 ;
  int pfo_ret_____5 ;
  int pfo_ret_____6 ;
  {
  channel = *((struct efx_channel **)dev_id);
  efx = channel->efx;
  int_ker = (efx_oword_t *)efx->irq_status.addr;
  if (0) {
    if ((efx->msg_enable & 512U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_43455;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_43455;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_43455;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_43455;
      default:
      __bad_percpu_size();
      }
      ldv_43455:
      pscr_ret__ = pfo_ret__;
      goto ldv_43461;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_43465;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_43465;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_43465;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_43465;
      default:
      __bad_percpu_size();
      }
      ldv_43465:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_43461;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_43474;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_43474;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_43474;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_43474;
      default:
      __bad_percpu_size();
      }
      ldv_43474:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_43461;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_43483;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_43483;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_43483;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_43483;
      default:
      __bad_percpu_size();
      }
      ldv_43483:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_43461;
      default:
      __bad_size_call_parameter();
      goto ldv_43461;
      }
      ldv_43461:
      netdev_printk("\017", (struct net_device const *)efx->net_dev, "IRQ %d on CPU %d status %08x:%08x:%08x:%08x\n",
                    irq, pscr_ret__, int_ker->u32[3], int_ker->u32[2], int_ker->u32[1],
                    int_ker->u32[0]);
    } else {
    }
  } else {
  }
  if ((unsigned int )channel->channel == efx->irq_level) {
    syserr = (int )int_ker->u64[1] & 1;
    tmp___0 = ldv__builtin_expect(syserr != 0, 0L);
    if (tmp___0 != 0L) {
      tmp = efx_nic_fatal_interrupt(efx);
      return (tmp);
    } else {
    }
    __vpp_verify___0 = 0;
    switch (4UL) {
    case 1UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____3): "m" (cpu_number));
    goto ldv_43497;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
    goto ldv_43497;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
    goto ldv_43497;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
    goto ldv_43497;
    default:
    __bad_percpu_size();
    }
    ldv_43497:
    pscr_ret_____0 = pfo_ret_____3;
    goto ldv_43503;
    case 2UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____4): "m" (cpu_number));
    goto ldv_43507;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
    goto ldv_43507;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
    goto ldv_43507;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
    goto ldv_43507;
    default:
    __bad_percpu_size();
    }
    ldv_43507:
    pscr_ret_____0 = pfo_ret_____4;
    goto ldv_43503;
    case 4UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____5): "m" (cpu_number));
    goto ldv_43516;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
    goto ldv_43516;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
    goto ldv_43516;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
    goto ldv_43516;
    default:
    __bad_percpu_size();
    }
    ldv_43516:
    pscr_ret_____0 = pfo_ret_____5;
    goto ldv_43503;
    case 8UL: ;
    switch (4UL) {
    case 1UL:
    __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____6): "m" (cpu_number));
    goto ldv_43525;
    case 2UL:
    __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
    goto ldv_43525;
    case 4UL:
    __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
    goto ldv_43525;
    case 8UL:
    __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
    goto ldv_43525;
    default:
    __bad_percpu_size();
    }
    ldv_43525:
    pscr_ret_____0 = pfo_ret_____6;
    goto ldv_43503;
    default:
    __bad_size_call_parameter();
    goto ldv_43503;
    }
    ldv_43503:
    efx->last_irq_cpu = pscr_ret_____0;
  } else {
  }
  efx_schedule_channel_irq(channel);
  return (1);
}
}
void efx_nic_push_rx_indir_table(struct efx_nic *efx )
{
  size_t i ;
  efx_dword_t dword ;
  int tmp ;
  {
  i = 0UL;
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    return;
  } else {
  }
  i = 0UL;
  goto ldv_43539;
  ldv_43538:
  dword.u32[0] = efx->rx_indir_table[i];
  efx_writed(efx, & dword, (unsigned int )(i + 1028096UL) * 16U);
  i = i + 1UL;
  ldv_43539: ;
  if (i <= 127UL) {
    goto ldv_43538;
  } else {
  }
  return;
}
}
int efx_nic_init_interrupt(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  int rc ;
  irqreturn_t (*handler)(int , void * ) ;
  int tmp ;
  {
  if ((unsigned int )efx->interrupt_mode > 1U) {
    tmp = efx_nic_rev(efx);
    if (tmp > 1) {
      handler = & efx_legacy_interrupt;
    } else {
      handler = & falcon_legacy_interrupt_a1;
    }
    rc = request_irq((unsigned int )efx->legacy_irq, handler, 128UL, (char const *)(& efx->name),
                     (void *)efx);
    if (rc != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "failed to hook legacy IRQ %d\n",
                   (efx->pci_dev)->irq);
      } else {
      }
      goto fail1;
    } else {
    }
    return (0);
  } else {
  }
  channel = efx->channel[0];
  goto ldv_43550;
  ldv_43549:
  rc = request_irq((unsigned int )channel->irq, & efx_msi_interrupt, 256UL, (char const *)(& efx->channel_name) + (unsigned long )channel->channel,
                   (void *)(& efx->channel) + (unsigned long )channel->channel);
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to hook IRQ %d\n",
                 channel->irq);
    } else {
    }
    goto fail2;
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43550: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43549;
  } else {
  }
  return (0);
  fail2:
  channel = efx->channel[0];
  goto ldv_43553;
  ldv_43552:
  free_irq((unsigned int )channel->irq, (void *)(& efx->channel) + (unsigned long )channel->channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43553: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43552;
  } else {
  }
  fail1: ;
  return (rc);
}
}
void efx_nic_fini_interrupt(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  efx_oword_t reg ;
  int tmp ;
  {
  channel = efx->channel[0];
  goto ldv_43561;
  ldv_43560: ;
  if (channel->irq != 0) {
    free_irq((unsigned int )channel->irq, (void *)(& efx->channel) + (unsigned long )channel->channel);
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_43561: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_43560;
  } else {
  }
  tmp = efx_nic_rev(efx);
  if (tmp > 1) {
    efx_reado(efx, & reg, 144U);
  } else {
    falcon_irq_ack_a1(efx);
  }
  if (efx->legacy_irq != 0) {
    free_irq((unsigned int )efx->legacy_irq, (void *)efx);
  } else {
  }
  return;
}
}
void efx_nic_dimension_resources(struct efx_nic *efx , unsigned int sram_lim_qw )
{
  unsigned int vi_count ;
  unsigned int buftbl_min ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  unsigned int vi_dc_entries ;
  unsigned int buftbl_free ;
  unsigned int entries_per_vf ;
  unsigned int vf_limit ;
  unsigned int _max1___0 ;
  unsigned int _max2___0 ;
  unsigned int tmp ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  unsigned int tmp___0 ;
  bool tmp___1 ;
  {
  buftbl_min = (unsigned int )(((((unsigned long )efx->n_rx_channels + (unsigned long )(efx->n_tx_channels * 4U)) + (unsigned long )efx->n_channels * 4UL) * 32768UL) / 4096UL);
  _max1 = efx->n_channels;
  _max2 = efx->n_tx_channels * 4U;
  vi_count = _max1 > _max2 ? _max1 : _max2;
  tmp___1 = efx_sriov_wanted(efx);
  if ((int )tmp___1) {
    efx->vf_buftbl_base = buftbl_min;
    vi_dc_entries = 80U;
    _max1___0 = vi_count;
    _max2___0 = 128U;
    vi_count = _max1___0 > _max2___0 ? _max1___0 : _max2___0;
    buftbl_free = (sram_lim_qw - buftbl_min) - vi_count * vi_dc_entries;
    tmp = efx_vf_size(efx);
    entries_per_vf = (unsigned int )((unsigned long )vi_dc_entries + 32UL) * tmp;
    _min1 = buftbl_free / entries_per_vf;
    _min2 = 896U >> (int )efx->vi_scale;
    vf_limit = _min1 < _min2 ? _min1 : _min2;
    if (efx->vf_count > vf_limit) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "Reducing VF count from from %d to %d\n",
                   efx->vf_count, vf_limit);
      } else {
      }
      efx->vf_count = vf_limit;
    } else {
    }
    tmp___0 = efx_vf_size(efx);
    vi_count = efx->vf_count * tmp___0 + vi_count;
  } else {
  }
  efx->tx_dc_base = sram_lim_qw - vi_count * 16U;
  efx->rx_dc_base = efx->tx_dc_base - vi_count * 64U;
  return;
}
}
u32 efx_nic_fpga_ver(struct efx_nic *efx )
{
  efx_oword_t altera_build ;
  {
  efx_reado(efx, & altera_build, 768U);
  return ((u32 )altera_build.u64[0]);
}
}
void efx_nic_init_common(struct efx_nic *efx )
{
  efx_oword_t temp ;
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  temp.u64[0] = (unsigned long long )efx->tx_dc_base;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 1568U);
  temp.u64[0] = (unsigned long long )efx->rx_dc_base;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 1552U);
  temp.u64[0] = 1ULL;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 2592U);
  temp.u64[0] = 3ULL;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 2112U);
  temp.u64[0] = 56ULL;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 2128U);
  temp.u64[0] = efx->irq_status.dma_addr;
  temp.u64[1] = (unsigned int )efx->interrupt_mode <= 1U;
  efx_writeo(efx, & temp, 48U);
  tmp = efx_nic_rev(efx);
  if (tmp == 3 && (unsigned int )efx->interrupt_mode > 1U) {
    efx->irq_level = 31U;
  } else {
    efx->irq_level = 0U;
  }
  temp.u64[0] = 833223655424ULL;
  temp.u64[1] = 0ULL;
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 > 2) {
    temp.u64[0] = temp.u64[0] | 17592186044416ULL;
    temp.u64[1] = temp.u64[1];
  } else {
  }
  temp.u64[0] = ~ temp.u64[0];
  temp.u64[1] = ~ temp.u64[1];
  efx_writeo(efx, & temp, 560U);
  efx_nic_push_rx_indir_table(efx);
  efx_reado(efx, & temp, 2688U);
  temp.u64[0] = temp.u64[0];
  temp.u64[1] = (temp.u64[1] & 0xffffffffffffff00ULL) | 254ULL;
  temp.u64[0] = temp.u64[0] | 144115188075855872ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 262144ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0];
  temp.u64[1] = temp.u64[1] | 33554432ULL;
  temp.u64[0] = temp.u64[0] | 131072ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 576460752303423488ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = (temp.u64[0] & 0xffffffffffe7ffffULL) | 1048576ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 17592181850112ULL;
  temp.u64[1] = temp.u64[1];
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 > 1) {
    temp.u64[0] = temp.u64[0] | 128ULL;
    temp.u64[1] = temp.u64[1];
  } else {
  }
  efx_writeo(efx, & temp, 2688U);
  tmp___2 = efx_nic_rev(efx);
  if (tmp___2 > 1) {
    temp.u64[0] = 11015701ULL;
    temp.u64[1] = 0ULL;
    efx_writeo(efx, & temp, 2704U);
  } else {
  }
  return;
}
}
static struct efx_nic_reg const efx_nic_regs[83U] =
  { {0U, 1U, 3U},
        {16U, 1U, 3U},
        {32U, 2U, 3U},
        {48U, 1U, 3U},
        {64U, 2U, 3U},
        {192U, 1U, 3U},
        {256U, 3U, 3U},
        {256U, 1U, 2U},
        {272U, 1U, 2U},
        {288U, 1U, 2U},
        {304U, 1U, 2U},
        {320U, 1U, 2U},
        {512U, 1U, 2U},
        {528U, 1U, 2U},
        {544U, 1U, 2U},
        {592U, 2U, 3U},
        {608U, 1U, 3U},
        {624U, 1U, 3U},
        {768U, 1U, 3U},
        {784U, 1U, 3U},
        {800U, 1U, 2U},
        {816U, 1U, 2U},
        {832U, 1U, 2U},
        {1104U, 1U, 3U},
        {1120U, 1U, 3U},
        {1136U, 1U, 3U},
        {1536U, 1U, 3U},
        {1552U, 1U, 3U},
        {1568U, 1U, 3U},
        {1584U, 1U, 3U},
        {1632U, 1U, 3U},
        {1648U, 1U, 3U},
        {2048U, 1U, 3U},
        {2064U, 2U, 3U},
        {2112U, 1U, 3U},
        {2128U, 1U, 3U},
        {2144U, 2U, 3U},
        {2192U, 1U, 1U},
        {2256U, 3U, 3U},
        {2272U, 3U, 3U},
        {2288U, 3U, 3U},
        {2592U, 1U, 3U},
        {2608U, 1U, 1U},
        {2640U, 1U, 3U},
        {2688U, 1U, 3U},
        {2704U, 2U, 3U},
        {2784U, 2U, 2U},
        {2800U, 2U, 3U},
        {3072U, 1U, 2U},
        {3088U, 1U, 2U},
        {3104U, 1U, 2U},
        {3120U, 1U, 2U},
        {3136U, 1U, 2U},
        {3168U, 1U, 2U},
        {3200U, 1U, 2U},
        {3216U, 2U, 2U},
        {3232U, 1U, 2U},
        {3248U, 1U, 2U},
        {3584U, 1U, 2U},
        {3600U, 1U, 2U},
        {3648U, 1U, 2U},
        {3840U, 1U, 2U},
        {3856U, 1U, 2U},
        {3872U, 1U, 2U},
        {3888U, 1U, 2U},
        {3904U, 1U, 2U},
        {3920U, 1U, 2U},
        {3936U, 1U, 2U},
        {3952U, 1U, 2U},
        {4352U, 2U, 2U},
        {4608U, 1U, 2U},
        {4624U, 1U, 2U},
        {4640U, 1U, 2U},
        {4656U, 1U, 2U},
        {4672U, 1U, 2U},
        {4688U, 1U, 2U},
        {4720U, 1U, 2U},
        {4752U, 1U, 2U},
        {4816U, 1U, 2U},
        {4832U, 1U, 2U},
        {4864U, 1U, 2U},
        {4880U, 1U, 2U},
        {4896U, 1U, 2U}};
static struct efx_nic_reg_table const efx_nic_reg_tables[22U] =
  { {2816U, 2U, 2U, 16U, 16U},
        {4096U, 2U, 2U, 16U, 16U},
        {71680U, 1U, 1U, 16U, 4U},
        {15990784U, 2U, 2U, 16U, 4096U},
        {15990784U, 3U, 3U, 16U, 1024U},
        {71936U, 1U, 1U, 16U, 8U},
        {16056320U, 2U, 2U, 16U, 4096U},
        {16056320U, 3U, 3U, 16U, 1024U},
        {72192U, 1U, 1U, 16U, 4U},
        {16121856U, 2U, 2U, 16U, 4096U},
        {16121856U, 3U, 3U, 16U, 1024U},
        {98304U, 1U, 1U, 8U, 1024U},
        {8388608U, 2U, 3U, 8U, 1024U},
        {15728656U, 3U, 3U, 32U, 512U},
        {16187392U, 2U, 2U, 16U, 4096U},
        {16187392U, 3U, 3U, 16U, 1024U},
        {16252928U, 2U, 2U, 16U, 4096U},
        {16252928U, 3U, 3U, 16U, 1024U},
        {16449536U, 2U, 3U, 16U, 128U},
        {16646144U, 3U, 3U, 16U, 512U},
        {16711680U, 3U, 3U, 4U, 512U},
        {15728640U, 2U, 3U, 32U, 8192U}};
size_t efx_nic_get_regs_len(struct efx_nic *efx )
{
  struct efx_nic_reg const *reg ;
  struct efx_nic_reg_table const *table ;
  size_t len ;
  size_t __min1 ;
  size_t __min2 ;
  {
  len = 0UL;
  reg = (struct efx_nic_reg const *)(& efx_nic_regs);
  goto ldv_43611;
  ldv_43610: ;
  if ((int )(efx->type)->revision >= (int )reg->min_revision && (int )(efx->type)->revision <= (int )reg->max_revision) {
    len = len + 16UL;
  } else {
  }
  reg = reg + 1;
  ldv_43611: ;
  if ((unsigned long )reg < (unsigned long )((struct efx_nic_reg const *)(& efx_nic_regs) + 83UL)) {
    goto ldv_43610;
  } else {
  }
  table = (struct efx_nic_reg_table const *)(& efx_nic_reg_tables);
  goto ldv_43619;
  ldv_43618: ;
  if ((int )(efx->type)->revision >= (int )table->min_revision && (int )(efx->type)->revision <= (int )table->max_revision) {
    __min1 = (size_t )table->step;
    __min2 = 16UL;
    len = (size_t )table->rows * (__min1 < __min2 ? __min1 : __min2) + len;
  } else {
  }
  table = table + 1;
  ldv_43619: ;
  if ((unsigned long )table < (unsigned long )((struct efx_nic_reg_table const *)(& efx_nic_reg_tables) + 22UL)) {
    goto ldv_43618;
  } else {
  }
  return (len);
}
}
void efx_nic_get_regs(struct efx_nic *efx , void *buf )
{
  struct efx_nic_reg const *reg ;
  struct efx_nic_reg_table const *table ;
  size_t size ;
  size_t i ;
  size_t __min1 ;
  size_t __min2 ;
  int __ret_warn_on ;
  long tmp ;
  {
  reg = (struct efx_nic_reg const *)(& efx_nic_regs);
  goto ldv_43630;
  ldv_43629: ;
  if ((int )(efx->type)->revision >= (int )reg->min_revision && (int )(efx->type)->revision <= (int )reg->max_revision) {
    efx_reado(efx, (efx_oword_t *)buf, (unsigned int )reg->offset);
    buf = buf + 16UL;
  } else {
  }
  reg = reg + 1;
  ldv_43630: ;
  if ((unsigned long )reg < (unsigned long )((struct efx_nic_reg const *)(& efx_nic_regs) + 83UL)) {
    goto ldv_43629;
  } else {
  }
  table = (struct efx_nic_reg_table const *)(& efx_nic_reg_tables);
  goto ldv_43652;
  ldv_43651: ;
  if ((int )(efx->type)->revision < (int )table->min_revision || (int )(efx->type)->revision > (int )table->max_revision) {
    goto ldv_43636;
  } else {
  }
  __min1 = (size_t )table->step;
  __min2 = 16UL;
  size = __min1 < __min2 ? __min1 : __min2;
  i = 0UL;
  goto ldv_43649;
  ldv_43648: ;
  switch ((int )table->step) {
  case 4:
  efx_readd(efx, (efx_dword_t *)buf, (unsigned int )table->offset + (unsigned int )i * 4U);
  goto ldv_43641;
  case 8:
  efx_sram_readq(efx, efx->membase + (unsigned long )table->offset, (efx_qword_t *)buf,
                 (unsigned int )i);
  goto ldv_43641;
  case 16:
  efx_reado_table(efx, (efx_oword_t *)buf, (unsigned int )table->offset, (unsigned int )i);
  goto ldv_43641;
  case 32:
  efx_reado_table(efx, (efx_oword_t *)buf, (unsigned int )table->offset, (unsigned int )i * 2U);
  goto ldv_43641;
  default:
  __ret_warn_on = 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/nic.c.prepared",
                       2185);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  return;
  }
  ldv_43641:
  buf = buf + size;
  i = i + 1UL;
  ldv_43649: ;
  if ((size_t )table->rows > i) {
    goto ldv_43648;
  } else {
  }
  ldv_43636:
  table = table + 1;
  ldv_43652: ;
  if ((unsigned long )table < (unsigned long )((struct efx_nic_reg_table const *)(& efx_nic_reg_tables) + 22UL)) {
    goto ldv_43651;
  } else {
  }
  return;
}
}
void ldv_mutex_lock_65(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_66(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_67(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_68(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_69(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_70(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_71(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
extern void __might_sleep(char const * , int , int ) ;
extern int memcmp(void const * , void const * , size_t ) ;
__inline static int ldv_mutex_is_locked_8(struct mutex *lock ) ;
__inline static int ldv_mutex_is_locked_87(struct mutex *lock ) ;
int ldv_mutex_trylock_82(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_80(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_83(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_85(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_89(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_91(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_93(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_95(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_97(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_79(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_81(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_84(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_88(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_90(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_92(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_94(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_96(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_mdio_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_mdio_lock(struct mutex *lock ) ;
void ldv_mutex_lock_spi_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_spi_lock(struct mutex *lock ) ;
extern void __const_udelay(unsigned long ) ;
__inline static int test_ti_thread_flag(struct thread_info *ti , int flag )
{
  int tmp ;
  {
  tmp = variable_test_bit(flag, (unsigned long const volatile *)(& ti->flags));
  return (tmp);
}
}
extern int del_timer_sync(struct timer_list * ) ;
extern unsigned long round_jiffies_up(unsigned long ) ;
extern struct pci_dev *pci_dev_get(struct pci_dev * ) ;
extern void pci_dev_put(struct pci_dev * ) ;
extern struct pci_dev *pci_get_device(unsigned int , unsigned int , struct pci_dev * ) ;
extern long schedule_timeout_uninterruptible(long ) ;
__inline static int test_tsk_thread_flag(struct task_struct *tsk , int flag )
{
  int tmp ;
  {
  tmp = test_ti_thread_flag((struct thread_info *)tsk->stack, flag);
  return (tmp);
}
}
__inline static int signal_pending(struct task_struct *p )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = test_tsk_thread_flag(p, 2);
  tmp___0 = ldv__builtin_expect(tmp != 0, 0L);
  return ((int )tmp___0);
}
}
extern int _cond_resched(void) ;
extern int i2c_del_adapter(struct i2c_adapter * ) ;
__inline static bool efx_link_state_equal(struct efx_link_state const *left , struct efx_link_state const *right )
{
  {
  return ((bool )((((int const )left->up == (int const )right->up && (int const )left->fd == (int const )right->fd) && (int )((unsigned char )left->fc) == (int )((unsigned char )right->fc)) && (unsigned int )left->speed == (unsigned int )right->speed));
}
}
__inline static void efx_schedule_channel___1(struct efx_channel *channel )
{
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  {
  if (0) {
    if (((channel->efx)->msg_enable & 512U) != 0U) {
      __vpp_verify = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
      goto ldv_41799;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_41799;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_41799;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
      goto ldv_41799;
      default:
      __bad_percpu_size();
      }
      ldv_41799:
      pscr_ret__ = pfo_ret__;
      goto ldv_41805;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_41809;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_41809;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_41809;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
      goto ldv_41809;
      default:
      __bad_percpu_size();
      }
      ldv_41809:
      pscr_ret__ = pfo_ret_____0;
      goto ldv_41805;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_41818;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_41818;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_41818;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
      goto ldv_41818;
      default:
      __bad_percpu_size();
      }
      ldv_41818:
      pscr_ret__ = pfo_ret_____1;
      goto ldv_41805;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_41827;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_41827;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_41827;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
      goto ldv_41827;
      default:
      __bad_percpu_size();
      }
      ldv_41827:
      pscr_ret__ = pfo_ret_____2;
      goto ldv_41805;
      default:
      __bad_size_call_parameter();
      goto ldv_41805;
      }
      ldv_41805:
      netdev_printk("\017", (struct net_device const *)(channel->efx)->net_dev,
                    "channel %d scheduling NAPI poll on CPU%d\n", channel->channel,
                    pscr_ret__);
    } else {
    }
  } else {
  }
  channel->work_pending = 1;
  napi_schedule(& channel->napi_str);
  return;
}
}
__inline static void efx_schedule_channel_irq___0(struct efx_channel *channel )
{
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  {
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_41844;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_41844;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_41844;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_41844;
  default:
  __bad_percpu_size();
  }
  ldv_41844:
  pscr_ret__ = pfo_ret__;
  goto ldv_41850;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_41854;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_41854;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_41854;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_41854;
  default:
  __bad_percpu_size();
  }
  ldv_41854:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_41850;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_41863;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_41863;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_41863;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_41863;
  default:
  __bad_percpu_size();
  }
  ldv_41863:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_41850;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_41872;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_41872;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_41872;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_41872;
  default:
  __bad_percpu_size();
  }
  ldv_41872:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_41850;
  default:
  __bad_size_call_parameter();
  goto ldv_41850;
  }
  ldv_41850:
  channel->event_test_cpu = pscr_ret__;
  efx_schedule_channel___1(channel);
  return;
}
}
__inline static bool efx_spi_present(struct efx_spi_device const *spi )
{
  {
  return ((unsigned int )spi->size != 0U);
}
}
int falcon_spi_cmd(struct efx_nic *efx , struct efx_spi_device const *spi , unsigned int command ,
                   int address , void const *in , void *out , size_t len ) ;
int falcon_spi_wait_write(struct efx_nic *efx , struct efx_spi_device const *spi ) ;
int falcon_spi_read(struct efx_nic *efx , struct efx_spi_device const *spi , loff_t start ,
                    size_t len , size_t *retlen , u8 *buffer ) ;
int falcon_spi_write(struct efx_nic *efx , struct efx_spi_device const *spi , loff_t start ,
                     size_t len , size_t *retlen , u8 const *buffer ) ;
extern int i2c_bit_add_bus(struct i2c_adapter * ) ;
__inline static struct falcon_board *falcon_board(struct efx_nic *efx )
{
  struct falcon_nic_data *data ;
  {
  data = (struct falcon_nic_data *)efx->nic_data;
  return (& data->board);
}
}
int falcon_probe_board(struct efx_nic *efx , u16 revision_info ) ;
void falcon_drain_tx_fifo(struct efx_nic *efx ) ;
void falcon_reconfigure_mac_wrapper(struct efx_nic *efx ) ;
bool falcon_xmac_check_fault(struct efx_nic *efx ) ;
int falcon_reconfigure_xmac(struct efx_nic *efx ) ;
void falcon_update_stats_xmac(struct efx_nic *efx ) ;
void falcon_start_nic_stats(struct efx_nic *efx ) ;
void falcon_stop_nic_stats(struct efx_nic *efx ) ;
void falcon_setup_xaui(struct efx_nic *efx ) ;
void falcon_poll_xmac(struct efx_nic *efx ) ;
__inline static void _efx_writed_page_locked(struct efx_nic *efx , efx_dword_t *value ,
                                             unsigned int reg , unsigned int page )
{
  unsigned long flags ;
  raw_spinlock_t *tmp ;
  {
  if (page == 0U) {
    tmp = spinlock_check(& efx->biu_lock);
    flags = _raw_spin_lock_irqsave(tmp);
    efx_writed(efx, value, page * 8192U + reg);
    spin_unlock_irqrestore(& efx->biu_lock, flags);
  } else {
    efx_writed(efx, value, page * 8192U + reg);
  }
  return;
}
}
struct efx_phy_operations const falcon_sfx7101_phy_ops ;
struct efx_phy_operations const falcon_qt202x_phy_ops ;
struct efx_phy_operations const falcon_txc_phy_ops ;
static int falcon_reset_hw(struct efx_nic *efx , enum reset_type method ) ;
static unsigned int const large_eeprom_type = 83886221U;
static unsigned int const default_flash_type = 135221969U;
static void falcon_setsda(void *data , int state )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  {
  efx = (struct efx_nic *)data;
  efx_reado(efx, & reg, 528U);
  reg.u64[0] = (reg.u64[0] & 0xfffffffff7ffffffULL) | (state == 0 ? 134217728ULL : 0ULL);
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 528U);
  return;
}
}
static void falcon_setscl(void *data , int state )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  {
  efx = (struct efx_nic *)data;
  efx_reado(efx, & reg, 528U);
  reg.u64[0] = (reg.u64[0] & 0xfffffffffeffffffULL) | (state == 0 ? 16777216ULL : 0ULL);
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 528U);
  return;
}
}
static int falcon_getsda(void *data )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  {
  efx = (struct efx_nic *)data;
  efx_reado(efx, & reg, 528U);
  return ((int )(reg.u64[0] >> 11) & 1);
}
}
static int falcon_getscl(void *data )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  {
  efx = (struct efx_nic *)data;
  efx_reado(efx, & reg, 528U);
  return ((int )(reg.u64[0] >> 8) & 1);
}
}
static struct i2c_algo_bit_data const falcon_i2c_bit_operations =
     {0, & falcon_setsda, & falcon_setscl, & falcon_getsda, & falcon_getscl, 0, 0, 5,
    13};
static void falcon_push_irq_moderation(struct efx_channel *channel )
{
  efx_dword_t timer_cmd ;
  struct efx_nic *efx ;
  {
  efx = channel->efx;
  if (channel->irq_moderation != 0U) {
    timer_cmd.u32[0] = (channel->irq_moderation - 1U) | 8192U;
  } else {
    timer_cmd.u32[0] = 0U;
  }
  _efx_writed_page_locked(efx, & timer_cmd, 1056U, (unsigned int )channel->channel);
  return;
}
}
static void falcon_deconfigure_mac_wrapper(struct efx_nic *efx ) ;
static void falcon_prepare_flush(struct efx_nic *efx )
{
  {
  falcon_deconfigure_mac_wrapper(efx);
  msleep(10U);
  return;
}
}
void falcon_irq_ack_a1(struct efx_nic *efx )
{
  efx_dword_t reg ;
  {
  reg.u32[0] = 12053374U;
  efx_writed(efx, & reg, 80U);
  efx_readd(efx, & reg, 112U);
  return;
}
}
irqreturn_t falcon_legacy_interrupt_a1(int irq , void *dev_id )
{
  struct efx_nic *efx ;
  efx_oword_t *int_ker ;
  int syserr ;
  int queues ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  long tmp ;
  int pscr_ret_____0 ;
  void const *__vpp_verify___0 ;
  int pfo_ret_____3 ;
  int pfo_ret_____4 ;
  int pfo_ret_____5 ;
  int pfo_ret_____6 ;
  int pscr_ret_____1 ;
  void const *__vpp_verify___1 ;
  int pfo_ret_____7 ;
  int pfo_ret_____8 ;
  int pfo_ret_____9 ;
  int pfo_ret_____10 ;
  irqreturn_t tmp___0 ;
  long tmp___1 ;
  struct efx_channel *tmp___2 ;
  struct efx_channel *tmp___3 ;
  {
  efx = (struct efx_nic *)dev_id;
  int_ker = (efx_oword_t *)efx->irq_status.addr;
  tmp = ldv__builtin_expect((int_ker->u64[0] | int_ker->u64[1]) == 0ULL, 0L);
  if (tmp != 0L) {
    if (0) {
      if ((efx->msg_enable & 512U) != 0U) {
        __vpp_verify = 0;
        switch (4UL) {
        case 1UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
        goto ldv_42668;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
        goto ldv_42668;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
        goto ldv_42668;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
        goto ldv_42668;
        default:
        __bad_percpu_size();
        }
        ldv_42668:
        pscr_ret__ = pfo_ret__;
        goto ldv_42674;
        case 2UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
        goto ldv_42678;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
        goto ldv_42678;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
        goto ldv_42678;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
        goto ldv_42678;
        default:
        __bad_percpu_size();
        }
        ldv_42678:
        pscr_ret__ = pfo_ret_____0;
        goto ldv_42674;
        case 4UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
        goto ldv_42687;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
        goto ldv_42687;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
        goto ldv_42687;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
        goto ldv_42687;
        default:
        __bad_percpu_size();
        }
        ldv_42687:
        pscr_ret__ = pfo_ret_____1;
        goto ldv_42674;
        case 8UL: ;
        switch (4UL) {
        case 1UL:
        __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
        goto ldv_42696;
        case 2UL:
        __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
        goto ldv_42696;
        case 4UL:
        __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
        goto ldv_42696;
        case 8UL:
        __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
        goto ldv_42696;
        default:
        __bad_percpu_size();
        }
        ldv_42696:
        pscr_ret__ = pfo_ret_____2;
        goto ldv_42674;
        default:
        __bad_size_call_parameter();
        goto ldv_42674;
        }
        ldv_42674:
        netdev_printk("\017", (struct net_device const *)efx->net_dev, "IRQ %d on CPU %d not for me\n",
                      irq, pscr_ret__);
      } else {
      }
    } else {
    }
    return (0);
  } else {
  }
  __vpp_verify___0 = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____3): "m" (cpu_number));
  goto ldv_42710;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
  goto ldv_42710;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
  goto ldv_42710;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____3): "m" (cpu_number));
  goto ldv_42710;
  default:
  __bad_percpu_size();
  }
  ldv_42710:
  pscr_ret_____0 = pfo_ret_____3;
  goto ldv_42716;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____4): "m" (cpu_number));
  goto ldv_42720;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
  goto ldv_42720;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
  goto ldv_42720;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____4): "m" (cpu_number));
  goto ldv_42720;
  default:
  __bad_percpu_size();
  }
  ldv_42720:
  pscr_ret_____0 = pfo_ret_____4;
  goto ldv_42716;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____5): "m" (cpu_number));
  goto ldv_42729;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
  goto ldv_42729;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
  goto ldv_42729;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____5): "m" (cpu_number));
  goto ldv_42729;
  default:
  __bad_percpu_size();
  }
  ldv_42729:
  pscr_ret_____0 = pfo_ret_____5;
  goto ldv_42716;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____6): "m" (cpu_number));
  goto ldv_42738;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
  goto ldv_42738;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
  goto ldv_42738;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____6): "m" (cpu_number));
  goto ldv_42738;
  default:
  __bad_percpu_size();
  }
  ldv_42738:
  pscr_ret_____0 = pfo_ret_____6;
  goto ldv_42716;
  default:
  __bad_size_call_parameter();
  goto ldv_42716;
  }
  ldv_42716:
  efx->last_irq_cpu = pscr_ret_____0;
  if (0) {
    if ((efx->msg_enable & 512U) != 0U) {
      __vpp_verify___1 = 0;
      switch (4UL) {
      case 1UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____7): "m" (cpu_number));
      goto ldv_42751;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____7): "m" (cpu_number));
      goto ldv_42751;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____7): "m" (cpu_number));
      goto ldv_42751;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____7): "m" (cpu_number));
      goto ldv_42751;
      default:
      __bad_percpu_size();
      }
      ldv_42751:
      pscr_ret_____1 = pfo_ret_____7;
      goto ldv_42757;
      case 2UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____8): "m" (cpu_number));
      goto ldv_42761;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____8): "m" (cpu_number));
      goto ldv_42761;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____8): "m" (cpu_number));
      goto ldv_42761;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____8): "m" (cpu_number));
      goto ldv_42761;
      default:
      __bad_percpu_size();
      }
      ldv_42761:
      pscr_ret_____1 = pfo_ret_____8;
      goto ldv_42757;
      case 4UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____9): "m" (cpu_number));
      goto ldv_42770;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____9): "m" (cpu_number));
      goto ldv_42770;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____9): "m" (cpu_number));
      goto ldv_42770;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____9): "m" (cpu_number));
      goto ldv_42770;
      default:
      __bad_percpu_size();
      }
      ldv_42770:
      pscr_ret_____1 = pfo_ret_____9;
      goto ldv_42757;
      case 8UL: ;
      switch (4UL) {
      case 1UL:
      __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____10): "m" (cpu_number));
      goto ldv_42779;
      case 2UL:
      __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____10): "m" (cpu_number));
      goto ldv_42779;
      case 4UL:
      __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____10): "m" (cpu_number));
      goto ldv_42779;
      case 8UL:
      __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____10): "m" (cpu_number));
      goto ldv_42779;
      default:
      __bad_percpu_size();
      }
      ldv_42779:
      pscr_ret_____1 = pfo_ret_____10;
      goto ldv_42757;
      default:
      __bad_size_call_parameter();
      goto ldv_42757;
      }
      ldv_42757:
      netdev_printk("\017", (struct net_device const *)efx->net_dev, "IRQ %d on CPU %d status %08x:%08x:%08x:%08x\n",
                    irq, pscr_ret_____1, int_ker->u32[3], int_ker->u32[2], int_ker->u32[1],
                    int_ker->u32[0]);
    } else {
    }
  } else {
  }
  syserr = (int )int_ker->u64[1] & 1;
  tmp___1 = ldv__builtin_expect(syserr != 0, 0L);
  if (tmp___1 != 0L) {
    tmp___0 = efx_nic_fatal_interrupt(efx);
    return (tmp___0);
  } else {
  }
  queues = (int )(int_ker->u64[0] >> 40) & 15;
  int_ker->u64[0] = 0ULL;
  int_ker->u64[1] = 0ULL;
  __asm__ volatile ("sfence": : : "memory");
  falcon_irq_ack_a1(efx);
  if (queues & 1) {
    tmp___2 = efx_get_channel(efx, 0U);
    efx_schedule_channel_irq___0(tmp___2);
  } else {
  }
  if ((queues & 2) != 0) {
    tmp___3 = efx_get_channel(efx, 1U);
    efx_schedule_channel_irq___0(tmp___3);
  } else {
  }
  return (1);
}
}
static int falcon_spi_poll(struct efx_nic *efx )
{
  efx_oword_t reg ;
  {
  efx_reado(efx, & reg, 256U);
  return ((int )(reg.u64[0] >> 31) & 1 ? -16 : 0);
}
}
static int falcon_spi_wait(struct efx_nic *efx )
{
  unsigned long timeout ;
  int i ;
  int tmp ;
  int tmp___0 ;
  {
  timeout = (unsigned long )jiffies + 26UL;
  i = 0;
  goto ldv_42798;
  ldv_42797:
  tmp = falcon_spi_poll(efx);
  if (tmp == 0) {
    return (0);
  } else {
  }
  __const_udelay(42950UL);
  i = i + 1;
  ldv_42798: ;
  if (i <= 9) {
    goto ldv_42797;
  } else {
  }
  ldv_42806:
  tmp___0 = falcon_spi_poll(efx);
  if (tmp___0 == 0) {
    return (0);
  } else {
  }
  if ((long )jiffies - (long )timeout >= 0L) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for SPI\n");
    } else {
    }
    return (-110);
  } else {
  }
  schedule_timeout_uninterruptible(1L);
  goto ldv_42806;
}
}
int falcon_spi_cmd(struct efx_nic *efx , struct efx_spi_device const *spi , unsigned int command ,
                   int address , void const *in , void *out , size_t len )
{
  bool addressed ;
  bool reading ;
  efx_oword_t reg ;
  int rc ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  {
  addressed = address >= 0;
  reading = (unsigned long )out != (unsigned long )((void *)0);
  if (len > 16UL) {
    return (-22);
  } else {
  }
  rc = falcon_spi_poll(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((int )addressed) {
    reg.u64[0] = (unsigned long long )address;
    reg.u64[1] = 0ULL;
    efx_writeo(efx, & reg, 272U);
  } else {
  }
  if ((unsigned long )in != (unsigned long )((void const *)0)) {
    __len = len;
    __ret = memcpy((void *)(& reg), in, __len);
    efx_writeo(efx, & reg, 288U);
  } else {
  }
  reg.u64[0] = ((((((unsigned long long )spi->device_id << 24) | ((unsigned long long )len << 16)) | ((unsigned long long )reading << 15)) | ((int )addressed ? (unsigned long long )spi->addr_len << 8 : 0ULL)) | (unsigned long long )command) | 2147483648ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 256U);
  rc = falcon_spi_wait(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((unsigned long )out != (unsigned long )((void *)0)) {
    efx_reado(efx, & reg, 288U);
    __len___0 = len;
    __ret___0 = memcpy(out, (void const *)(& reg), __len___0);
  } else {
  }
  return (0);
}
}
static size_t falcon_spi_write_limit(struct efx_spi_device const *spi , size_t start )
{
  unsigned long _min1 ;
  size_t _min2 ;
  {
  _min1 = 16UL;
  _min2 = (size_t )spi->block_size - ((size_t )((unsigned int )spi->block_size - 1U) & start);
  return (_min1 < _min2 ? _min1 : _min2);
}
}
__inline static u8 efx_spi_munge_command(struct efx_spi_device const *spi , u8 const command ,
                                         unsigned int const address )
{
  {
  return ((((int )((u8 )(address >> 8)) & (int )((u8 )spi->munge_address)) << 3U) | (int )((u8 )command));
}
}
int falcon_spi_wait_write(struct efx_nic *efx , struct efx_spi_device const *spi )
{
  unsigned long timeout ;
  u8 status ;
  int rc ;
  {
  timeout = (unsigned long )jiffies + 4UL;
  ldv_42851:
  rc = falcon_spi_cmd(efx, spi, 5U, -1, 0, (void *)(& status), 1UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (((int )status & 1) == 0) {
    return (0);
  } else {
  }
  if ((long )jiffies - (long )timeout >= 0L) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "SPI write timeout on device %d last status=0x%02x\n",
                 spi->device_id, (int )status);
    } else {
    }
    return (-110);
  } else {
  }
  schedule_timeout_uninterruptible(1L);
  goto ldv_42851;
}
}
int falcon_spi_read(struct efx_nic *efx , struct efx_spi_device const *spi , loff_t start ,
                    size_t len , size_t *retlen , u8 *buffer )
{
  size_t block_len ;
  size_t pos ;
  unsigned int command ;
  int rc ;
  size_t _min1 ;
  unsigned long _min2 ;
  u8 tmp ;
  struct task_struct *tmp___0 ;
  int tmp___1 ;
  {
  pos = 0UL;
  rc = 0;
  goto ldv_42870;
  ldv_42869:
  _min1 = len - pos;
  _min2 = 16UL;
  block_len = _min1 < _min2 ? _min1 : _min2;
  tmp = efx_spi_munge_command(spi, 3, (unsigned int const )start + (unsigned int const )pos);
  command = (unsigned int )tmp;
  rc = falcon_spi_cmd(efx, spi, command, (int )((unsigned int )start + (unsigned int )pos),
                      0, (void *)(buffer + pos), block_len);
  if (rc != 0) {
    goto ldv_42867;
  } else {
  }
  pos = pos + block_len;
  __might_sleep("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                446, 0);
  _cond_resched();
  tmp___0 = get_current();
  tmp___1 = signal_pending(tmp___0);
  if (tmp___1 != 0) {
    rc = -4;
    goto ldv_42867;
  } else {
  }
  ldv_42870: ;
  if (pos < len) {
    goto ldv_42869;
  } else {
  }
  ldv_42867: ;
  if ((unsigned long )retlen != (unsigned long )((size_t *)0)) {
    *retlen = pos;
  } else {
  }
  return (rc);
}
}
int falcon_spi_write(struct efx_nic *efx , struct efx_spi_device const *spi , loff_t start ,
                     size_t len , size_t *retlen , u8 const *buffer )
{
  u8 verify_buffer[16U] ;
  size_t block_len ;
  size_t pos ;
  unsigned int command ;
  int rc ;
  size_t _min1 ;
  size_t _min2 ;
  size_t tmp ;
  u8 tmp___0 ;
  u8 tmp___1 ;
  int tmp___2 ;
  struct task_struct *tmp___3 ;
  int tmp___4 ;
  {
  pos = 0UL;
  rc = 0;
  goto ldv_42890;
  ldv_42889:
  rc = falcon_spi_cmd(efx, spi, 6U, -1, 0, 0, 0UL);
  if (rc != 0) {
    goto ldv_42884;
  } else {
  }
  _min1 = len - pos;
  tmp = falcon_spi_write_limit(spi, (size_t )((unsigned long long )start + (unsigned long long )pos));
  _min2 = tmp;
  block_len = _min1 < _min2 ? _min1 : _min2;
  tmp___0 = efx_spi_munge_command(spi, 2, (unsigned int const )start + (unsigned int const )pos);
  command = (unsigned int )tmp___0;
  rc = falcon_spi_cmd(efx, spi, command, (int )((unsigned int )start + (unsigned int )pos),
                      (void const *)(buffer + pos), 0, block_len);
  if (rc != 0) {
    goto ldv_42884;
  } else {
  }
  rc = falcon_spi_wait_write(efx, spi);
  if (rc != 0) {
    goto ldv_42884;
  } else {
  }
  tmp___1 = efx_spi_munge_command(spi, 3, (unsigned int const )start + (unsigned int const )pos);
  command = (unsigned int )tmp___1;
  rc = falcon_spi_cmd(efx, spi, command, (int )((unsigned int )start + (unsigned int )pos),
                      0, (void *)(& verify_buffer), block_len);
  tmp___2 = memcmp((void const *)(& verify_buffer), (void const *)(buffer + pos),
                   block_len);
  if (tmp___2 != 0) {
    rc = -5;
    goto ldv_42884;
  } else {
  }
  pos = pos + block_len;
  __might_sleep("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                495, 0);
  _cond_resched();
  tmp___3 = get_current();
  tmp___4 = signal_pending(tmp___3);
  if (tmp___4 != 0) {
    rc = -4;
    goto ldv_42884;
  } else {
  }
  ldv_42890: ;
  if (pos < len) {
    goto ldv_42889;
  } else {
  }
  ldv_42884: ;
  if ((unsigned long )retlen != (unsigned long )((size_t *)0)) {
    *retlen = pos;
  } else {
  }
  return (rc);
}
}
static void falcon_push_multicast_hash(struct efx_nic *efx )
{
  union efx_multicast_hash *mc_hash ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  {
  mc_hash = & efx->multicast_hash;
  tmp = ldv_mutex_is_locked_8(& efx->mac_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       518);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  efx_writeo(efx, (efx_oword_t *)(& mc_hash->oword), 3232U);
  efx_writeo(efx, (efx_oword_t *)(& mc_hash->oword) + 1UL, 3248U);
  return;
}
}
static void falcon_reset_macs(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  efx_oword_t mac_ctrl ;
  int count ;
  int tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  struct _ddebug descriptor ;
  long tmp___1 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    reg.u64[0] = 1ULL;
    reg.u64[1] = 0ULL;
    efx_writeo(efx, & reg, 4640U);
    count = 0;
    goto ldv_42905;
    ldv_42904:
    efx_reado(efx, & reg, 4640U);
    if ((reg.u64[0] & 1ULL) == 0ULL) {
      return;
    } else {
    }
    __const_udelay(42950UL);
    count = count + 1;
    ldv_42905: ;
    if (count <= 9999) {
      goto ldv_42904;
    } else {
    }
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for XMAC core reset\n");
    } else {
    }
  } else {
  }
  __ret_warn_on = nic_data->stats_disable_count == 0U;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       550);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  efx_reado(efx, & mac_ctrl, 3200U);
  mac_ctrl.u64[0] = mac_ctrl.u64[0] | 128ULL;
  mac_ctrl.u64[1] = mac_ctrl.u64[1];
  efx_writeo(efx, & mac_ctrl, 3200U);
  efx_reado(efx, & reg, 544U);
  reg.u64[0] = reg.u64[0] | 8388608ULL;
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = reg.u64[0] | 16777216ULL;
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = reg.u64[0] | 4194304ULL;
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 544U);
  count = 0;
  ldv_42912:
  efx_reado(efx, & reg, 544U);
  if ((((reg.u64[0] >> 23) & 1ULL) == 0ULL && ((reg.u64[0] >> 24) & 1ULL) == 0ULL) && ((reg.u64[0] >> 22) & 1ULL) == 0ULL) {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "falcon_reset_macs";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor.format = "Completed MAC reset after %d loops\n";
      descriptor.lineno = 570U;
      descriptor.flags = 0U;
      tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___1 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "Completed MAC reset after %d loops\n", count);
      } else {
      }
    } else {
    }
    goto ldv_42911;
  } else {
  }
  if (count > 20) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "MAC reset failed\n");
    } else {
    }
    goto ldv_42911;
  } else {
  }
  count = count + 1;
  __const_udelay(42950UL);
  goto ldv_42912;
  ldv_42911:
  efx_writeo(efx, & mac_ctrl, 3200U);
  falcon_setup_xaui(efx);
  return;
}
}
void falcon_drain_tx_fifo(struct efx_nic *efx )
{
  efx_oword_t reg ;
  int tmp ;
  {
  tmp = efx_nic_rev(efx);
  if (tmp <= 1 || (unsigned int )efx->loopback_mode != 0U) {
    return;
  } else {
  }
  efx_reado(efx, & reg, 3200U);
  if ((int )(reg.u64[0] >> 7) & 1) {
    return;
  } else {
  }
  falcon_reset_macs(efx);
  return;
}
}
static void falcon_deconfigure_mac_wrapper(struct efx_nic *efx )
{
  efx_oword_t reg ;
  int tmp ;
  {
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    return;
  } else {
  }
  efx_reado(efx, & reg, 2048U);
  reg.u64[0] = reg.u64[0] & 0xffff7fffffffffffULL;
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 2048U);
  falcon_drain_tx_fifo(efx);
  return;
}
}
void falcon_reconfigure_mac_wrapper(struct efx_nic *efx )
{
  struct efx_link_state *link_state ;
  efx_oword_t reg ;
  int link_speed ;
  int isolate ;
  int tmp ;
  int tmp___0 ;
  {
  link_state = & efx->link_state;
  isolate = (unsigned long )*((unsigned long volatile *)(& efx->reset_pending)) != 0UL;
  switch (link_state->speed) {
  case 10000U:
  link_speed = 3;
  goto ldv_42929;
  case 1000U:
  link_speed = 2;
  goto ldv_42929;
  case 100U:
  link_speed = 1;
  goto ldv_42929;
  default:
  link_speed = 0;
  goto ldv_42929;
  }
  ldv_42929:
  reg.u64[0] = (((unsigned long long )efx->promiscuous << 3) | (unsigned long long )link_speed) | 4294901780ULL;
  reg.u64[1] = 0ULL;
  tmp = efx_nic_rev(efx);
  if (tmp > 1) {
    reg.u64[0] = (reg.u64[0] & 0xffffffffffffff7fULL) | ((unsigned long long )(! link_state->up || isolate != 0) << 7);
    reg.u64[1] = reg.u64[1];
  } else {
  }
  efx_writeo(efx, & reg, 3200U);
  falcon_push_multicast_hash(efx);
  efx_reado(efx, & reg, 2048U);
  reg.u64[0] = reg.u64[0] | 1ULL;
  reg.u64[1] = reg.u64[1];
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 > 1) {
    reg.u64[0] = (reg.u64[0] & 0xffff7fffffffffffULL) | (isolate == 0 ? 140737488355328ULL : 0ULL);
    reg.u64[1] = reg.u64[1];
  } else {
  }
  efx_writeo(efx, & reg, 2048U);
  return;
}
}
static void falcon_stats_request(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  int __ret_warn_on ;
  long tmp ;
  int __ret_warn_on___0 ;
  long tmp___0 ;
  unsigned long tmp___1 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  __ret_warn_on = (int )nic_data->stats_pending;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       671);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __ret_warn_on___0 = nic_data->stats_disable_count != 0U;
  tmp___0 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       672);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if ((unsigned long )nic_data->stats_dma_done == (unsigned long )((u32 *)0)) {
    return;
  } else {
  }
  *(nic_data->stats_dma_done) = 0U;
  nic_data->stats_pending = 1;
  __asm__ volatile ("sfence": : : "memory");
  reg.u64[0] = efx->stats_buffer.dma_addr | 281474976710656ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3168U);
  tmp___1 = round_jiffies_up((unsigned long )jiffies + 125UL);
  mod_timer(& nic_data->stats_timer, tmp___1);
  return;
}
}
static void falcon_stats_complete(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if (! nic_data->stats_pending) {
    return;
  } else {
  }
  nic_data->stats_pending = 0;
  if (*(nic_data->stats_dma_done) == 4294967295U) {
    __asm__ volatile ("lfence": : : "memory");
    falcon_update_stats_xmac(efx);
  } else
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for statistics\n");
  } else {
  }
  return;
}
}
static void falcon_stats_timer_func(unsigned long context )
{
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  {
  efx = (struct efx_nic *)context;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  spin_lock(& efx->stats_lock);
  falcon_stats_complete(efx);
  if (nic_data->stats_disable_count == 0U) {
    falcon_stats_request(efx);
  } else {
  }
  spin_unlock(& efx->stats_lock);
  return;
}
}
static bool falcon_loopback_link_poll(struct efx_nic *efx )
{
  struct efx_link_state old_state ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  int __ret_warn_on___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  {
  old_state = efx->link_state;
  tmp = ldv_mutex_is_locked_87(& efx->mac_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       726);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __ret_warn_on___0 = ((66600958 >> (int )efx->loopback_mode) & 1) == 0;
  tmp___1 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       727);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  efx->link_state.fd = 1;
  efx->link_state.fc = efx->wanted_fc;
  efx->link_state.up = 1;
  efx->link_state.speed = 10000U;
  tmp___2 = efx_link_state_equal((struct efx_link_state const *)(& efx->link_state),
                                 (struct efx_link_state const *)(& old_state));
  if ((int )tmp___2 != 0) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  return ((bool )tmp___3);
}
}
static int falcon_reconfigure_port(struct efx_nic *efx )
{
  int rc ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp = efx_nic_rev(efx);
  __ret_warn_on = tmp > 2;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                       741);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if ((66600958 >> (int )efx->loopback_mode) & 1) {
    falcon_loopback_link_poll(efx);
  } else {
    (*((efx->phy_op)->poll))(efx);
  }
  falcon_stop_nic_stats(efx);
  falcon_deconfigure_mac_wrapper(efx);
  falcon_reset_macs(efx);
  (*((efx->phy_op)->reconfigure))(efx);
  rc = falcon_reconfigure_xmac(efx);
  tmp___1 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                         "i" (759), "i" (12UL));
    ldv_42965: ;
    goto ldv_42965;
  } else {
  }
  falcon_start_nic_stats(efx);
  efx_link_status_changed(efx);
  return (0);
}
}
static int falcon_gmii_wait(struct efx_nic *efx )
{
  efx_oword_t md_stat ;
  int count ;
  {
  count = 0;
  goto ldv_42972;
  ldv_42971:
  efx_reado(efx, & md_stat, 3152U);
  if ((md_stat.u64[0] & 1ULL) == 0ULL) {
    if ((int )(md_stat.u64[0] >> 1) & 1 || (int )(md_stat.u64[0] >> 2) & 1) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "error from GMII access %08x:%08x:%08x:%08x\n",
                   md_stat.u32[3], md_stat.u32[2], md_stat.u32[1], md_stat.u32[0]);
      } else {
      }
      return (-5);
    } else {
    }
    return (0);
  } else {
  }
  __const_udelay(42950UL);
  count = count + 1;
  ldv_42972: ;
  if (count <= 4999) {
    goto ldv_42971;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for GMII\n");
  } else {
  }
  return (-110);
}
}
static int falcon_mdio_write(struct net_device *net_dev , int prtad , int devad ,
                             u16 addr , u16 value )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  int rc ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  ldv_mutex_lock_88(& nic_data->mdio_lock);
  rc = falcon_gmii_wait(efx);
  if (rc != 0) {
    goto out;
  } else {
  }
  reg.u64[0] = (unsigned long long )addr;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3120U);
  reg.u64[0] = ((unsigned long long )prtad << 11) | ((unsigned long long )devad << 6);
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3136U);
  reg.u64[0] = (unsigned long long )value;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3072U);
  reg.u64[0] = 1ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3104U);
  rc = falcon_gmii_wait(efx);
  if (rc != 0) {
    reg.u64[0] = 16ULL;
    reg.u64[1] = 0ULL;
    efx_writeo(efx, & reg, 3104U);
    __const_udelay(42950UL);
  } else {
  }
  out:
  ldv_mutex_unlock_89(& nic_data->mdio_lock);
  return (rc);
}
}
static int falcon_mdio_read(struct net_device *net_dev , int prtad , int devad , u16 addr )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  ldv_mutex_lock_90(& nic_data->mdio_lock);
  rc = falcon_gmii_wait(efx);
  if (rc != 0) {
    goto out;
  } else {
  }
  reg.u64[0] = (unsigned long long )addr;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3120U);
  reg.u64[0] = ((unsigned long long )prtad << 11) | ((unsigned long long )devad << 6);
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3136U);
  reg.u64[0] = 2ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 3104U);
  rc = falcon_gmii_wait(efx);
  if (rc == 0) {
    efx_reado(efx, & reg, 3088U);
    rc = (int )reg.u64[0] & 65535;
  } else {
    reg.u64[0] = 16ULL;
    reg.u64[1] = 0ULL;
    efx_writeo(efx, & reg, 3104U);
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "falcon_mdio_read";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor.format = "read from MDIO %d register %d.%d, got error %d\n";
      descriptor.lineno = 899U;
      descriptor.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "read from MDIO %d register %d.%d, got error %d\n", prtad,
                             devad, (int )addr, rc);
      } else {
      }
    } else {
    }
  }
  out:
  ldv_mutex_unlock_91(& nic_data->mdio_lock);
  return (rc);
}
}
static int falcon_probe_port(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  int rc ;
  struct lock_class_key __key ;
  int tmp ;
  struct _ddebug descriptor ;
  phys_addr_t tmp___0 ;
  long tmp___1 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  switch (efx->phy_type) {
  case 3U:
  efx->phy_op = & falcon_sfx7101_phy_ops;
  goto ldv_43007;
  case 4U: ;
  case 9U:
  efx->phy_op = & falcon_qt202x_phy_ops;
  goto ldv_43007;
  case 1U:
  efx->phy_op = & falcon_txc_phy_ops;
  goto ldv_43007;
  default: ;
  if ((efx->msg_enable & 2U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "Unknown PHY type %d\n",
               efx->phy_type);
  } else {
  }
  return (-19);
  }
  ldv_43007:
  __mutex_init(& nic_data->mdio_lock, "&nic_data->mdio_lock", & __key);
  efx->mdio.mdio_read = & falcon_mdio_read;
  efx->mdio.mdio_write = & falcon_mdio_write;
  rc = (*((efx->phy_op)->probe))(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  efx->link_state.speed = 10000U;
  efx->link_state.fd = 1;
  tmp = efx_nic_rev(efx);
  if (tmp > 1) {
    efx->wanted_fc = 3U;
  } else {
    efx->wanted_fc = 2U;
  }
  if ((efx->mdio.mmds & 128U) != 0U) {
    efx->wanted_fc = (u8 )((unsigned int )efx->wanted_fc | 4U);
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & efx->stats_buffer, 256U);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "falcon_probe_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor.format = "stats buffer at %llx (virt %p phys %llx)\n";
    descriptor.lineno = 959U;
    descriptor.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      tmp___0 = virt_to_phys((void volatile *)efx->stats_buffer.addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "stats buffer at %llx (virt %p phys %llx)\n", efx->stats_buffer.dma_addr,
                           efx->stats_buffer.addr, tmp___0);
    } else {
    }
  } else {
  }
  nic_data->stats_dma_done = (u32 *)efx->stats_buffer.addr + 212U;
  return (0);
}
}
static void falcon_remove_port(struct efx_nic *efx )
{
  {
  (*((efx->phy_op)->remove))(efx);
  efx_nic_free_buffer(efx, & efx->stats_buffer);
  return;
}
}
static bool falcon_handle_global_event(struct efx_channel *channel , efx_qword_t *event )
{
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  efx = channel->efx;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if (((int )(event->u64[0] >> 7) & 1 || (int )(event->u64[0] >> 9) & 1) || (int )(event->u64[0] >> 10) & 1) {
    return (1);
  } else {
  }
  tmp = efx_nic_rev(efx);
  if (tmp == 2 && (int )(event->u64[0] >> 11) & 1) {
    nic_data->xmac_poll_required = 1;
    return (1);
  } else {
  }
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 <= 1 ? (int )(event->u64[0] >> 11) & 1 : (int )(event->u64[0] >> 12) & 1) {
    if ((efx->msg_enable & 64U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "channel %d seen global RX_RESET event. Resetting.\n",
                 channel->channel);
    } else {
    }
    atomic_inc(& efx->rx_reset);
    tmp___0 = efx_nic_rev(efx);
    efx_schedule_reset(efx, tmp___0 <= 1 ? 7 : 3);
    return (1);
  } else {
  }
  return (0);
}
}
static int falcon_read_nvram(struct efx_nic *efx , struct falcon_nvconfig *nvconfig_out )
{
  struct falcon_nic_data *nic_data ;
  struct falcon_nvconfig *nvconfig ;
  struct efx_spi_device *spi ;
  void *region ;
  int rc ;
  int magic_num ;
  int struct_ver ;
  __le16 *word ;
  __le16 *limit ;
  u32 csum ;
  bool tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  size_t __len ;
  void *__ret ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  tmp___0 = efx_spi_present((struct efx_spi_device const *)(& nic_data->spi_flash));
  if ((int )tmp___0) {
    spi = & nic_data->spi_flash;
  } else {
    tmp = efx_spi_present((struct efx_spi_device const *)(& nic_data->spi_eeprom));
    if ((int )tmp) {
      spi = & nic_data->spi_eeprom;
    } else {
      return (-22);
    }
  }
  region = kmalloc(1024UL, 208U);
  if ((unsigned long )region == (unsigned long )((void *)0)) {
    return (-12);
  } else {
  }
  nvconfig = (struct falcon_nvconfig *)region + 768U;
  ldv_mutex_lock_92(& nic_data->spi_lock);
  rc = falcon_spi_read(efx, (struct efx_spi_device const *)spi, 0LL, 1024UL, 0,
                       (u8 *)region);
  ldv_mutex_unlock_93(& nic_data->spi_lock);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      tmp___1 = efx_spi_present((struct efx_spi_device const *)(& nic_data->spi_flash));
      netdev_err((struct net_device const *)efx->net_dev, "Failed to read %s\n",
                 (int )tmp___1 ? (char *)"flash" : (char *)"EEPROM");
    } else {
    }
    rc = -5;
    goto out;
  } else {
  }
  magic_num = (int )nvconfig->board_magic_num;
  struct_ver = (int )nvconfig->board_struct_ver;
  rc = -22;
  if (magic_num != 64028) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "NVRAM bad magic 0x%x\n",
                 magic_num);
    } else {
    }
    goto out;
  } else {
  }
  if (struct_ver <= 1) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "NVRAM has ancient version 0x%x\n",
                 struct_ver);
    } else {
    }
    goto out;
  } else
  if (struct_ver <= 3) {
    word = & nvconfig->board_magic_num;
    limit = (__le16 *)nvconfig + 1U;
  } else {
    word = (__le16 *)region;
    limit = (__le16 *)region + 1024U;
  }
  csum = 0U;
  goto ldv_43040;
  ldv_43039:
  csum = (u32 )*word + csum;
  word = word + 1;
  ldv_43040: ;
  if ((unsigned long )word < (unsigned long )limit) {
    goto ldv_43039;
  } else {
  }
  if ((~ csum & 65535U) != 0U) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "NVRAM has incorrect checksum\n");
    } else {
    }
    goto out;
  } else {
  }
  rc = 0;
  if ((unsigned long )nvconfig_out != (unsigned long )((struct falcon_nvconfig *)0)) {
    __len = 200UL;
    if (__len > 63UL) {
      __ret = memcpy((void *)nvconfig_out, (void const *)nvconfig, __len);
    } else {
      __ret = memcpy((void *)nvconfig_out, (void const *)nvconfig, __len);
    }
  } else {
  }
  out:
  kfree((void const *)region);
  return (rc);
}
}
static int falcon_test_nvram(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = falcon_read_nvram(efx, 0);
  return (tmp);
}
}
static struct efx_nic_register_test const falcon_b0_register_tests[18U] =
  { {0U, {.u32 = {262143U, 262143U, 262143U, 262143U}}},
        {2048U, {.u32 = {4294967294U, 98303U, 0U, 0U}}},
        {2640U, {.u32 = {2147418167U, 0U, 0U, 0U}}},
        {2688U, {.u32 = {4294901376U, 536870911U, 33554686U, 8388607U}}},
        {3200U, {.u32 = {4294901760U, 0U, 0U, 0U}}},
        {1568U, {.u32 = {2097151U, 0U, 0U, 0U}}},
        {2112U, {.u32 = {15U, 0U, 0U, 0U}}},
        {2128U, {.u32 = {1023U, 0U, 0U, 0U}}},
        {592U, {.u32 = {4095U, 0U, 0U, 0U}}},
        {3600U, {.u32 = {29495U, 0U, 0U, 0U}}},
        {3872U, {.u32 = {7967U, 0U, 0U, 0U}}},
        {4640U, {.u32 = {3176U, 0U, 0U, 0U}}},
        {4656U, {.u32 = {524644U, 0U, 0U, 0U}}},
        {4672U, {.u32 = {118491660U, 0U, 0U, 0U}}},
        {4832U, {.u32 = {8184U, 0U, 0U, 0U}}},
        {4720U, {.u32 = {4294901761U, 0U, 0U, 0U}}},
        {4608U, {.u32 = {4294967295U, 0U, 0U, 0U}}},
        {4880U, {.u32 = {261903U, 0U, 0U, 0U}}}};
static int falcon_b0_test_chip(struct efx_nic *efx , struct efx_self_tests *tests )
{
  enum reset_type reset_method ;
  int rc ;
  int rc2 ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  reset_method = 0;
  ldv_mutex_lock_94(& efx->mac_lock);
  if (efx->loopback_modes != 0ULL) {
    if ((efx->loopback_modes & 8ULL) != 0ULL) {
      efx->loopback_mode = 3;
    } else {
      tmp = __ffs((unsigned long )efx->loopback_modes);
      efx->loopback_mode = (enum efx_loopback_mode )tmp;
    }
  } else {
  }
  __efx_reconfigure_port(efx);
  ldv_mutex_unlock_95(& efx->mac_lock);
  efx_reset_down(efx, reset_method);
  tmp___0 = efx_nic_test_registers(efx, (struct efx_nic_register_test const *)(& falcon_b0_register_tests),
                                   18UL);
  tests->registers = tmp___0 != 0 ? -1 : 1;
  rc = falcon_reset_hw(efx, reset_method);
  rc2 = efx_reset_up(efx, reset_method, rc == 0);
  return (rc != 0 ? rc : rc2);
}
}
static enum reset_type falcon_map_reset_reason(enum reset_type reason )
{
  {
  switch ((unsigned int )reason) {
  case 7U: ;
  case 8U: ;
  case 9U: ;
  case 10U: ;
  return (0);
  default: ;
  return (1);
  }
}
}
static int falcon_map_reset_flags(u32 *flags )
{
  {
  if ((*flags & 126U) == 126U) {
    *flags = *flags & 4294967169U;
    return (2);
  } else {
  }
  if ((*flags & 124U) == 124U) {
    *flags = *flags & 4294967171U;
    return (1);
  } else {
  }
  if ((*flags & 60U) == 60U) {
    *flags = *flags & 4294967235U;
    return (0);
  } else {
  }
  return (-22);
}
}
static int __falcon_reset_hw(struct efx_nic *efx , enum reset_type method )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t glb_ctl_reg_ker ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  bool tmp___0 ;
  struct _ddebug descriptor___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  struct _ddebug descriptor___1 ;
  long tmp___3 ;
  struct _ddebug descriptor___2 ;
  long tmp___4 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "__falcon_reset_hw";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor.format = "performing %s hardware reset\n";
    descriptor.lineno = 1217U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "performing %s hardware reset\n", (unsigned int )method < (unsigned int )efx_reset_type_max ? efx_reset_type_names[(unsigned int )method] : (char const * )"(invalid)");
    } else {
    }
  } else {
  }
  if ((unsigned int )method == 2U) {
    rc = pci_save_state(efx->pci_dev);
    if (rc != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "failed to backup PCI state of primary function prior to hardware reset\n");
      } else {
      }
      goto fail1;
    } else {
    }
    tmp___0 = efx_nic_is_dual_func(efx);
    if ((int )tmp___0) {
      rc = pci_save_state(nic_data->pci_dev2);
      if (rc != 0) {
        if ((int )efx->msg_enable & 1) {
          netdev_err((struct net_device const *)efx->net_dev, "failed to backup PCI state of secondary function prior to hardware reset\n");
        } else {
        }
        goto fail2;
      } else {
      }
    } else {
    }
    glb_ctl_reg_ker.u64[0] = 15ULL;
    glb_ctl_reg_ker.u64[1] = 0ULL;
  } else {
    glb_ctl_reg_ker.u64[0] = (unsigned int )method == 0U ? 0xa60200000000000fULL : 2738751523394682895ULL;
    glb_ctl_reg_ker.u64[1] = 0ULL;
  }
  efx_writeo(efx, & glb_ctl_reg_ker, 544U);
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "__falcon_reset_hw";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor___0.format = "waiting for hardware reset\n";
    descriptor___0.lineno = 1259U;
    descriptor___0.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "waiting for hardware reset\n");
    } else {
    }
  } else {
  }
  schedule_timeout_uninterruptible(12L);
  if ((unsigned int )method == 2U) {
    tmp___2 = efx_nic_is_dual_func(efx);
    if ((int )tmp___2) {
      pci_restore_state(nic_data->pci_dev2);
    } else {
    }
    pci_restore_state(efx->pci_dev);
    if ((int )efx->msg_enable & 1) {
      descriptor___1.modname = "sfc";
      descriptor___1.function = "__falcon_reset_hw";
      descriptor___1.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor___1.format = "successfully restored PCI config\n";
      descriptor___1.lineno = 1268U;
      descriptor___1.flags = 0U;
      tmp___3 = ldv__builtin_expect((long )descriptor___1.flags & 1L, 0L);
      if (tmp___3 != 0L) {
        __dynamic_netdev_dbg(& descriptor___1, (struct net_device const *)efx->net_dev,
                             "successfully restored PCI config\n");
      } else {
      }
    } else {
    }
  } else {
  }
  efx_reado(efx, & glb_ctl_reg_ker, 544U);
  if ((int )glb_ctl_reg_ker.u64[0] & 1) {
    rc = -110;
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for hardware reset\n");
    } else {
    }
    goto fail3;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor___2.modname = "sfc";
    descriptor___2.function = "__falcon_reset_hw";
    descriptor___2.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor___2.format = "hardware reset complete\n";
    descriptor___2.lineno = 1279U;
    descriptor___2.flags = 0U;
    tmp___4 = ldv__builtin_expect((long )descriptor___2.flags & 1L, 0L);
    if (tmp___4 != 0L) {
      __dynamic_netdev_dbg(& descriptor___2, (struct net_device const *)efx->net_dev,
                           "hardware reset complete\n");
    } else {
    }
  } else {
  }
  return (0);
  fail2:
  pci_restore_state(efx->pci_dev);
  fail1: ;
  fail3: ;
  return (rc);
}
}
static int falcon_reset_hw(struct efx_nic *efx , enum reset_type method )
{
  struct falcon_nic_data *nic_data ;
  int rc ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  ldv_mutex_lock_96(& nic_data->spi_lock);
  rc = __falcon_reset_hw(efx, method);
  ldv_mutex_unlock_97(& nic_data->spi_lock);
  return (rc);
}
}
static void falcon_monitor(struct efx_nic *efx )
{
  bool link_changed ;
  int rc ;
  int tmp ;
  long tmp___0 ;
  struct falcon_board *tmp___1 ;
  int __ret_warn_on ;
  long tmp___2 ;
  long tmp___3 ;
  {
  tmp = mutex_is_locked(& efx->mac_lock);
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                         "i" (1308), "i" (12UL));
    ldv_43099: ;
    goto ldv_43099;
  } else {
  }
  tmp___1 = falcon_board(efx);
  rc = (*((tmp___1->type)->monitor))(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "Board sensor %s; shutting down PHY\n",
                 rc == -34 ? (char *)"reported fault" : (char *)"failed");
    } else {
    }
    efx->phy_mode = (enum efx_phy_mode )((unsigned int )efx->phy_mode | 2U);
    rc = __efx_reconfigure_port(efx);
    __ret_warn_on = rc != 0;
    tmp___2 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
    if (tmp___2 != 0L) {
      warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                         1317);
    } else {
    }
    ldv__builtin_expect(__ret_warn_on != 0, 0L);
  } else {
  }
  if ((66600958 >> (int )efx->loopback_mode) & 1) {
    link_changed = falcon_loopback_link_poll(efx);
  } else {
    link_changed = (*((efx->phy_op)->poll))(efx);
  }
  if ((int )link_changed) {
    falcon_stop_nic_stats(efx);
    falcon_deconfigure_mac_wrapper(efx);
    falcon_reset_macs(efx);
    rc = falcon_reconfigure_xmac(efx);
    tmp___3 = ldv__builtin_expect(rc != 0, 0L);
    if (tmp___3 != 0L) {
      __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                           "i" (1331), "i" (12UL));
      ldv_43102: ;
      goto ldv_43102;
    } else {
    }
    falcon_start_nic_stats(efx);
    efx_link_status_changed(efx);
  } else {
  }
  falcon_poll_xmac(efx);
  return;
}
}
static int falcon_reset_sram(struct efx_nic *efx )
{
  efx_oword_t srm_cfg_reg_ker ;
  efx_oword_t gpio_cfg_reg_ker ;
  int count ;
  struct _ddebug descriptor ;
  long tmp ;
  struct _ddebug descriptor___0 ;
  long tmp___0 ;
  {
  efx_reado(efx, & gpio_cfg_reg_ker, 528U);
  gpio_cfg_reg_ker.u64[0] = gpio_cfg_reg_ker.u64[0] | 33554432ULL;
  gpio_cfg_reg_ker.u64[1] = gpio_cfg_reg_ker.u64[1];
  gpio_cfg_reg_ker.u64[0] = gpio_cfg_reg_ker.u64[0] | 131072ULL;
  gpio_cfg_reg_ker.u64[1] = gpio_cfg_reg_ker.u64[1];
  efx_writeo(efx, & gpio_cfg_reg_ker, 528U);
  srm_cfg_reg_ker.u64[0] = 8ULL;
  srm_cfg_reg_ker.u64[1] = 0ULL;
  efx_writeo(efx, & srm_cfg_reg_ker, 1584U);
  count = 0;
  ldv_43112: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "falcon_reset_sram";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor.format = "waiting for SRAM reset (attempt %d)...\n";
    descriptor.lineno = 1365U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "waiting for SRAM reset (attempt %d)...\n", count);
    } else {
    }
  } else {
  }
  schedule_timeout_uninterruptible(5L);
  efx_reado(efx, & srm_cfg_reg_ker, 1584U);
  if (((srm_cfg_reg_ker.u64[0] >> 3) & 1ULL) == 0ULL) {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor___0.modname = "sfc";
      descriptor___0.function = "falcon_reset_sram";
      descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor___0.format = "SRAM reset complete\n";
      descriptor___0.lineno = 1374U;
      descriptor___0.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                             "SRAM reset complete\n");
      } else {
      }
    } else {
    }
    return (0);
  } else {
  }
  count = count + 1;
  if (count <= 19) {
    goto ldv_43112;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for SRAM reset\n");
  } else {
  }
  return (-110);
}
}
static void falcon_spi_device_init(struct efx_nic *efx , struct efx_spi_device *spi_device ,
                                   unsigned int device_id , u32 device_type )
{
  {
  if (device_type != 0U) {
    spi_device->device_id = (int )device_id;
    spi_device->size = (unsigned int )(1 << ((int )device_type & 31));
    spi_device->addr_len = (device_type >> 6) & 3U;
    spi_device->munge_address = (unsigned char )(spi_device->size == 512U && spi_device->addr_len == 1U);
    spi_device->erase_command = (u8 )(device_type >> 8);
    spi_device->erase_size = (unsigned int )(1 << ((int )(device_type >> 16) & 31));
    spi_device->block_size = (unsigned int )(1 << ((int )(device_type >> 24) & 31));
  } else {
    spi_device->size = 0U;
  }
  return;
}
}
static int falcon_probe_nvconfig(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  struct falcon_nvconfig *nvconfig ;
  int rc ;
  void *tmp ;
  size_t __len ;
  void *__ret ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  tmp = kmalloc(200UL, 208U);
  nvconfig = (struct falcon_nvconfig *)tmp;
  if ((unsigned long )nvconfig == (unsigned long )((struct falcon_nvconfig *)0)) {
    return (-12);
  } else {
  }
  rc = falcon_read_nvram(efx, nvconfig);
  if (rc != 0) {
    goto out;
  } else {
  }
  efx->phy_type = (unsigned int )nvconfig->board_v2.port0_phy_type;
  efx->mdio.prtad = (int )nvconfig->board_v2.port0_phy_addr;
  if ((unsigned int )nvconfig->board_struct_ver > 2U) {
    falcon_spi_device_init(efx, & nic_data->spi_flash, 1U, nvconfig->board_v3.spi_device_type[1]);
    falcon_spi_device_init(efx, & nic_data->spi_eeprom, 0U, nvconfig->board_v3.spi_device_type[0]);
  } else {
  }
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& (efx->net_dev)->perm_addr), (void const *)(& nvconfig->mac_address),
                     __len);
  } else {
    __ret = memcpy((void *)(& (efx->net_dev)->perm_addr), (void const *)(& nvconfig->mac_address),
                             __len);
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "falcon_probe_nvconfig";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor.format = "PHY is %d phy_id %d\n";
    descriptor.lineno = 1442U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "PHY is %d phy_id %d\n", efx->phy_type, efx->mdio.prtad);
    } else {
    }
  } else {
  }
  rc = falcon_probe_board(efx, (int )nvconfig->board_v2.board_revision);
  out:
  kfree((void const *)nvconfig);
  return (rc);
}
}
static void falcon_dimension_resources(struct efx_nic *efx )
{
  {
  efx->rx_dc_base = 131072U;
  efx->tx_dc_base = 155648U;
  return;
}
}
static void falcon_probe_spi_devices(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t nic_stat ;
  efx_oword_t gpio_ctl ;
  efx_oword_t ee_vpd_cfg ;
  int boot_dev ;
  struct _ddebug descriptor ;
  long tmp ;
  struct _ddebug descriptor___0 ;
  long tmp___0 ;
  struct lock_class_key __key ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  efx_reado(efx, & gpio_ctl, 528U);
  efx_reado(efx, & nic_stat, 512U);
  efx_reado(efx, & ee_vpd_cfg, 320U);
  if ((int )(gpio_ctl.u64[0] >> 3) & 1) {
    boot_dev = (int )(nic_stat.u64[0] >> 9) & 1;
    if ((efx->msg_enable & 2U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "falcon_probe_spi_devices";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor.format = "Booted from %s\n";
      descriptor.lineno = 1473U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "Booted from %s\n", boot_dev == 1 ? (char *)"flash" : (char *)"EEPROM");
      } else {
      }
    } else {
    }
  } else {
    boot_dev = -1;
    if ((efx->msg_enable & 2U) != 0U) {
      descriptor___0.modname = "sfc";
      descriptor___0.function = "falcon_probe_spi_devices";
      descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
      descriptor___0.format = "Booted from internal ASIC settings; setting SPI config\n";
      descriptor___0.lineno = 1480U;
      descriptor___0.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                             "Booted from internal ASIC settings; setting SPI config\n");
      } else {
      }
    } else {
    }
    ee_vpd_cfg.u64[0] = 0ULL;
    ee_vpd_cfg.u64[1] = 522136081798266880ULL;
    efx_writeo(efx, & ee_vpd_cfg, 320U);
  }
  __mutex_init(& nic_data->spi_lock, "&nic_data->spi_lock", & __key);
  if (boot_dev == 1) {
    falcon_spi_device_init(efx, & nic_data->spi_flash, 1U, default_flash_type);
  } else {
  }
  if (boot_dev == 0) {
    falcon_spi_device_init(efx, & nic_data->spi_eeprom, 0U, large_eeprom_type);
  } else {
  }
  return;
}
}
static int falcon_probe_nic(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  struct falcon_board *board ;
  int rc ;
  void *tmp ;
  u32 tmp___0 ;
  efx_oword_t nic_stat ;
  struct pci_dev *dev ;
  u8 pci_rev ;
  int tmp___1 ;
  long tmp___2 ;
  struct _ddebug descriptor ;
  phys_addr_t tmp___3 ;
  long tmp___4 ;
  struct falcon_board *tmp___5 ;
  struct lock_class_key __key ;
  int tmp___6 ;
  long tmp___7 ;
  {
  tmp = kzalloc(2288UL, 208U);
  nic_data = (struct falcon_nic_data *)tmp;
  if ((unsigned long )nic_data == (unsigned long )((struct falcon_nic_data *)0)) {
    return (-12);
  } else {
  }
  efx->nic_data = (void *)nic_data;
  rc = -19;
  tmp___0 = efx_nic_fpga_ver(efx);
  if (tmp___0 != 0U) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "Falcon FPGA not supported\n");
    } else {
    }
    goto fail1;
  } else {
  }
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 <= 1) {
    pci_rev = (efx->pci_dev)->revision;
    if ((unsigned int )pci_rev == 255U || (unsigned int )pci_rev == 0U) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "Falcon rev A0 not supported\n");
      } else {
      }
      goto fail1;
    } else {
    }
    efx_reado(efx, & nic_stat, 512U);
    if (((nic_stat.u64[0] >> 2) & 1ULL) == 0ULL) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "Falcon rev A1 1G not supported\n");
      } else {
      }
      goto fail1;
    } else {
    }
    if ((nic_stat.u64[0] & 1ULL) == 0ULL) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "Falcon rev A1 PCI-X not supported\n");
      } else {
      }
      goto fail1;
    } else {
    }
    dev = pci_dev_get(efx->pci_dev);
    goto ldv_43159;
    ldv_43158: ;
    if ((unsigned long )dev->bus == (unsigned long )(efx->pci_dev)->bus && dev->devfn == (efx->pci_dev)->devfn + 1U) {
      nic_data->pci_dev2 = dev;
      goto ldv_43157;
    } else {
    }
    ldv_43159:
    dev = pci_get_device(6436U, 26371U, dev);
    if ((unsigned long )dev != (unsigned long )((struct pci_dev *)0)) {
      goto ldv_43158;
    } else {
    }
    ldv_43157: ;
    if ((unsigned long )nic_data->pci_dev2 == (unsigned long )((struct pci_dev *)0)) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "failed to find secondary function\n");
      } else {
      }
      rc = -19;
      goto fail2;
    } else {
    }
  } else {
  }
  rc = __falcon_reset_hw(efx, 1);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to reset NIC\n");
    } else {
    }
    goto fail3;
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & efx->irq_status, 16U);
  if (rc != 0) {
    goto fail4;
  } else {
  }
  tmp___2 = ldv__builtin_expect((efx->irq_status.dma_addr & 15ULL) != 0ULL, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                         "i" (1572), "i" (12UL));
    ldv_43163: ;
    goto ldv_43163;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "falcon_probe_nic";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared";
    descriptor.format = "INT_KER at %llx (virt %p phys %llx)\n";
    descriptor.lineno = 1578U;
    descriptor.flags = 0U;
    tmp___4 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___4 != 0L) {
      tmp___3 = virt_to_phys((void volatile *)efx->irq_status.addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "INT_KER at %llx (virt %p phys %llx)\n", efx->irq_status.dma_addr,
                           efx->irq_status.addr, tmp___3);
    } else {
    }
  } else {
  }
  falcon_probe_spi_devices(efx);
  rc = falcon_probe_nvconfig(efx);
  if (rc != 0) {
    if (rc == -22) {
      if ((efx->msg_enable & 2U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "NVRAM is invalid\n");
      } else {
      }
    } else {
    }
    goto fail5;
  } else {
  }
  efx->timer_quantum_ns = 4968U;
  board = falcon_board(efx);
  board->i2c_adap.owner = & __this_module;
  board->i2c_data = falcon_i2c_bit_operations;
  board->i2c_data.data = (void *)efx;
  board->i2c_adap.algo_data = (void *)(& board->i2c_data);
  board->i2c_adap.dev.parent = & (efx->pci_dev)->dev;
  strlcpy((char *)(& board->i2c_adap.name), "SFC4000 GPIO", 48UL);
  rc = i2c_bit_add_bus(& board->i2c_adap);
  if (rc != 0) {
    goto fail5;
  } else {
  }
  tmp___5 = falcon_board(efx);
  rc = (*((tmp___5->type)->init))(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to initialise board\n");
    } else {
    }
    goto fail6;
  } else {
  }
  nic_data->stats_disable_count = 1U;
  init_timer_key(& nic_data->stats_timer, 0U, "((&nic_data->stats_timer))", & __key);
  nic_data->stats_timer.function = & falcon_stats_timer_func;
  nic_data->stats_timer.data = (unsigned long )efx;
  return (0);
  fail6:
  tmp___6 = i2c_del_adapter(& board->i2c_adap);
  tmp___7 = ldv__builtin_expect(tmp___6 != 0, 0L);
  if (tmp___7 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                         "i" (1619), "i" (12UL));
    ldv_43169: ;
    goto ldv_43169;
  } else {
  }
  memset((void *)(& board->i2c_adap), 0, 1648UL);
  fail5:
  efx_nic_free_buffer(efx, & efx->irq_status);
  fail4: ;
  fail3: ;
  if ((unsigned long )nic_data->pci_dev2 != (unsigned long )((struct pci_dev *)0)) {
    pci_dev_put(nic_data->pci_dev2);
    nic_data->pci_dev2 = 0;
  } else {
  }
  fail2: ;
  fail1:
  kfree((void const *)efx->nic_data);
  return (rc);
}
}
static void falcon_init_rx_cfg(struct efx_nic *efx )
{
  unsigned int huge_buf_size ;
  unsigned int ctrl_xon_thr ;
  unsigned int ctrl_xoff_thr ;
  efx_oword_t reg ;
  int tmp ;
  {
  huge_buf_size = 384U;
  ctrl_xon_thr = 20U;
  ctrl_xoff_thr = 25U;
  efx_reado(efx, & reg, 2048U);
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    reg.u64[0] = reg.u64[0] & 0xfffffff7ffffffffULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffffff007ffULL) | ((unsigned long long )huge_buf_size << 11);
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffffffff83fULL) | 128ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xffffffffffffffc1ULL) | 16ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xffffffffc1ffffffULL) | ((unsigned long long )ctrl_xon_thr << 25);
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffffe0fffffULL) | ((unsigned long long )ctrl_xoff_thr << 20);
    reg.u64[1] = reg.u64[1];
  } else {
    reg.u64[0] = reg.u64[0] & 0xfffff7ffffffffffULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffff007ffffULL) | ((unsigned long long )huge_buf_size << 19);
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffffff803ffULL) | 110592ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffffffffc01ULL) | 424ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xffffffc1ffffffffULL) | ((unsigned long long )ctrl_xon_thr << 33);
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = (reg.u64[0] & 0xfffffffe0fffffffULL) | ((unsigned long long )ctrl_xoff_thr << 28);
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = reg.u64[0] | 140737488355328ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = reg.u64[0] | 17592186044416ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = reg.u64[0] | 35184372088832ULL;
    reg.u64[1] = reg.u64[1];
    reg.u64[0] = reg.u64[0] | 70368744177664ULL;
    reg.u64[1] = reg.u64[1];
  }
  reg.u64[0] = reg.u64[0] | 1ULL;
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 2048U);
  return;
}
}
static int falcon_init_nic(struct efx_nic *efx )
{
  efx_oword_t temp ;
  int rc ;
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  size_t __len ;
  void *__ret ;
  int tmp___2 ;
  {
  efx_reado(efx, & temp, 512U);
  temp.u64[0] = temp.u64[0] | 65536ULL;
  temp.u64[1] = temp.u64[1];
  efx_writeo(efx, & temp, 512U);
  rc = falcon_reset_sram(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    efx_reado(efx, & temp, 784U);
    temp.u64[0] = temp.u64[0];
    temp.u64[1] = temp.u64[1] & 0xfffffffffffffcffULL;
    efx_writeo(efx, & temp, 784U);
  } else {
  }
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 <= 1) {
    efx_reado(efx, & temp, 2064U);
    temp.u64[0] = (temp.u64[0] & 0xffffff00ffffffffULL) | 34359738368ULL;
    temp.u64[1] = temp.u64[1];
    temp.u64[0] = (temp.u64[0] & 0xffffffffff00ffffULL) | 524288ULL;
    temp.u64[1] = temp.u64[1];
    temp.u64[0] = (temp.u64[0] & 0xffffffffffffff00ULL) | 8ULL;
    temp.u64[1] = temp.u64[1];
    temp.u64[0] = (temp.u64[0] & 0xffffffffffff00ffULL) | 2048ULL;
    temp.u64[1] = temp.u64[1];
    efx_writeo(efx, & temp, 2064U);
  } else {
  }
  efx_reado(efx, & temp, 2192U);
  temp.u64[0] = temp.u64[0] | 512ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 256ULL;
  temp.u64[1] = temp.u64[1];
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 <= 1) {
    temp.u64[0] = temp.u64[0] | 131072ULL;
    temp.u64[1] = temp.u64[1];
  } else {
  }
  efx_writeo(efx, & temp, 2192U);
  efx_reado(efx, & temp, 2640U);
  temp.u64[0] = temp.u64[0] & 0xffffffffffffffdfULL;
  temp.u64[1] = temp.u64[1];
  efx_writeo(efx, & temp, 2640U);
  falcon_init_rx_cfg(efx);
  tmp___2 = efx_nic_rev(efx);
  if (tmp___2 > 1) {
    __len = 16UL;
    if (__len > 63UL) {
      __ret = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key), __len);
    } else {
      __ret = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key),
                               __len);
    }
    efx_writeo(efx, & temp, 2144U);
    temp.u64[0] = 0ULL;
    temp.u64[1] = 0ULL;
    efx_writeo(efx, & temp, 592U);
  } else {
  }
  efx_nic_init_common(efx);
  return (0);
}
}
static void falcon_remove_nic(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  int rc ;
  long tmp___0 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  tmp = falcon_board(efx);
  board = tmp;
  (*((board->type)->fini))(efx);
  rc = i2c_del_adapter(& board->i2c_adap);
  tmp___0 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared"),
                         "i" (1762), "i" (12UL));
    ldv_43191: ;
    goto ldv_43191;
  } else {
  }
  memset((void *)(& board->i2c_adap), 0, 1648UL);
  efx_nic_free_buffer(efx, & efx->irq_status);
  __falcon_reset_hw(efx, 1);
  if ((unsigned long )nic_data->pci_dev2 != (unsigned long )((struct pci_dev *)0)) {
    pci_dev_put(nic_data->pci_dev2);
    nic_data->pci_dev2 = 0;
  } else {
  }
  kfree((void const *)efx->nic_data);
  efx->nic_data = 0;
  return;
}
}
static void falcon_update_nic_stats(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t cnt ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if (nic_data->stats_disable_count != 0U) {
    return;
  } else {
  }
  efx_reado(efx, & cnt, 2176U);
  efx->n_rx_nodesc_drop_cnt = efx->n_rx_nodesc_drop_cnt + ((unsigned int )cnt.u64[0] & 65535U);
  if ((int )nic_data->stats_pending && *(nic_data->stats_dma_done) == 4294967295U) {
    nic_data->stats_pending = 0;
    __asm__ volatile ("lfence": : : "memory");
    falcon_update_stats_xmac(efx);
  } else {
  }
  return;
}
}
void falcon_start_nic_stats(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  spin_lock_bh(& efx->stats_lock);
  nic_data->stats_disable_count = nic_data->stats_disable_count - 1U;
  if (nic_data->stats_disable_count == 0U) {
    falcon_stats_request(efx);
  } else {
  }
  spin_unlock_bh(& efx->stats_lock);
  return;
}
}
void falcon_stop_nic_stats(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  int i ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  __might_sleep("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon.c.prepared",
                1815, 0);
  spin_lock_bh(& efx->stats_lock);
  nic_data->stats_disable_count = nic_data->stats_disable_count + 1U;
  spin_unlock_bh(& efx->stats_lock);
  del_timer_sync(& nic_data->stats_timer);
  i = 0;
  goto ldv_43208;
  ldv_43207: ;
  if (*(nic_data->stats_dma_done) == 4294967295U) {
    goto ldv_43206;
  } else {
  }
  msleep(1U);
  i = i + 1;
  ldv_43208: ;
  if (i <= 3 && (int )nic_data->stats_pending) {
    goto ldv_43207;
  } else {
  }
  ldv_43206:
  spin_lock_bh(& efx->stats_lock);
  falcon_stats_complete(efx);
  spin_unlock_bh(& efx->stats_lock);
  return;
}
}
static void falcon_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  (*((tmp->type)->set_id_led))(efx, mode);
  return;
}
}
static void falcon_get_wol(struct efx_nic *efx , struct ethtool_wolinfo *wol )
{
  {
  wol->supported = 0U;
  wol->wolopts = 0U;
  memset((void *)(& wol->sopass), 0, 6UL);
  return;
}
}
static int falcon_set_wol(struct efx_nic *efx , u32 type )
{
  {
  if (type != 0U) {
    return (-22);
  } else {
  }
  return (0);
}
}
struct efx_nic_type const falcon_a1_nic_type =
     {& falcon_probe_nic, & falcon_remove_nic, & falcon_init_nic, & falcon_dimension_resources,
    & efx_port_dummy_op_void, & falcon_monitor, & falcon_map_reset_reason, & falcon_map_reset_flags,
    & falcon_reset_hw, & falcon_probe_port, & falcon_remove_port, & falcon_handle_global_event,
    & falcon_prepare_flush, & efx_port_dummy_op_void, & falcon_update_nic_stats, & falcon_start_nic_stats,
    & falcon_stop_nic_stats, & falcon_set_id_led, & falcon_push_irq_moderation, & falcon_reconfigure_port,
    & falcon_reconfigure_xmac, & falcon_xmac_check_fault, & falcon_get_wol, & falcon_set_wol,
    & efx_port_dummy_op_void, 0, & falcon_test_nvram, 1, 131072U, 71936U, 71680U,
    98304U, 72192U, 72448U, 70368744177663ULL, 0U, 36U, 1U, 4U, 4096U, 2ULL};
struct efx_nic_type const falcon_b0_nic_type =
     {& falcon_probe_nic, & falcon_remove_nic, & falcon_init_nic, & falcon_dimension_resources,
    & efx_port_dummy_op_void, & falcon_monitor, & falcon_map_reset_reason, & falcon_map_reset_flags,
    & falcon_reset_hw, & falcon_probe_port, & falcon_remove_port, & falcon_handle_global_event,
    & falcon_prepare_flush, & efx_port_dummy_op_void, & falcon_update_nic_stats, & falcon_start_nic_stats,
    & falcon_stop_nic_stats, & falcon_set_id_led, & falcon_push_irq_moderation, & falcon_reconfigure_port,
    & falcon_reconfigure_xmac, & falcon_xmac_check_fault, & falcon_get_wol, & falcon_set_wol,
    & efx_port_dummy_op_void, & falcon_b0_test_chip, & falcon_test_nvram, 2, 16451584U,
    16056320U, 15990784U, 8388608U, 16121856U, 16384000U, 70368744177663ULL, 16U,
    0U, 0U, 32U, 4096U, 402653186ULL};
void ldv_main2_sequence_infinite_withcheck_stateful(void)
{
  void *var_falcon_setsda_0_p0 ;
  int var_falcon_setsda_0_p1 ;
  void *var_falcon_setscl_1_p0 ;
  int var_falcon_setscl_1_p1 ;
  void *var_falcon_getsda_2_p0 ;
  void *var_falcon_getscl_3_p0 ;
  struct efx_nic *var_group1 ;
  int res_falcon_probe_nic_45 ;
  enum reset_type var_falcon_map_reset_reason_35_p0 ;
  u32 *var_falcon_map_reset_flags_36_p0 ;
  enum reset_type var_falcon_reset_hw_38_p1 ;
  struct efx_channel *var_group2 ;
  efx_qword_t *var_falcon_handle_global_event_31_p1 ;
  enum efx_led_mode var_falcon_set_id_led_52_p1 ;
  struct ethtool_wolinfo *var_group3 ;
  u32 var_falcon_set_wol_54_p1 ;
  struct efx_self_tests *var_group4 ;
  int var_falcon_legacy_interrupt_a1_7_p0 ;
  void *var_falcon_legacy_interrupt_a1_7_p1 ;
  int ldv_s_falcon_a1_nic_type_efx_nic_type ;
  int ldv_s_falcon_b0_nic_type_efx_nic_type ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_falcon_a1_nic_type_efx_nic_type = 0;
  ldv_s_falcon_b0_nic_type_efx_nic_type = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_43312;
  ldv_43311:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0:
  ldv_handler_precall();
  falcon_setsda(var_falcon_setsda_0_p0, var_falcon_setsda_0_p1);
  goto ldv_43261;
  case 1:
  ldv_handler_precall();
  falcon_setscl(var_falcon_setscl_1_p0, var_falcon_setscl_1_p1);
  goto ldv_43261;
  case 2:
  ldv_handler_precall();
  falcon_getsda(var_falcon_getsda_2_p0);
  goto ldv_43261;
  case 3:
  ldv_handler_precall();
  falcon_getscl(var_falcon_getscl_3_p0);
  goto ldv_43261;
  case 4: ;
  if (ldv_s_falcon_a1_nic_type_efx_nic_type == 0) {
    res_falcon_probe_nic_45 = falcon_probe_nic(var_group1);
    ldv_check_return_value(res_falcon_probe_nic_45);
    ldv_check_return_value_probe(res_falcon_probe_nic_45);
    if (res_falcon_probe_nic_45 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_falcon_a1_nic_type_efx_nic_type = ldv_s_falcon_a1_nic_type_efx_nic_type + 1;
  } else {
  }
  goto ldv_43261;
  case 5: ;
  if (ldv_s_falcon_a1_nic_type_efx_nic_type == 1) {
    ldv_handler_precall();
    falcon_remove_nic(var_group1);
    ldv_s_falcon_a1_nic_type_efx_nic_type = 0;
  } else {
  }
  goto ldv_43261;
  case 6:
  ldv_handler_precall();
  falcon_init_nic(var_group1);
  goto ldv_43261;
  case 7:
  ldv_handler_precall();
  falcon_dimension_resources(var_group1);
  goto ldv_43261;
  case 8:
  ldv_handler_precall();
  falcon_monitor(var_group1);
  goto ldv_43261;
  case 9:
  ldv_handler_precall();
  falcon_map_reset_reason(var_falcon_map_reset_reason_35_p0);
  goto ldv_43261;
  case 10:
  ldv_handler_precall();
  falcon_map_reset_flags(var_falcon_map_reset_flags_36_p0);
  goto ldv_43261;
  case 11:
  ldv_handler_precall();
  falcon_reset_hw(var_group1, var_falcon_reset_hw_38_p1);
  goto ldv_43261;
  case 12:
  ldv_handler_precall();
  falcon_probe_port(var_group1);
  goto ldv_43261;
  case 13:
  ldv_handler_precall();
  falcon_remove_port(var_group1);
  goto ldv_43261;
  case 14:
  ldv_handler_precall();
  falcon_handle_global_event(var_group2, var_falcon_handle_global_event_31_p1);
  goto ldv_43261;
  case 15:
  ldv_handler_precall();
  falcon_prepare_flush(var_group1);
  goto ldv_43261;
  case 16:
  ldv_handler_precall();
  falcon_update_nic_stats(var_group1);
  goto ldv_43261;
  case 17:
  ldv_handler_precall();
  falcon_start_nic_stats(var_group1);
  goto ldv_43261;
  case 18:
  ldv_handler_precall();
  falcon_stop_nic_stats(var_group1);
  goto ldv_43261;
  case 19:
  ldv_handler_precall();
  falcon_set_id_led(var_group1, var_falcon_set_id_led_52_p1);
  goto ldv_43261;
  case 20:
  ldv_handler_precall();
  falcon_push_irq_moderation(var_group2);
  goto ldv_43261;
  case 21:
  ldv_handler_precall();
  falcon_reconfigure_port(var_group1);
  goto ldv_43261;
  case 22:
  ldv_handler_precall();
  falcon_get_wol(var_group1, var_group3);
  goto ldv_43261;
  case 23:
  ldv_handler_precall();
  falcon_set_wol(var_group1, var_falcon_set_wol_54_p1);
  goto ldv_43261;
  case 24:
  ldv_handler_precall();
  falcon_test_nvram(var_group1);
  goto ldv_43261;
  case 25: ;
  if (ldv_s_falcon_b0_nic_type_efx_nic_type == 0) {
    res_falcon_probe_nic_45 = falcon_probe_nic(var_group1);
    ldv_check_return_value(res_falcon_probe_nic_45);
    ldv_check_return_value_probe(res_falcon_probe_nic_45);
    if (res_falcon_probe_nic_45 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_falcon_b0_nic_type_efx_nic_type = ldv_s_falcon_b0_nic_type_efx_nic_type + 1;
  } else {
  }
  goto ldv_43261;
  case 26: ;
  if (ldv_s_falcon_b0_nic_type_efx_nic_type == 1) {
    ldv_handler_precall();
    falcon_remove_nic(var_group1);
    ldv_s_falcon_b0_nic_type_efx_nic_type = 0;
  } else {
  }
  goto ldv_43261;
  case 27:
  ldv_handler_precall();
  falcon_init_nic(var_group1);
  goto ldv_43261;
  case 28:
  ldv_handler_precall();
  falcon_dimension_resources(var_group1);
  goto ldv_43261;
  case 29:
  ldv_handler_precall();
  falcon_monitor(var_group1);
  goto ldv_43261;
  case 30:
  ldv_handler_precall();
  falcon_map_reset_reason(var_falcon_map_reset_reason_35_p0);
  goto ldv_43261;
  case 31:
  ldv_handler_precall();
  falcon_map_reset_flags(var_falcon_map_reset_flags_36_p0);
  goto ldv_43261;
  case 32:
  ldv_handler_precall();
  falcon_reset_hw(var_group1, var_falcon_reset_hw_38_p1);
  goto ldv_43261;
  case 33:
  ldv_handler_precall();
  falcon_probe_port(var_group1);
  goto ldv_43261;
  case 34:
  ldv_handler_precall();
  falcon_remove_port(var_group1);
  goto ldv_43261;
  case 35:
  ldv_handler_precall();
  falcon_handle_global_event(var_group2, var_falcon_handle_global_event_31_p1);
  goto ldv_43261;
  case 36:
  ldv_handler_precall();
  falcon_prepare_flush(var_group1);
  goto ldv_43261;
  case 37:
  ldv_handler_precall();
  falcon_update_nic_stats(var_group1);
  goto ldv_43261;
  case 38:
  ldv_handler_precall();
  falcon_start_nic_stats(var_group1);
  goto ldv_43261;
  case 39:
  ldv_handler_precall();
  falcon_stop_nic_stats(var_group1);
  goto ldv_43261;
  case 40:
  ldv_handler_precall();
  falcon_set_id_led(var_group1, var_falcon_set_id_led_52_p1);
  goto ldv_43261;
  case 41:
  ldv_handler_precall();
  falcon_push_irq_moderation(var_group2);
  goto ldv_43261;
  case 42:
  ldv_handler_precall();
  falcon_reconfigure_port(var_group1);
  goto ldv_43261;
  case 43:
  ldv_handler_precall();
  falcon_get_wol(var_group1, var_group3);
  goto ldv_43261;
  case 44:
  ldv_handler_precall();
  falcon_set_wol(var_group1, var_falcon_set_wol_54_p1);
  goto ldv_43261;
  case 45:
  ldv_handler_precall();
  falcon_b0_test_chip(var_group1, var_group4);
  goto ldv_43261;
  case 46:
  ldv_handler_precall();
  falcon_test_nvram(var_group1);
  goto ldv_43261;
  case 47:
  LDV_IN_INTERRUPT = 2;
  ldv_handler_precall();
  falcon_legacy_interrupt_a1(var_falcon_legacy_interrupt_a1_7_p0, var_falcon_legacy_interrupt_a1_7_p1);
  LDV_IN_INTERRUPT = 1;
  goto ldv_43261;
  default: ;
  goto ldv_43261;
  }
  ldv_43261: ;
  ldv_43312:
  tmp___0 = __VERIFIER_nondet_int();
  if ((tmp___0 != 0 || ldv_s_falcon_a1_nic_type_efx_nic_type != 0) || ldv_s_falcon_b0_nic_type_efx_nic_type != 0) {
    goto ldv_43311;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_79(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_80(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_81(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_82(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_83(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_84(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_85(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int ldv_mutex_is_locked_87(struct mutex *lock )
{
  ldv_func_ret_type___7 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_is_locked(lock);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_is_locked_mac_lock(lock);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_lock_88(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mdio_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_89(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mdio_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_90(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mdio_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_91(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mdio_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_92(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_spi_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_93(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_94(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_95(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_96(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_spi_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_97(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_120(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_118(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_121(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_123(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_117(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_119(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_122(struct mutex *ldv_func_arg1 ) ;
extern int pci_wake_from_d3(struct pci_dev * , bool ) ;
void efx_mcdi_init(struct efx_nic *efx ) ;
int efx_mcdi_drv_attach(struct efx_nic *efx , bool driver_operating , bool *was_attached ) ;
int efx_mcdi_get_board_cfg(struct efx_nic *efx , u8 *mac_address , u16 *fw_subtype_list ,
                           u32 *capabilities ) ;
int efx_mcdi_log_ctrl(struct efx_nic *efx , bool evq , bool uart , u32 dest_evq ) ;
int efx_mcdi_nvram_test_all(struct efx_nic *efx ) ;
int efx_mcdi_handle_assertion(struct efx_nic *efx ) ;
void efx_mcdi_set_id_led(struct efx_nic *efx , enum efx_led_mode mode ) ;
int efx_mcdi_reset_port(struct efx_nic *efx ) ;
int efx_mcdi_reset_mc(struct efx_nic *efx ) ;
int efx_mcdi_wol_filter_set_magic(struct efx_nic *efx , u8 const *mac , int *id_out ) ;
int efx_mcdi_wol_filter_get_magic(struct efx_nic *efx , int *id_out ) ;
int efx_mcdi_wol_filter_remove(struct efx_nic *efx , int id ) ;
int efx_mcdi_wol_filter_reset(struct efx_nic *efx ) ;
int efx_mcdi_set_mac(struct efx_nic *efx ) ;
int efx_mcdi_mac_stats(struct efx_nic *efx , dma_addr_t dma_addr , u32 dma_len , int enable ,
                       int clear ) ;
int efx_mcdi_mac_reconfigure(struct efx_nic *efx ) ;
bool efx_mcdi_mac_check_fault(struct efx_nic *efx ) ;
int efx_mcdi_mon_probe(struct efx_nic *efx ) ;
void efx_mcdi_mon_remove(struct efx_nic *efx ) ;
void efx_sriov_probe(struct efx_nic *efx ) ;
void efx_ptp_probe(struct efx_nic *efx ) ;
__inline static void efx_update_diff_stat(u64 *stat , u64 diff )
{
  {
  if ((long long )(diff - *stat) > 0LL) {
    *stat = diff;
  } else {
  }
  return;
}
}
void siena_prepare_flush(struct efx_nic *efx ) ;
void siena_finish_flush(struct efx_nic *efx ) ;
struct efx_phy_operations const efx_mcdi_phy_ops ;
int efx_mcdi_mdio_read(struct efx_nic *efx , unsigned int bus , unsigned int prtad ,
                       unsigned int devad , u16 addr , u16 *value_out , u32 *status_out ) ;
int efx_mcdi_mdio_write(struct efx_nic *efx , unsigned int bus , unsigned int prtad ,
                        unsigned int devad , u16 addr , u16 value , u32 *status_out ) ;
int efx_mcdi_phy_reconfigure(struct efx_nic *efx ) ;
static void siena_init_wol(struct efx_nic *efx ) ;
static int siena_reset_hw(struct efx_nic *efx , enum reset_type method ) ;
static void siena_push_irq_moderation(struct efx_channel *channel )
{
  efx_dword_t timer_cmd ;
  {
  if (channel->irq_moderation != 0U) {
    timer_cmd.u32[0] = (channel->irq_moderation - 1U) | 49152U;
  } else {
    timer_cmd.u32[0] = 0U;
  }
  _efx_writed_page_locked(channel->efx, & timer_cmd, 1056U, (unsigned int )channel->channel);
  return;
}
}
static int siena_mdio_write(struct net_device *net_dev , int prtad , int devad , u16 addr ,
                            u16 value )
{
  struct efx_nic *efx ;
  void *tmp ;
  uint32_t status ;
  int rc ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = efx_mcdi_mdio_write(efx, efx->mdio_bus, (unsigned int )prtad, (unsigned int )devad,
                           (int )addr, (int )value, & status);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (status != 8U) {
    return (-5);
  } else {
  }
  return (0);
}
}
static int siena_mdio_read(struct net_device *net_dev , int prtad , int devad , u16 addr )
{
  struct efx_nic *efx ;
  void *tmp ;
  uint16_t value ;
  uint32_t status ;
  int rc ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = efx_mcdi_mdio_read(efx, efx->mdio_bus, (unsigned int )prtad, (unsigned int )devad,
                          (int )addr, & value, & status);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (status != 8U) {
    return (-5);
  } else {
  }
  return ((int )value);
}
}
static int siena_probe_port(struct efx_nic *efx )
{
  int rc ;
  struct _ddebug descriptor ;
  phys_addr_t tmp ;
  long tmp___0 ;
  {
  efx->phy_op = & efx_mcdi_phy_ops;
  efx->mdio.mode_support = 6U;
  efx->mdio.mdio_read = & siena_mdio_read;
  efx->mdio.mdio_write = & siena_mdio_write;
  rc = (*((efx->phy_op)->probe))(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & efx->stats_buffer, 776U);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "siena_probe_port";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena.c.prepared";
    descriptor.format = "stats buffer at %llx (virt %p phys %llx)\n";
    descriptor.lineno = 205U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      tmp = virt_to_phys((void volatile *)efx->stats_buffer.addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "stats buffer at %llx (virt %p phys %llx)\n", efx->stats_buffer.dma_addr,
                           efx->stats_buffer.addr, tmp);
    } else {
    }
  } else {
  }
  efx_mcdi_mac_stats(efx, efx->stats_buffer.dma_addr, 0U, 0, 1);
  return (0);
}
}
static void siena_remove_port(struct efx_nic *efx )
{
  {
  (*((efx->phy_op)->remove))(efx);
  efx_nic_free_buffer(efx, & efx->stats_buffer);
  return;
}
}
void siena_prepare_flush(struct efx_nic *efx )
{
  unsigned int tmp ;
  {
  tmp = efx->fc_disable;
  efx->fc_disable = efx->fc_disable + 1U;
  if (tmp == 0U) {
    efx_mcdi_set_mac(efx);
  } else {
  }
  return;
}
}
void siena_finish_flush(struct efx_nic *efx )
{
  {
  efx->fc_disable = efx->fc_disable - 1U;
  if (efx->fc_disable == 0U) {
    efx_mcdi_set_mac(efx);
  } else {
  }
  return;
}
}
static struct efx_nic_register_test const siena_register_tests[13U] =
  { {0U, {.u32 = {262143U, 262143U, 262143U, 262143U}}},
        {256U, {.u32 = {66559U, 0U, 0U, 0U}}},
        {2048U, {.u32 = {4294967294U, 4294967295U, 262143U, 0U}}},
        {2640U, {.u32 = {2147418167U, 4294934528U, 4294967295U, 67108863U}}},
        {2688U, {.u32 = {4294901376U, 536870911U, 33554686U, 8388607U}}},
        {1568U, {.u32 = {2097151U, 0U, 0U, 0U}}},
        {2112U, {.u32 = {3U, 0U, 0U, 0U}}},
        {2128U, {.u32 = {1023U, 0U, 0U, 0U}}},
        {592U, {.u32 = {4095U, 0U, 0U, 0U}}},
        {2144U, {.u32 = {4294967295U, 4294967295U, 4294967295U, 4294967295U}}},
        {2256U, {.u32 = {4294967295U, 4294967295U, 4294967295U, 4294967295U}}},
        {2272U, {.u32 = {4294967295U, 4294967295U, 4294967295U, 4294967295U}}},
        {2288U, {.u32 = {4294967295U, 4294967295U, 7U, 0U}}}};
static int siena_test_chip(struct efx_nic *efx , struct efx_self_tests *tests )
{
  enum reset_type reset_method ;
  int rc ;
  int rc2 ;
  int tmp ;
  {
  reset_method = 1;
  efx_reset_down(efx, reset_method);
  rc = siena_reset_hw(efx, reset_method);
  if (rc != 0) {
    goto out;
  } else {
  }
  tmp = efx_nic_test_registers(efx, (struct efx_nic_register_test const *)(& siena_register_tests),
                               13UL);
  tests->registers = tmp != 0 ? -1 : 1;
  rc = siena_reset_hw(efx, reset_method);
  out:
  rc2 = efx_reset_up(efx, reset_method, rc == 0);
  return (rc != 0 ? rc : rc2);
}
}
static enum reset_type siena_map_reset_reason(enum reset_type reason )
{
  {
  return (1);
}
}
static int siena_map_reset_flags(u32 *flags )
{
  {
  if ((*flags & 65660U) == 65660U) {
    *flags = *flags & 4294901635U;
    return (2);
  } else {
  }
  if ((*flags & 124U) == 124U) {
    *flags = *flags & 4294967171U;
    return (1);
  } else {
  }
  return (-22);
}
}
static int siena_reset_hw(struct efx_nic *efx , enum reset_type method )
{
  int rc ;
  int tmp ;
  int tmp___0 ;
  {
  rc = efx_mcdi_handle_assertion(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((unsigned int )method == 2U) {
    tmp = efx_mcdi_reset_mc(efx);
    return (tmp);
  } else {
    tmp___0 = efx_mcdi_reset_port(efx);
    return (tmp___0);
  }
}
}
static int siena_probe_nvconfig(struct efx_nic *efx )
{
  u32 caps ;
  int rc ;
  {
  caps = 0U;
  rc = efx_mcdi_get_board_cfg(efx, (u8 *)(& (efx->net_dev)->perm_addr), 0, & caps);
  efx->timer_quantum_ns = (caps & 4U) != 0U ? 3072U : 6144U;
  return (rc);
}
}
static void siena_dimension_resources(struct efx_nic *efx )
{
  {
  efx_nic_dimension_resources(efx, 73728U);
  return;
}
}
static int siena_probe_nic(struct efx_nic *efx )
{
  struct siena_nic_data *nic_data ;
  bool already_attached ;
  efx_oword_t reg ;
  int rc ;
  void *tmp ;
  u32 tmp___0 ;
  long tmp___1 ;
  struct _ddebug descriptor ;
  phys_addr_t tmp___2 ;
  long tmp___3 ;
  {
  already_attached = 0;
  tmp = kzalloc(424UL, 208U);
  nic_data = (struct siena_nic_data *)tmp;
  if ((unsigned long )nic_data == (unsigned long )((struct siena_nic_data *)0)) {
    return (-12);
  } else {
  }
  efx->nic_data = (void *)nic_data;
  tmp___0 = efx_nic_fpga_ver(efx);
  if (tmp___0 != 0U) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "Siena FPGA not supported\n");
    } else {
    }
    rc = -19;
    goto fail1;
  } else {
  }
  efx_reado(efx, & reg, 624U);
  (efx->net_dev)->dev_id = ((unsigned int )((unsigned short )(reg.u64[0] >> 40)) & 3U) - 1U;
  efx_mcdi_init(efx);
  rc = efx_mcdi_handle_assertion(efx);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  rc = efx_mcdi_drv_attach(efx, 1, & already_attached);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "Unable to register driver with MCPU\n");
    } else {
    }
    goto fail2;
  } else {
  }
  if ((int )already_attached) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "Host already registered with MCPU\n");
    } else {
    }
  } else {
  }
  rc = siena_reset_hw(efx, 1);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to reset NIC\n");
    } else {
    }
    goto fail3;
  } else {
  }
  siena_init_wol(efx);
  rc = efx_nic_alloc_buffer(efx, & efx->irq_status, 16U);
  if (rc != 0) {
    goto fail4;
  } else {
  }
  tmp___1 = ldv__builtin_expect((efx->irq_status.dma_addr & 15ULL) != 0ULL, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena.c.prepared"),
                         "i" (414), "i" (12UL));
    ldv_42651: ;
    goto ldv_42651;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "siena_probe_nic";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena.c.prepared";
    descriptor.format = "INT_KER at %llx (virt %p phys %llx)\n";
    descriptor.lineno = 420U;
    descriptor.flags = 0U;
    tmp___3 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___3 != 0L) {
      tmp___2 = virt_to_phys((void volatile *)efx->irq_status.addr);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "INT_KER at %llx (virt %p phys %llx)\n", efx->irq_status.dma_addr,
                           efx->irq_status.addr, tmp___2);
    } else {
    }
  } else {
  }
  rc = siena_probe_nvconfig(efx);
  if (rc == -22) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "NVRAM is invalid therefore using defaults\n");
    } else {
    }
    efx->phy_type = 0U;
    efx->mdio.prtad = -1;
  } else
  if (rc != 0) {
    goto fail5;
  } else {
  }
  rc = efx_mcdi_mon_probe(efx);
  if (rc != 0) {
    goto fail5;
  } else {
  }
  efx_sriov_probe(efx);
  efx_ptp_probe(efx);
  return (0);
  fail5:
  efx_nic_free_buffer(efx, & efx->irq_status);
  fail4: ;
  fail3:
  efx_mcdi_drv_attach(efx, 0, 0);
  fail2: ;
  fail1:
  kfree((void const *)efx->nic_data);
  return (rc);
}
}
static int siena_init_nic(struct efx_nic *efx )
{
  efx_oword_t temp ;
  int rc ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  size_t __len___1 ;
  void *__ret___1 ;
  size_t __len___2 ;
  void *__ret___2 ;
  {
  rc = efx_mcdi_handle_assertion(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  efx_reado(efx, & temp, 2688U);
  temp.u64[0] = temp.u64[0] | 128ULL;
  temp.u64[1] = temp.u64[1];
  efx_writeo(efx, & temp, 2688U);
  efx_reado(efx, & temp, 2640U);
  temp.u64[0] = temp.u64[0] & 0xffffffffffffffdfULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 140737488355328ULL;
  temp.u64[1] = temp.u64[1];
  efx_writeo(efx, & temp, 2640U);
  efx_reado(efx, & temp, 2048U);
  temp.u64[0] = temp.u64[0] & 0xfffff7ffffffffffULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 140737488355328ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 17592186044416ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 35184372088832ULL;
  temp.u64[1] = temp.u64[1];
  temp.u64[0] = temp.u64[0] | 70368744177664ULL;
  temp.u64[1] = temp.u64[1];
  efx_writeo(efx, & temp, 2048U);
  __len = 16UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key), __len);
  } else {
    __ret = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key),
                             __len);
  }
  efx_writeo(efx, & temp, 2144U);
  __len___0 = 16UL;
  if (__len___0 > 63UL) {
    __ret___0 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key), __len___0);
  } else {
    __ret___0 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key),
                                 __len___0);
  }
  efx_writeo(efx, & temp, 2256U);
  __len___1 = 16UL;
  if (__len___1 > 63UL) {
    __ret___1 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key) + 16U,
                         __len___1);
  } else {
    __ret___1 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key) + 16U,
                                 __len___1);
  }
  efx_writeo(efx, & temp, 2272U);
  temp.u64[0] = 0ULL;
  temp.u64[1] = 6ULL;
  __len___2 = 8UL;
  if (__len___2 > 63UL) {
    __ret___2 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key) + 32U,
                         __len___2);
  } else {
    __ret___2 = memcpy((void *)(& temp), (void const *)(& efx->rx_hash_key) + 32U,
                                 __len___2);
  }
  efx_writeo(efx, & temp, 2288U);
  rc = efx_mcdi_log_ctrl(efx, 1, 0, 0U);
  if (rc != 0) {
    return (rc);
  } else {
  }
  temp.u64[0] = 0ULL;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 592U);
  temp.u64[0] = 65536ULL;
  temp.u64[1] = 0ULL;
  efx_writeo(efx, & temp, 256U);
  efx_nic_init_common(efx);
  return (0);
}
}
static void siena_remove_nic(struct efx_nic *efx )
{
  {
  efx_mcdi_mon_remove(efx);
  efx_nic_free_buffer(efx, & efx->irq_status);
  siena_reset_hw(efx, 1);
  efx_mcdi_drv_attach(efx, 0, 0);
  kfree((void const *)efx->nic_data);
  efx->nic_data = 0;
  return;
}
}
static int siena_try_update_nic_stats(struct efx_nic *efx )
{
  __le64 *dma_stats ;
  struct efx_mac_stats *mac_stats ;
  __le64 generation_start ;
  __le64 generation_end ;
  {
  mac_stats = & efx->mac_stats;
  dma_stats = (__le64 *)efx->stats_buffer.addr;
  generation_end = *(dma_stats + 96UL);
  if (generation_end == 0xffffffffffffffffULL) {
    return (0);
  } else {
  }
  __asm__ volatile ("lfence": : : "memory");
  mac_stats->tx_bytes = *(dma_stats + 7UL);
  mac_stats->tx_bad_bytes = *(dma_stats + 8UL);
  efx_update_diff_stat(& mac_stats->tx_good_bytes, mac_stats->tx_bytes - mac_stats->tx_bad_bytes);
  mac_stats->tx_packets = *(dma_stats + 1UL);
  mac_stats->tx_bad = *(dma_stats + 18UL);
  mac_stats->tx_pause = *(dma_stats + 2UL);
  mac_stats->tx_control = *(dma_stats + 3UL);
  mac_stats->tx_unicast = *(dma_stats + 4UL);
  mac_stats->tx_multicast = *(dma_stats + 5UL);
  mac_stats->tx_broadcast = *(dma_stats + 6UL);
  mac_stats->tx_lt64 = *(dma_stats + 9UL);
  mac_stats->tx_64 = *(dma_stats + 10UL);
  mac_stats->tx_65_to_127 = *(dma_stats + 11UL);
  mac_stats->tx_128_to_255 = *(dma_stats + 12UL);
  mac_stats->tx_256_to_511 = *(dma_stats + 13UL);
  mac_stats->tx_512_to_1023 = *(dma_stats + 14UL);
  mac_stats->tx_1024_to_15xx = *(dma_stats + 15UL);
  mac_stats->tx_15xx_to_jumbo = *(dma_stats + 16UL);
  mac_stats->tx_gtjumbo = *(dma_stats + 17UL);
  mac_stats->tx_collision = 0ULL;
  mac_stats->tx_single_collision = *(dma_stats + 19UL);
  mac_stats->tx_multiple_collision = *(dma_stats + 20UL);
  mac_stats->tx_excessive_collision = *(dma_stats + 21UL);
  mac_stats->tx_deferred = *(dma_stats + 23UL);
  mac_stats->tx_late_collision = *(dma_stats + 22UL);
  mac_stats->tx_collision = ((mac_stats->tx_single_collision + mac_stats->tx_multiple_collision) + mac_stats->tx_excessive_collision) + mac_stats->tx_late_collision;
  mac_stats->tx_excessive_deferred = *(dma_stats + 24UL);
  mac_stats->tx_non_tcpudp = *(dma_stats + 25UL);
  mac_stats->tx_mac_src_error = *(dma_stats + 26UL);
  mac_stats->tx_ip_src_error = *(dma_stats + 27UL);
  mac_stats->rx_bytes = *(dma_stats + 35UL);
  mac_stats->rx_bad_bytes = *(dma_stats + 36UL);
  efx_update_diff_stat(& mac_stats->rx_good_bytes, mac_stats->rx_bytes - mac_stats->rx_bad_bytes);
  mac_stats->rx_packets = *(dma_stats + 28UL);
  mac_stats->rx_good = *(dma_stats + 30UL);
  mac_stats->rx_bad = *(dma_stats + 46UL);
  mac_stats->rx_pause = *(dma_stats + 29UL);
  mac_stats->rx_control = *(dma_stats + 31UL);
  mac_stats->rx_unicast = *(dma_stats + 32UL);
  mac_stats->rx_multicast = *(dma_stats + 33UL);
  mac_stats->rx_broadcast = *(dma_stats + 34UL);
  mac_stats->rx_lt64 = *(dma_stats + 45UL);
  mac_stats->rx_64 = *(dma_stats + 37UL);
  mac_stats->rx_65_to_127 = *(dma_stats + 38UL);
  mac_stats->rx_128_to_255 = *(dma_stats + 39UL);
  mac_stats->rx_256_to_511 = *(dma_stats + 40UL);
  mac_stats->rx_512_to_1023 = *(dma_stats + 41UL);
  mac_stats->rx_1024_to_15xx = *(dma_stats + 42UL);
  mac_stats->rx_15xx_to_jumbo = *(dma_stats + 43UL);
  mac_stats->rx_gtjumbo = *(dma_stats + 44UL);
  mac_stats->rx_bad_lt64 = 0ULL;
  mac_stats->rx_bad_64_to_15xx = 0ULL;
  mac_stats->rx_bad_15xx_to_jumbo = 0ULL;
  mac_stats->rx_bad_gtjumbo = *(dma_stats + 53UL);
  mac_stats->rx_overflow = *(dma_stats + 47UL);
  mac_stats->rx_missed = 0ULL;
  mac_stats->rx_false_carrier = *(dma_stats + 48UL);
  mac_stats->rx_symbol_error = *(dma_stats + 49UL);
  mac_stats->rx_align_error = *(dma_stats + 50UL);
  mac_stats->rx_length_error = *(dma_stats + 51UL);
  mac_stats->rx_internal_error = *(dma_stats + 52UL);
  mac_stats->rx_good_lt64 = 0ULL;
  efx->n_rx_nodesc_drop_cnt = (unsigned int )*(dma_stats + 54UL);
  __asm__ volatile ("lfence": : : "memory");
  generation_start = *dma_stats;
  if (generation_end != generation_start) {
    return (-11);
  } else {
  }
  return (0);
}
}
static void siena_update_nic_stats(struct efx_nic *efx )
{
  int retry ;
  int tmp ;
  {
  retry = 0;
  goto ldv_42687;
  ldv_42686:
  tmp = siena_try_update_nic_stats(efx);
  if (tmp == 0) {
    return;
  } else {
  }
  __const_udelay(429500UL);
  retry = retry + 1;
  ldv_42687: ;
  if (retry <= 99) {
    goto ldv_42686;
  } else {
  }
  return;
}
}
static void siena_start_nic_stats(struct efx_nic *efx )
{
  __le64 *dma_stats ;
  {
  dma_stats = (__le64 *)efx->stats_buffer.addr;
  *(dma_stats + 96UL) = 0xffffffffffffffffULL;
  efx_mcdi_mac_stats(efx, efx->stats_buffer.dma_addr, 776U, 1, 0);
  return;
}
}
static void siena_stop_nic_stats(struct efx_nic *efx )
{
  {
  efx_mcdi_mac_stats(efx, efx->stats_buffer.dma_addr, 0U, 0, 0);
  return;
}
}
static void siena_get_wol(struct efx_nic *efx , struct ethtool_wolinfo *wol )
{
  struct siena_nic_data *nic_data ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  wol->supported = 32U;
  if (nic_data->wol_filter_id != -1) {
    wol->wolopts = 32U;
  } else {
    wol->wolopts = 0U;
  }
  memset((void *)(& wol->sopass), 0, 6UL);
  return;
}
}
static int siena_set_wol(struct efx_nic *efx , u32 type )
{
  struct siena_nic_data *nic_data ;
  int rc ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  if ((type & 4294967263U) != 0U) {
    return (-22);
  } else {
  }
  if ((type & 32U) != 0U) {
    if (nic_data->wol_filter_id != -1) {
      efx_mcdi_wol_filter_remove(efx, nic_data->wol_filter_id);
    } else {
    }
    rc = efx_mcdi_wol_filter_set_magic(efx, (u8 const *)(efx->net_dev)->dev_addr,
                                       & nic_data->wol_filter_id);
    if (rc != 0) {
      goto fail;
    } else {
    }
    pci_wake_from_d3(efx->pci_dev, 1);
  } else {
    rc = efx_mcdi_wol_filter_reset(efx);
    nic_data->wol_filter_id = -1;
    pci_wake_from_d3(efx->pci_dev, 0);
    if (rc != 0) {
      goto fail;
    } else {
    }
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s failed: type=%d rc=%d\n",
               "siena_set_wol", type, rc);
  } else {
  }
  return (rc);
}
}
static void siena_init_wol(struct efx_nic *efx )
{
  struct siena_nic_data *nic_data ;
  int rc ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  rc = efx_mcdi_wol_filter_get_magic(efx, & nic_data->wol_filter_id);
  if (rc != 0) {
    efx_mcdi_wol_filter_reset(efx);
    nic_data->wol_filter_id = -1;
  } else
  if (nic_data->wol_filter_id != -1) {
    pci_wake_from_d3(efx->pci_dev, 1);
  } else {
  }
  return;
}
}
struct efx_nic_type const siena_a0_nic_type =
     {& siena_probe_nic, & siena_remove_nic, & siena_init_nic, & siena_dimension_resources,
    & efx_port_dummy_op_void, 0, & siena_map_reset_reason, & siena_map_reset_flags,
    & siena_reset_hw, & siena_probe_port, & siena_remove_port, 0, & siena_prepare_flush,
    & siena_finish_flush, & siena_update_nic_stats, & siena_start_nic_stats, & siena_stop_nic_stats,
    & efx_mcdi_set_id_led, & siena_push_irq_moderation, & efx_mcdi_phy_reconfigure,
    & efx_mcdi_mac_reconfigure, & efx_mcdi_mac_check_fault, & siena_get_wol, & siena_set_wol,
    & siena_init_wol, & siena_test_chip, & efx_mcdi_nvram_test_all, 3, 16713728U,
    16056320U, 15990784U, 8388608U, 16121856U, 16384000U, 70368744177663ULL, 16U,
    0U, 0U, 32U, 16384U, 402653202ULL};
int main(void)
{
  struct efx_nic *var_group1 ;
  int res_siena_probe_nic_13 ;
  enum reset_type var_siena_map_reset_reason_8_p0 ;
  u32 *var_siena_map_reset_flags_9_p0 ;
  enum reset_type var_siena_reset_hw_10_p1 ;
  struct efx_channel *var_group2 ;
  struct ethtool_wolinfo *var_group3 ;
  u32 var_siena_set_wol_21_p1 ;
  struct efx_self_tests *var_group4 ;
  int ldv_s_siena_a0_nic_type_efx_nic_type ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_siena_a0_nic_type_efx_nic_type = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_42764;
  ldv_42763:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_s_siena_a0_nic_type_efx_nic_type == 0) {
    res_siena_probe_nic_13 = siena_probe_nic(var_group1);
    ldv_check_return_value(res_siena_probe_nic_13);
    ldv_check_return_value_probe(res_siena_probe_nic_13);
    if (res_siena_probe_nic_13 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_siena_a0_nic_type_efx_nic_type = ldv_s_siena_a0_nic_type_efx_nic_type + 1;
  } else {
  }
  goto ldv_42743;
  case 1: ;
  if (ldv_s_siena_a0_nic_type_efx_nic_type == 1) {
    ldv_handler_precall();
    siena_remove_nic(var_group1);
    ldv_s_siena_a0_nic_type_efx_nic_type = 0;
  } else {
  }
  goto ldv_42743;
  case 2:
  ldv_handler_precall();
  siena_init_nic(var_group1);
  goto ldv_42743;
  case 3:
  ldv_handler_precall();
  siena_dimension_resources(var_group1);
  goto ldv_42743;
  case 4:
  ldv_handler_precall();
  siena_map_reset_reason(var_siena_map_reset_reason_8_p0);
  goto ldv_42743;
  case 5:
  ldv_handler_precall();
  siena_map_reset_flags(var_siena_map_reset_flags_9_p0);
  goto ldv_42743;
  case 6:
  ldv_handler_precall();
  siena_reset_hw(var_group1, var_siena_reset_hw_10_p1);
  goto ldv_42743;
  case 7:
  ldv_handler_precall();
  siena_probe_port(var_group1);
  goto ldv_42743;
  case 8:
  ldv_handler_precall();
  siena_remove_port(var_group1);
  goto ldv_42743;
  case 9:
  ldv_handler_precall();
  siena_prepare_flush(var_group1);
  goto ldv_42743;
  case 10:
  ldv_handler_precall();
  siena_finish_flush(var_group1);
  goto ldv_42743;
  case 11:
  ldv_handler_precall();
  siena_update_nic_stats(var_group1);
  goto ldv_42743;
  case 12:
  ldv_handler_precall();
  siena_start_nic_stats(var_group1);
  goto ldv_42743;
  case 13:
  ldv_handler_precall();
  siena_stop_nic_stats(var_group1);
  goto ldv_42743;
  case 14:
  ldv_handler_precall();
  siena_push_irq_moderation(var_group2);
  goto ldv_42743;
  case 15:
  ldv_handler_precall();
  siena_get_wol(var_group1, var_group3);
  goto ldv_42743;
  case 16:
  ldv_handler_precall();
  siena_set_wol(var_group1, var_siena_set_wol_21_p1);
  goto ldv_42743;
  case 17:
  ldv_handler_precall();
  siena_init_wol(var_group1);
  goto ldv_42743;
  case 18:
  ldv_handler_precall();
  siena_test_chip(var_group1, var_group4);
  goto ldv_42743;
  default: ;
  goto ldv_42743;
  }
  ldv_42743: ;
  ldv_42764:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0 || ldv_s_siena_a0_nic_type_efx_nic_type != 0) {
    goto ldv_42763;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return 0;
}
}
void ldv_mutex_lock_117(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_118(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_119(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_120(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_121(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_122(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_123(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static __u32 __arch_swab32(__u32 val )
{
  {
  __asm__ ("bswapl %0": "=r" (val): "0" (val));
  return (val);
}
}
__inline static __u16 __fswab16(__u16 val )
{
  {
  return ((__u16 )((int )((short )((int )val << 8)) | (int )((short )((int )val >> 8))));
}
}
__inline static __u32 __fswab32(__u32 val )
{
  __u32 tmp ;
  {
  tmp = __arch_swab32(val);
  return (tmp);
}
}
int ldv_mutex_trylock_134(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_132(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_135(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_137(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_131(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_133(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_136(struct mutex *ldv_func_arg1 ) ;
__inline static void *lowmem_page_address(struct page const *page )
{
  {
  return ((void *)((unsigned long )((unsigned long long )(((long )page + 24189255811072L) / 80L) << 12) + 0xffff880000000000UL));
}
}
__inline static void *kmalloc_array(size_t n , size_t size , gfp_t flags )
{
  void *tmp ;
  {
  if (size != 0UL && 0xffffffffffffffffUL / size < n) {
    return (0);
  } else {
  }
  tmp = __kmalloc(n * size, flags);
  return (tmp);
}
}
__inline static void *kcalloc(size_t n , size_t size , gfp_t flags )
{
  void *tmp ;
  {
  tmp = kmalloc_array(n, size, flags | 32768U);
  return (tmp);
}
}
__inline static int valid_dma_direction(int dma_direction )
{
  {
  return ((dma_direction == 0 || dma_direction == 1) || dma_direction == 2);
}
}
__inline static void kmemcheck_mark_initialized(void *address , unsigned int n )
{
  {
  return;
}
}
extern void debug_dma_map_page(struct device * , struct page * , size_t , size_t ,
                               int , dma_addr_t , bool ) ;
extern void debug_dma_mapping_error(struct device * , dma_addr_t ) ;
extern void debug_dma_unmap_page(struct device * , dma_addr_t , size_t , int ,
                                 bool ) ;
__inline static dma_addr_t dma_map_single_attrs(struct device *dev , void *ptr , size_t size ,
                                                enum dma_data_direction dir , struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  int tmp___0 ;
  long tmp___1 ;
  unsigned long tmp___2 ;
  unsigned long tmp___3 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  kmemcheck_mark_initialized(ptr, (unsigned int )size);
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (19), "i" (12UL));
    ldv_18531: ;
    goto ldv_18531;
  } else {
  }
  tmp___2 = __phys_addr((unsigned long )ptr);
  addr = (*(ops->map_page))(dev, 0xffffea0000000000UL + (tmp___2 >> 12), (unsigned long )ptr & 4095UL,
                            size, dir, attrs);
  tmp___3 = __phys_addr((unsigned long )ptr);
  debug_dma_map_page(dev, 0xffffea0000000000UL + (tmp___3 >> 12), (unsigned long )ptr & 4095UL,
                     size, (int )dir, addr, 1);
  return (addr);
}
}
__inline static void dma_unmap_single_attrs(struct device *dev , dma_addr_t addr ,
                                            size_t size , enum dma_data_direction dir ,
                                            struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (36), "i" (12UL));
    ldv_18540: ;
    goto ldv_18540;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, attrs);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 1);
  return;
}
}
__inline static dma_addr_t dma_map_page(struct device *dev , struct page *page , size_t offset ,
                                        size_t size , enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  void *tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = lowmem_page_address((struct page const *)page);
  kmemcheck_mark_initialized(tmp___0 + offset, (unsigned int )size);
  tmp___1 = valid_dma_direction((int )dir);
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (79), "i" (12UL));
    ldv_18574: ;
    goto ldv_18574;
  } else {
  }
  addr = (*(ops->map_page))(dev, page, offset, size, dir, 0);
  debug_dma_map_page(dev, page, offset, size, (int )dir, addr, 0);
  return (addr);
}
}
__inline static void dma_unmap_page(struct device *dev , dma_addr_t addr , size_t size ,
                                    enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (91), "i" (12UL));
    ldv_18582: ;
    goto ldv_18582;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, 0);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 0);
  return;
}
}
__inline static int dma_mapping_error(struct device *dev , dma_addr_t dma_addr )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  debug_dma_mapping_error(dev, dma_addr);
  if ((unsigned long )ops->mapping_error != (unsigned long )((int (*)(struct device * ,
                                                                      dma_addr_t ))0)) {
    tmp___0 = (*(ops->mapping_error))(dev, dma_addr);
    return (tmp___0);
  } else {
  }
  return (dma_addr == 0ULL);
}
}
__inline static unsigned int skb_frag_size(skb_frag_t const *frag )
{
  {
  return ((unsigned int )frag->size);
}
}
extern int skb_pad(struct sk_buff * , int ) ;
__inline static unsigned char *skb_end_pointer(struct sk_buff const *skb )
{
  {
  return ((unsigned char *)skb->head + (unsigned long )skb->end);
}
}
__inline static unsigned int skb_headlen(struct sk_buff const *skb )
{
  {
  return ((unsigned int )skb->len - (unsigned int )skb->data_len);
}
}
__inline static unsigned char *skb_transport_header(struct sk_buff const *skb )
{
  {
  return ((unsigned char *)skb->head + (unsigned long )skb->transport_header);
}
}
__inline static unsigned char *skb_network_header(struct sk_buff const *skb )
{
  {
  return ((unsigned char *)skb->head + (unsigned long )skb->network_header);
}
}
__inline static struct page *skb_frag_page(skb_frag_t const *frag )
{
  {
  return ((struct page *)frag->page.p);
}
}
__inline static dma_addr_t skb_frag_dma_map(struct device *dev , skb_frag_t const *frag ,
                                            size_t offset , size_t size , enum dma_data_direction dir )
{
  struct page *tmp ;
  dma_addr_t tmp___0 ;
  {
  tmp = skb_frag_page(frag);
  tmp___0 = dma_map_page(dev, tmp, (size_t )frag->page_offset + offset, size, dir);
  return (tmp___0);
}
}
__inline static u16 skb_get_queue_mapping(struct sk_buff const *skb )
{
  {
  return ((u16 )skb->queue_mapping);
}
}
__inline static void dql_queued(struct dql *dql , unsigned int count )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect(count > 268435455U, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/dynamic_queue_limits.h"),
                         "i" (74), "i" (12UL));
    ldv_23061: ;
    goto ldv_23061;
  } else {
  }
  dql->num_queued = dql->num_queued + count;
  dql->last_obj_cnt = count;
  return;
}
}
__inline static int dql_avail(struct dql const *dql )
{
  {
  return ((int )((unsigned int )dql->adj_limit - (unsigned int )dql->num_queued));
}
}
extern void dql_completed(struct dql * , unsigned int ) ;
extern void dql_reset(struct dql * ) ;
__inline static bool netif_tx_queue_stopped(struct netdev_queue const *dev_queue )
{
  int tmp ;
  {
  tmp = constant_test_bit(0U, (unsigned long const volatile *)(& dev_queue->state));
  return (tmp != 0);
}
}
__inline static void netdev_tx_sent_queue(struct netdev_queue *dev_queue , unsigned int bytes )
{
  int tmp ;
  long tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  dql_queued(& dev_queue->dql, bytes);
  tmp = dql_avail((struct dql const *)(& dev_queue->dql));
  tmp___0 = ldv__builtin_expect(tmp >= 0, 1L);
  if (tmp___0 != 0L) {
    return;
  } else {
  }
  set_bit(1U, (unsigned long volatile *)(& dev_queue->state));
  __asm__ volatile ("mfence": : : "memory");
  tmp___1 = dql_avail((struct dql const *)(& dev_queue->dql));
  tmp___2 = ldv__builtin_expect(tmp___1 >= 0, 0L);
  if (tmp___2 != 0L) {
    clear_bit(1, (unsigned long volatile *)(& dev_queue->state));
  } else {
  }
  return;
}
}
__inline static void netdev_tx_completed_queue(struct netdev_queue *dev_queue , unsigned int pkts ,
                                               unsigned int bytes )
{
  long tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv__builtin_expect(bytes == 0U, 0L);
  if (tmp != 0L) {
    return;
  } else {
  }
  dql_completed(& dev_queue->dql, bytes);
  __asm__ volatile ("mfence": : : "memory");
  tmp___0 = dql_avail((struct dql const *)(& dev_queue->dql));
  if (tmp___0 < 0) {
    return;
  } else {
  }
  tmp___1 = test_and_clear_bit(1, (unsigned long volatile *)(& dev_queue->state));
  if (tmp___1 != 0) {
    netif_schedule_queue(dev_queue);
  } else {
  }
  return;
}
}
__inline static void netdev_tx_reset_queue(struct netdev_queue *q )
{
  {
  clear_bit(1, (unsigned long volatile *)(& q->state));
  dql_reset(& q->dql);
  return;
}
}
extern void dev_kfree_skb_any(struct sk_buff * ) ;
__inline static struct tcphdr *tcp_hdr(struct sk_buff const *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_transport_header(skb);
  return ((struct tcphdr *)tmp);
}
}
__inline static struct iphdr *ip_hdr(struct sk_buff const *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_network_header(skb);
  return ((struct iphdr *)tmp);
}
}
__inline static bool efx_xmit_with_hwtstamp(struct sk_buff *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  return (((int )((struct skb_shared_info *)tmp)->tx_flags & 1) != 0);
}
}
netdev_tx_t efx_enqueue_skb(struct efx_tx_queue *tx_queue , struct sk_buff *skb ) ;
bool efx_ptp_is_ptp_tx(struct efx_nic *efx , struct sk_buff *skb ) ;
int efx_ptp_tx(struct efx_nic *efx , struct sk_buff *skb ) ;
static void efx_dequeue_buffer(struct efx_tx_queue *tx_queue , struct efx_tx_buffer *buffer ,
                               unsigned int *pkts_compl , unsigned int *bytes_compl )
{
  struct device *dma_dev ;
  dma_addr_t unmap_addr ;
  {
  if ((unsigned int )buffer->unmap_len != 0U) {
    dma_dev = & ((tx_queue->efx)->pci_dev)->dev;
    unmap_addr = (buffer->dma_addr + (dma_addr_t )buffer->len) - (dma_addr_t )buffer->unmap_len;
    if (((int )buffer->flags & 8) != 0) {
      dma_unmap_single_attrs(dma_dev, unmap_addr, (size_t )buffer->unmap_len, 1, 0);
    } else {
      dma_unmap_page(dma_dev, unmap_addr, (size_t )buffer->unmap_len, 1);
    }
    buffer->unmap_len = 0U;
  } else {
  }
  if (((int )buffer->flags & 2) != 0) {
    *pkts_compl = *pkts_compl + 1U;
    *bytes_compl = *bytes_compl + (unsigned int )(buffer->ldv_45115.skb)->len;
    dev_kfree_skb_any((struct sk_buff *)buffer->ldv_45115.skb);
  } else
  if (((int )buffer->flags & 4) != 0) {
    kfree((void const *)buffer->ldv_45115.heap_buf);
  } else {
  }
  buffer->len = 0U;
  buffer->flags = 0U;
  return;
}
}
static int efx_enqueue_skb_tso(struct efx_tx_queue *tx_queue , struct sk_buff *skb ) ;
__inline static unsigned int efx_max_tx_len(struct efx_nic *efx , dma_addr_t dma_addr )
{
  unsigned int len ;
  unsigned int __min1 ;
  unsigned int __min2 ;
  int tmp ;
  {
  len = (~ ((unsigned int )dma_addr) & 4095U) + 1U;
  tmp = efx_nic_rev(efx);
  if (tmp <= 1 && (dma_addr & 15ULL) != 0ULL) {
    __min1 = len;
    __min2 = 512U - ((unsigned int )dma_addr & 15U);
    len = __min1 < __min2 ? __min1 : __min2;
  } else {
  }
  return (len);
}
}
unsigned int efx_tx_max_skb_descs(struct efx_nic *efx )
{
  unsigned int max_descs ;
  int tmp ;
  {
  max_descs = 217U;
  tmp = efx_nic_rev(efx);
  if (tmp <= 1) {
    max_descs = max_descs + 100U;
  } else {
  }
  return (max_descs);
}
}
static struct efx_tx_queue *efx_tx_queue_partner(struct efx_tx_queue *tx_queue )
{
  {
  if ((int )tx_queue->queue & 1) {
    return (tx_queue + 0xffffffffffffffffUL);
  } else {
    return (tx_queue + 1UL);
  }
}
}
static void efx_tx_maybe_stop_queue(struct efx_tx_queue *txq1 )
{
  struct efx_tx_queue *txq2 ;
  struct efx_tx_queue *tmp ;
  struct efx_nic *efx ;
  unsigned int fill_level ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  long tmp___0 ;
  unsigned int _max1___0 ;
  unsigned int _max2___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = efx_tx_queue_partner(txq1);
  txq2 = tmp;
  efx = txq1->efx;
  _max1 = txq1->insert_count - txq1->old_read_count;
  _max2 = txq2->insert_count - txq2->old_read_count;
  fill_level = _max1 > _max2 ? _max1 : _max2;
  tmp___0 = ldv__builtin_expect(efx->txq_stop_thresh > fill_level, 1L);
  if (tmp___0 != 0L) {
    return;
  } else {
  }
  netif_tx_stop_queue(txq1->core_txq);
  __asm__ volatile ("mfence": : : "memory");
  txq1->old_read_count = *((unsigned int volatile *)(& txq1->read_count));
  txq2->old_read_count = *((unsigned int volatile *)(& txq2->read_count));
  _max1___0 = txq1->insert_count - txq1->old_read_count;
  _max2___0 = txq2->insert_count - txq2->old_read_count;
  fill_level = _max1___0 > _max2___0 ? _max1___0 : _max2___0;
  tmp___2 = ldv__builtin_expect(efx->txq_stop_thresh > fill_level, 1L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("mfence": : : "memory");
    tmp___1 = ldv__builtin_expect((unsigned long )efx->loopback_selftest == (unsigned long )((void *)0),
                               1L);
    if (tmp___1 != 0L) {
      netif_tx_start_queue(txq1->core_txq);
    } else {
    }
  } else {
  }
  return;
}
}
netdev_tx_t efx_enqueue_skb(struct efx_tx_queue *tx_queue , struct sk_buff *skb )
{
  struct efx_nic *efx ;
  struct device *dma_dev ;
  struct efx_tx_buffer *buffer ;
  skb_frag_t *fragment ;
  unsigned int len ;
  unsigned int unmap_len ;
  unsigned int insert_ptr ;
  dma_addr_t dma_addr ;
  dma_addr_t unmap_addr ;
  unsigned int dma_len ;
  unsigned short dma_flags ;
  int i ;
  int tmp ;
  unsigned char *tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  long tmp___4 ;
  long tmp___5 ;
  unsigned char *tmp___6 ;
  unsigned char *tmp___7 ;
  unsigned char *tmp___8 ;
  unsigned int pkts_compl ;
  unsigned int bytes_compl ;
  {
  efx = tx_queue->efx;
  dma_dev = & (efx->pci_dev)->dev;
  unmap_len = 0U;
  unmap_addr = 0ULL;
  i = 0;
  tmp___0 = skb_end_pointer((struct sk_buff const *)skb);
  if ((unsigned int )((struct skb_shared_info *)tmp___0)->gso_size != 0U) {
    tmp = efx_enqueue_skb_tso(tx_queue, skb);
    return ((netdev_tx_t )tmp);
  } else {
  }
  len = skb_headlen((struct sk_buff const *)skb);
  tmp___2 = efx_nic_rev(efx);
  if (tmp___2 <= 2 && skb->len <= 32U) {
    len = 33U;
    tmp___1 = skb_pad(skb, (int )(len - skb->len));
    if (tmp___1 != 0) {
      return (0);
    } else {
    }
  } else {
  }
  dma_flags = 8U;
  dma_addr = dma_map_single_attrs(dma_dev, (void *)skb->data, (size_t )len, 1, 0);
  ldv_46650:
  tmp___3 = dma_mapping_error(dma_dev, dma_addr);
  tmp___4 = ldv__builtin_expect(tmp___3 != 0, 0L);
  if (tmp___4 != 0L) {
    goto dma_err;
  } else {
  }
  unmap_len = len;
  unmap_addr = dma_addr;
  ldv_46647:
  insert_ptr = tx_queue->insert_count & tx_queue->ptr_mask;
  buffer = tx_queue->buffer + (unsigned long )insert_ptr;
  dma_len = efx_max_tx_len(efx, dma_addr);
  tmp___5 = ldv__builtin_expect(dma_len >= len, 1L);
  if (tmp___5 != 0L) {
    dma_len = len;
  } else {
  }
  buffer->len = (unsigned short )dma_len;
  buffer->dma_addr = dma_addr;
  buffer->flags = 1U;
  len = len - dma_len;
  dma_addr = (dma_addr_t )dma_len + dma_addr;
  tx_queue->insert_count = tx_queue->insert_count + 1U;
  if (len != 0U) {
    goto ldv_46647;
  } else {
  }
  buffer->flags = (unsigned int )dma_flags | 1U;
  buffer->unmap_len = (unsigned short )unmap_len;
  unmap_len = 0U;
  tmp___6 = skb_end_pointer((struct sk_buff const *)skb);
  if ((int )((struct skb_shared_info *)tmp___6)->nr_frags <= i) {
    goto ldv_46649;
  } else {
  }
  tmp___7 = skb_end_pointer((struct sk_buff const *)skb);
  fragment = (skb_frag_t *)(& ((struct skb_shared_info *)tmp___7)->frags) + (unsigned long )i;
  len = skb_frag_size((skb_frag_t const *)fragment);
  i = i + 1;
  dma_flags = 0U;
  dma_addr = skb_frag_dma_map(dma_dev, (skb_frag_t const *)fragment, 0UL, (size_t )len,
                              1);
  goto ldv_46650;
  ldv_46649:
  buffer->ldv_45115.skb = (struct sk_buff const *)skb;
  buffer->flags = (unsigned int )dma_flags | 2U;
  netdev_tx_sent_queue(tx_queue->core_txq, skb->len);
  efx_nic_push_buffers(tx_queue);
  efx_tx_maybe_stop_queue(tx_queue);
  return (0);
  dma_err: ;
  if ((efx->msg_enable & 128U) != 0U) {
    tmp___8 = skb_end_pointer((struct sk_buff const *)skb);
    netdev_err((struct net_device const *)efx->net_dev, " TX queue %d could not map skb with %d bytes %d fragments for DMA\n",
               tx_queue->queue, skb->len, (int )((struct skb_shared_info *)tmp___8)->nr_frags + 1);
  } else {
  }
  dev_kfree_skb_any(skb);
  goto ldv_46654;
  ldv_46653:
  pkts_compl = 0U;
  bytes_compl = 0U;
  tx_queue->insert_count = tx_queue->insert_count - 1U;
  insert_ptr = tx_queue->insert_count & tx_queue->ptr_mask;
  buffer = tx_queue->buffer + (unsigned long )insert_ptr;
  efx_dequeue_buffer(tx_queue, buffer, & pkts_compl, & bytes_compl);
  ldv_46654: ;
  if (tx_queue->insert_count != tx_queue->write_count) {
    goto ldv_46653;
  } else {
  }
  if (unmap_len != 0U) {
    if (((int )dma_flags & 8) != 0) {
      dma_unmap_single_attrs(dma_dev, unmap_addr, (size_t )unmap_len, 1, 0);
    } else {
      dma_unmap_page(dma_dev, unmap_addr, (size_t )unmap_len, 1);
    }
  } else {
  }
  return (0);
}
}
static void efx_dequeue_buffers(struct efx_tx_queue *tx_queue , unsigned int index ,
                                unsigned int *pkts_compl , unsigned int *bytes_compl )
{
  struct efx_nic *efx ;
  unsigned int stop_index ;
  unsigned int read_ptr ;
  struct efx_tx_buffer *buffer ;
  long tmp ;
  {
  efx = tx_queue->efx;
  stop_index = (index + 1U) & tx_queue->ptr_mask;
  read_ptr = tx_queue->read_count & tx_queue->ptr_mask;
  goto ldv_46667;
  ldv_46666:
  buffer = tx_queue->buffer + (unsigned long )read_ptr;
  tmp = ldv__builtin_expect((unsigned int )buffer->len == 0U, 0L);
  if (tmp != 0L) {
    if ((efx->msg_enable & 128U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "TX queue %d spurious TX completion id %x\n",
                 tx_queue->queue, read_ptr);
    } else {
    }
    efx_schedule_reset(efx, 10);
    return;
  } else {
  }
  efx_dequeue_buffer(tx_queue, buffer, pkts_compl, bytes_compl);
  tx_queue->read_count = tx_queue->read_count + 1U;
  read_ptr = tx_queue->read_count & tx_queue->ptr_mask;
  ldv_46667: ;
  if (read_ptr != stop_index) {
    goto ldv_46666;
  } else {
  }
  return;
}
}
netdev_tx_t efx_hard_start_xmit(struct sk_buff *skb , struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_tx_queue *tx_queue ;
  unsigned int index ;
  unsigned int type ;
  int tmp___0 ;
  bool tmp___1 ;
  long tmp___2 ;
  bool tmp___3 ;
  long tmp___4 ;
  u16 tmp___5 ;
  netdev_tx_t tmp___6 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___1 = efx_xmit_with_hwtstamp(skb);
  tmp___2 = ldv__builtin_expect((long )tmp___1, 0L);
  if (tmp___2 != 0L) {
    tmp___3 = efx_ptp_is_ptp_tx(efx, skb);
    tmp___4 = ldv__builtin_expect((long )tmp___3, 0L);
    if (tmp___4 != 0L) {
      tmp___0 = efx_ptp_tx(efx, skb);
      return ((netdev_tx_t )tmp___0);
    } else {
    }
  } else {
  }
  tmp___5 = skb_get_queue_mapping((struct sk_buff const *)skb);
  index = (unsigned int )tmp___5;
  type = (unsigned int )*((unsigned char *)skb + 124UL) == 12U;
  if (efx->n_tx_channels <= index) {
    index = index - efx->n_tx_channels;
    type = type | 2U;
  } else {
  }
  tx_queue = efx_get_tx_queue(efx, index, type);
  tmp___6 = efx_enqueue_skb(tx_queue, skb);
  return (tmp___6);
}
}
void efx_init_tx_queue_core_txq(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  {
  efx = tx_queue->efx;
  tx_queue->core_txq = netdev_get_tx_queue((struct net_device const *)efx->net_dev,
                                           tx_queue->queue / 4U + ((tx_queue->queue & 2U) != 0U ? efx->n_tx_channels : 0U));
  return;
}
}
int efx_setup_tc(struct net_device *net_dev , u8 num_tc )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  unsigned int tc ;
  int rc ;
  int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  int __max1 ;
  int __max2 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 <= 1 || (unsigned int )num_tc > 2U) {
    return (-22);
  } else {
  }
  if ((int )net_dev->num_tc == (int )num_tc) {
    return (0);
  } else {
  }
  tc = 0U;
  goto ldv_46691;
  ldv_46690:
  net_dev->tc_to_txq[tc].offset = (int )((u16 )efx->n_tx_channels) * (int )((u16 )tc);
  net_dev->tc_to_txq[tc].count = (u16 )efx->n_tx_channels;
  tc = tc + 1U;
  ldv_46691: ;
  if ((unsigned int )num_tc > tc) {
    goto ldv_46690;
  } else {
  }
  if ((int )net_dev->num_tc < (int )num_tc) {
    channel = efx->channel[0];
    goto ldv_46698;
    ldv_46697:
    tmp___1 = efx_channel_has_tx_queues(channel);
    if (tmp___1) {
      tmp___2 = 0;
    } else {
      tmp___2 = 1;
    }
    if (tmp___2) {
    } else {
      tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
      goto ldv_46695;
      ldv_46694: ;
      if ((tx_queue->queue & 2U) == 0U) {
        goto ldv_46693;
      } else {
      }
      if ((unsigned long )tx_queue->buffer == (unsigned long )((struct efx_tx_buffer *)0)) {
        rc = efx_probe_tx_queue(tx_queue);
        if (rc != 0) {
          return (rc);
        } else {
        }
      } else {
      }
      if (! tx_queue->initialised) {
        efx_init_tx_queue(tx_queue);
      } else {
      }
      efx_init_tx_queue_core_txq(tx_queue);
      ldv_46693:
      tx_queue = tx_queue + 1;
      ldv_46695: ;
      if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
        goto ldv_46694;
      } else {
      }
    }
    channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
    ldv_46698: ;
    if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
      goto ldv_46697;
    } else {
    }
  } else {
    net_dev->num_tc = num_tc;
  }
  __max1 = (int )num_tc;
  __max2 = 1;
  rc = netif_set_real_num_tx_queues(net_dev, (unsigned int )(__max1 > __max2 ? __max1 : __max2) * efx->n_tx_channels);
  if (rc != 0) {
    return (rc);
  } else {
  }
  net_dev->num_tc = num_tc;
  return (0);
}
}
void efx_xmit_done(struct efx_tx_queue *tx_queue , unsigned int index )
{
  unsigned int fill_level ;
  struct efx_nic *efx ;
  struct efx_tx_queue *txq2 ;
  unsigned int pkts_compl ;
  unsigned int bytes_compl ;
  unsigned int _max1 ;
  unsigned int _max2 ;
  bool tmp ;
  long tmp___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  long tmp___3 ;
  {
  efx = tx_queue->efx;
  pkts_compl = 0U;
  bytes_compl = 0U;
  efx_dequeue_buffers(tx_queue, index, & pkts_compl, & bytes_compl);
  netdev_tx_completed_queue(tx_queue->core_txq, pkts_compl, bytes_compl);
  __asm__ volatile ("mfence": : : "memory");
  tmp = netif_tx_queue_stopped((struct netdev_queue const *)tx_queue->core_txq);
  tmp___0 = ldv__builtin_expect((long )tmp, 0L);
  if (tmp___0 != 0L) {
    tmp___1 = ldv__builtin_expect((long )efx->port_enabled, 1L);
    if (tmp___1 != 0L) {
      tmp___2 = netif_device_present(efx->net_dev);
      tmp___3 = ldv__builtin_expect((long )tmp___2, 1L);
      if (tmp___3 != 0L) {
        txq2 = efx_tx_queue_partner(tx_queue);
        _max1 = tx_queue->insert_count - tx_queue->read_count;
        _max2 = txq2->insert_count - txq2->read_count;
        fill_level = _max1 > _max2 ? _max1 : _max2;
        if (efx->txq_wake_thresh >= fill_level) {
          netif_tx_wake_queue(tx_queue->core_txq);
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  if ((int )(tx_queue->read_count - tx_queue->old_write_count) >= 0) {
    tx_queue->old_write_count = *((unsigned int volatile *)(& tx_queue->write_count));
    if (tx_queue->read_count == tx_queue->old_write_count) {
      __asm__ volatile ("mfence": : : "memory");
      tx_queue->empty_read_count = tx_queue->read_count | 2147483648U;
    } else {
    }
  } else {
  }
  return;
}
}
static unsigned int efx_tsoh_page_count(struct efx_tx_queue *tx_queue )
{
  {
  return ((unsigned int )(((unsigned long )(tx_queue->ptr_mask + 1U) + 63UL) / 64UL));
}
}
int efx_probe_tx_queue(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  unsigned int entries ;
  int rc ;
  unsigned long _max1 ;
  unsigned long tmp ;
  unsigned long _max2 ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  void *tmp___1 ;
  unsigned int tmp___2 ;
  void *tmp___3 ;
  {
  efx = tx_queue->efx;
  tmp = __roundup_pow_of_two((unsigned long )efx->txq_entries);
  _max1 = tmp;
  _max2 = 512UL;
  entries = (unsigned int )(_max1 > _max2 ? _max1 : _max2);
  tx_queue->ptr_mask = entries - 1U;
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_tx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/tx.c.prepared";
    descriptor.format = "creating TX queue %d size %#x mask %#x\n";
    descriptor.lineno = 581U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "creating TX queue %d size %#x mask %#x\n", tx_queue->queue,
                           efx->txq_entries, tx_queue->ptr_mask);
    } else {
    }
  } else {
  }
  tmp___1 = kcalloc((size_t )entries, 24UL, 208U);
  tx_queue->buffer = (struct efx_tx_buffer *)tmp___1;
  if ((unsigned long )tx_queue->buffer == (unsigned long )((struct efx_tx_buffer *)0)) {
    return (-12);
  } else {
  }
  if ((int )tx_queue->queue & 1) {
    tmp___2 = efx_tsoh_page_count(tx_queue);
    tmp___3 = kcalloc((size_t )tmp___2, 24UL, 208U);
    tx_queue->tsoh_page = (struct efx_buffer *)tmp___3;
    if ((unsigned long )tx_queue->tsoh_page == (unsigned long )((struct efx_buffer *)0)) {
      rc = -12;
      goto fail1;
    } else {
    }
  } else {
  }
  rc = efx_nic_probe_tx(tx_queue);
  if (rc != 0) {
    goto fail2;
  } else {
  }
  return (0);
  fail2:
  kfree((void const *)tx_queue->tsoh_page);
  tx_queue->tsoh_page = 0;
  fail1:
  kfree((void const *)tx_queue->buffer);
  tx_queue->buffer = 0;
  return (rc);
}
}
void efx_init_tx_queue(struct efx_tx_queue *tx_queue )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if ((int )(tx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_tx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/tx.c.prepared";
    descriptor.format = "initialising TX queue %d\n";
    descriptor.lineno = 618U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(tx_queue->efx)->net_dev,
                           "initialising TX queue %d\n", tx_queue->queue);
    } else {
    }
  } else {
  }
  tx_queue->insert_count = 0U;
  tx_queue->write_count = 0U;
  tx_queue->old_write_count = 0U;
  tx_queue->read_count = 0U;
  tx_queue->old_read_count = 0U;
  tx_queue->empty_read_count = 2147483648U;
  efx_nic_init_tx(tx_queue);
  tx_queue->initialised = 1;
  return;
}
}
void efx_release_tx_buffers(struct efx_tx_queue *tx_queue )
{
  struct efx_tx_buffer *buffer ;
  unsigned int pkts_compl ;
  unsigned int bytes_compl ;
  {
  if ((unsigned long )tx_queue->buffer == (unsigned long )((struct efx_tx_buffer *)0)) {
    return;
  } else {
  }
  goto ldv_46743;
  ldv_46742:
  pkts_compl = 0U;
  bytes_compl = 0U;
  buffer = tx_queue->buffer + (unsigned long )(tx_queue->read_count & tx_queue->ptr_mask);
  efx_dequeue_buffer(tx_queue, buffer, & pkts_compl, & bytes_compl);
  tx_queue->read_count = tx_queue->read_count + 1U;
  ldv_46743: ;
  if (tx_queue->read_count != tx_queue->write_count) {
    goto ldv_46742;
  } else {
  }
  netdev_tx_reset_queue(tx_queue->core_txq);
  return;
}
}
void efx_fini_tx_queue(struct efx_tx_queue *tx_queue )
{
  struct _ddebug descriptor ;
  long tmp ;
  {
  if (! tx_queue->initialised) {
    return;
  } else {
  }
  if ((int )(tx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_tx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/tx.c.prepared";
    descriptor.format = "shutting down TX queue %d\n";
    descriptor.lineno = 657U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(tx_queue->efx)->net_dev,
                           "shutting down TX queue %d\n", tx_queue->queue);
    } else {
    }
  } else {
  }
  tx_queue->initialised = 0;
  efx_nic_fini_tx(tx_queue);
  efx_release_tx_buffers(tx_queue);
  return;
}
}
void efx_remove_tx_queue(struct efx_tx_queue *tx_queue )
{
  int i ;
  struct _ddebug descriptor ;
  long tmp ;
  unsigned int tmp___0 ;
  {
  if ((unsigned long )tx_queue->buffer == (unsigned long )((struct efx_tx_buffer *)0)) {
    return;
  } else {
  }
  if ((int )(tx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_tx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/tx.c.prepared";
    descriptor.format = "destroying TX queue %d\n";
    descriptor.lineno = 675U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(tx_queue->efx)->net_dev,
                           "destroying TX queue %d\n", tx_queue->queue);
    } else {
    }
  } else {
  }
  efx_nic_remove_tx(tx_queue);
  if ((unsigned long )tx_queue->tsoh_page != (unsigned long )((struct efx_buffer *)0)) {
    i = 0;
    goto ldv_46757;
    ldv_46756:
    efx_nic_free_buffer(tx_queue->efx, tx_queue->tsoh_page + (unsigned long )i);
    i = i + 1;
    ldv_46757:
    tmp___0 = efx_tsoh_page_count(tx_queue);
    if ((unsigned int )i < tmp___0) {
      goto ldv_46756;
    } else {
    }
    kfree((void const *)tx_queue->tsoh_page);
    tx_queue->tsoh_page = 0;
  } else {
  }
  kfree((void const *)tx_queue->buffer);
  tx_queue->buffer = 0;
  return;
}
}
static __be16 efx_tso_check_protocol(struct sk_buff *skb )
{
  __be16 protocol ;
  struct vlan_ethhdr *veh ;
  {
  protocol = skb->protocol;
  if ((unsigned int )protocol == 129U) {
    veh = (struct vlan_ethhdr *)skb->data;
    protocol = veh->h_vlan_encapsulated_proto;
  } else {
  }
  return (protocol);
}
}
static u8 *efx_tsoh_get_buffer(struct efx_tx_queue *tx_queue , struct efx_tx_buffer *buffer ,
                               unsigned int len )
{
  u8 *result ;
  unsigned int index ;
  struct efx_buffer *page_buf ;
  unsigned int offset ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp___2 = ldv__builtin_expect(len <= 128U, 1L);
  if (tmp___2 != 0L) {
    index = (tx_queue->insert_count & tx_queue->ptr_mask) / 2U;
    page_buf = tx_queue->tsoh_page + (unsigned long )(index / 32U);
    offset = (index & 31U) * 128U;
    tmp = ldv__builtin_expect((unsigned long )page_buf->addr == (unsigned long )((void *)0),
                           0L);
    if (tmp != 0L) {
      tmp___0 = efx_nic_alloc_buffer(tx_queue->efx, page_buf, 4096U);
      if (tmp___0 != 0) {
        return (0);
      } else {
      }
    } else {
    }
    result = (u8 *)page_buf->addr + (unsigned long )offset;
    buffer->dma_addr = page_buf->dma_addr + (dma_addr_t )offset;
    buffer->flags = 1U;
  } else {
    tx_queue->tso_long_headers = tx_queue->tso_long_headers + 1U;
    buffer->ldv_45115.heap_buf = kmalloc((size_t )len, 32U);
    tmp___1 = ldv__builtin_expect((unsigned long )buffer->ldv_45115.heap_buf == (unsigned long )((void *)0),
                               0L);
    if (tmp___1 != 0L) {
      return (0);
    } else {
    }
    result = (u8 *)buffer->ldv_45115.heap_buf;
    buffer->flags = 5U;
  }
  buffer->len = (unsigned short )len;
  return (result);
}
}
static void efx_tx_queue_insert(struct efx_tx_queue *tx_queue , dma_addr_t dma_addr ,
                                unsigned int len , struct efx_tx_buffer **final_buffer )
{
  struct efx_tx_buffer *buffer ;
  struct efx_nic *efx ;
  unsigned int dma_len ;
  unsigned int insert_ptr ;
  {
  efx = tx_queue->efx;
  ldv_46799:
  insert_ptr = tx_queue->insert_count & tx_queue->ptr_mask;
  buffer = tx_queue->buffer + (unsigned long )insert_ptr;
  tx_queue->insert_count = tx_queue->insert_count + 1U;
  buffer->dma_addr = dma_addr;
  dma_len = efx_max_tx_len(efx, dma_addr);
  if (dma_len >= len) {
    goto ldv_46798;
  } else {
  }
  buffer->len = (unsigned short )dma_len;
  buffer->flags = 1U;
  dma_addr = (dma_addr_t )dma_len + dma_addr;
  len = len - dma_len;
  goto ldv_46799;
  ldv_46798:
  buffer->len = (unsigned short )len;
  *final_buffer = buffer;
  return;
}
}
static int efx_tso_put_header(struct efx_tx_queue *tx_queue , struct efx_tx_buffer *buffer ,
                              u8 *header )
{
  int tmp ;
  long tmp___0 ;
  long tmp___1 ;
  {
  tmp___1 = ldv__builtin_expect(((int )buffer->flags & 4) != 0, 0L);
  if (tmp___1 != 0L) {
    buffer->dma_addr = dma_map_single_attrs(& ((tx_queue->efx)->pci_dev)->dev, (void *)header,
                                            (size_t )buffer->len, 1, 0);
    tmp = dma_mapping_error(& ((tx_queue->efx)->pci_dev)->dev, buffer->dma_addr);
    tmp___0 = ldv__builtin_expect(tmp != 0, 0L);
    if (tmp___0 != 0L) {
      kfree((void const *)buffer->ldv_45115.heap_buf);
      buffer->len = 0U;
      buffer->flags = 0U;
      return (-12);
    } else {
    }
    buffer->unmap_len = buffer->len;
    buffer->flags = (unsigned int )buffer->flags | 8U;
  } else {
  }
  tx_queue->insert_count = tx_queue->insert_count + 1U;
  return (0);
}
}
static void efx_enqueue_unwind(struct efx_tx_queue *tx_queue )
{
  struct efx_tx_buffer *buffer ;
  {
  goto ldv_46810;
  ldv_46809:
  tx_queue->insert_count = tx_queue->insert_count - 1U;
  buffer = tx_queue->buffer + (unsigned long )(tx_queue->insert_count & tx_queue->ptr_mask);
  efx_dequeue_buffer(tx_queue, buffer, 0, 0);
  ldv_46810: ;
  if (tx_queue->insert_count != tx_queue->write_count) {
    goto ldv_46809;
  } else {
  }
  return;
}
}
static void tso_start(struct tso_state *st , struct sk_buff const *skb )
{
  unsigned char *tmp ;
  unsigned char *tmp___0 ;
  struct tcphdr *tmp___1 ;
  struct iphdr *tmp___2 ;
  __u16 tmp___3 ;
  struct tcphdr *tmp___4 ;
  __u32 tmp___5 ;
  {
  tmp = skb_network_header(skb);
  st->ip_off = (unsigned int )((long )tmp) - (unsigned int )((long )skb->data);
  tmp___0 = skb_transport_header(skb);
  st->tcp_off = (unsigned int )((long )tmp___0) - (unsigned int )((long )skb->data);
  tmp___1 = tcp_hdr(skb);
  st->header_len = st->tcp_off + (unsigned int )((int )tmp___1->doff << 2);
  if ((unsigned int )st->protocol == 8U) {
    st->ip_base_len = st->header_len - st->ip_off;
    tmp___2 = ip_hdr(skb);
    tmp___3 = __fswab16((int )tmp___2->id);
    st->ipv4_id = (unsigned int )tmp___3;
  } else {
    st->ip_base_len = st->header_len - st->tcp_off;
    st->ipv4_id = 0U;
  }
  tmp___4 = tcp_hdr(skb);
  tmp___5 = __fswab32(tmp___4->seq);
  st->seqnum = tmp___5;
  st->out_len = (unsigned int )skb->len - st->header_len;
  st->unmap_len = 0U;
  st->dma_flags = 0U;
  return;
}
}
static int tso_get_fragment(struct tso_state *st , struct efx_nic *efx , skb_frag_t *frag )
{
  unsigned int tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = skb_frag_size((skb_frag_t const *)frag);
  st->unmap_addr = skb_frag_dma_map(& (efx->pci_dev)->dev, (skb_frag_t const *)frag,
                                    0UL, (size_t )tmp, 1);
  tmp___0 = dma_mapping_error(& (efx->pci_dev)->dev, st->unmap_addr);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 1L);
  if (tmp___1 != 0L) {
    st->dma_flags = 0U;
    st->unmap_len = skb_frag_size((skb_frag_t const *)frag);
    st->in_len = skb_frag_size((skb_frag_t const *)frag);
    st->dma_addr = st->unmap_addr;
    return (0);
  } else {
  }
  return (-12);
}
}
static int tso_get_head_fragment(struct tso_state *st , struct efx_nic *efx , struct sk_buff const *skb )
{
  int hl ;
  int len ;
  unsigned int tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  hl = (int )st->header_len;
  tmp = skb_headlen(skb);
  len = (int )(tmp - (unsigned int )hl);
  st->unmap_addr = dma_map_single_attrs(& (efx->pci_dev)->dev, (void *)skb->data + (unsigned long )hl,
                                        (size_t )len, 1, 0);
  tmp___0 = dma_mapping_error(& (efx->pci_dev)->dev, st->unmap_addr);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 1L);
  if (tmp___1 != 0L) {
    st->dma_flags = 8U;
    st->unmap_len = (unsigned int )len;
    st->in_len = (unsigned int )len;
    st->dma_addr = st->unmap_addr;
    return (0);
  } else {
  }
  return (-12);
}
}
static void tso_fill_packet_with_fragment(struct efx_tx_queue *tx_queue , struct sk_buff const *skb ,
                                          struct tso_state *st )
{
  struct efx_tx_buffer *buffer ;
  int n ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  {
  if (st->in_len == 0U) {
    return;
  } else {
  }
  if (st->packet_space == 0U) {
    return;
  } else {
  }
  _min1 = st->in_len;
  _min2 = st->packet_space;
  n = (int )(_min1 < _min2 ? _min1 : _min2);
  st->packet_space = st->packet_space - (unsigned int )n;
  st->out_len = st->out_len - (unsigned int )n;
  st->in_len = st->in_len - (unsigned int )n;
  efx_tx_queue_insert(tx_queue, st->dma_addr, (unsigned int )n, & buffer);
  if (st->out_len == 0U) {
    buffer->ldv_45115.skb = skb;
    buffer->flags = 2U;
  } else
  if (st->packet_space != 0U) {
    buffer->flags = 1U;
  } else {
  }
  if (st->in_len == 0U) {
    buffer->unmap_len = (unsigned short )st->unmap_len;
    buffer->flags = (int )buffer->flags | (int )st->dma_flags;
    st->unmap_len = 0U;
  } else {
  }
  st->dma_addr = st->dma_addr + (dma_addr_t )n;
  return;
}
}
static int tso_start_new_packet(struct efx_tx_queue *tx_queue , struct sk_buff const *skb ,
                                struct tso_state *st )
{
  struct efx_tx_buffer *buffer ;
  struct tcphdr *tsoh_th ;
  unsigned int ip_length ;
  u8 *header ;
  int rc ;
  size_t __len ;
  void *__ret ;
  __u32 tmp ;
  unsigned char *tmp___0 ;
  unsigned char *tmp___1 ;
  struct tcphdr *tmp___2 ;
  struct tcphdr *tmp___3 ;
  unsigned char *tmp___4 ;
  struct iphdr *tsoh_iph ;
  __u16 tmp___5 ;
  __u16 tmp___6 ;
  struct ipv6hdr *tsoh_iph___0 ;
  __u16 tmp___7 ;
  long tmp___8 ;
  {
  buffer = tx_queue->buffer + (unsigned long )(tx_queue->insert_count & tx_queue->ptr_mask);
  header = efx_tsoh_get_buffer(tx_queue, buffer, st->header_len);
  if ((unsigned long )header == (unsigned long )((u8 *)0)) {
    return (-12);
  } else {
  }
  tsoh_th = (struct tcphdr *)header + (unsigned long )st->tcp_off;
  __len = (size_t )st->header_len;
  __ret = memcpy((void *)header, (void const *)skb->data, __len);
  tmp = __fswab32(st->seqnum);
  tsoh_th->seq = tmp;
  tmp___0 = skb_end_pointer(skb);
  st->seqnum = st->seqnum + (unsigned int )((struct skb_shared_info *)tmp___0)->gso_size;
  tmp___4 = skb_end_pointer(skb);
  if (st->out_len > (unsigned int )((struct skb_shared_info *)tmp___4)->gso_size) {
    tmp___1 = skb_end_pointer(skb);
    st->packet_space = (unsigned int )((struct skb_shared_info *)tmp___1)->gso_size;
    tsoh_th->fin = 0U;
    tsoh_th->psh = 0U;
  } else {
    st->packet_space = st->out_len;
    tmp___2 = tcp_hdr(skb);
    tsoh_th->fin = tmp___2->fin;
    tmp___3 = tcp_hdr(skb);
    tsoh_th->psh = tmp___3->psh;
  }
  ip_length = st->ip_base_len + st->packet_space;
  if ((unsigned int )st->protocol == 8U) {
    tsoh_iph = (struct iphdr *)header + (unsigned long )st->ip_off;
    tmp___5 = __fswab16((int )((__u16 )ip_length));
    tsoh_iph->tot_len = tmp___5;
    tmp___6 = __fswab16((int )((__u16 )st->ipv4_id));
    tsoh_iph->id = tmp___6;
    st->ipv4_id = st->ipv4_id + 1U;
  } else {
    tsoh_iph___0 = (struct ipv6hdr *)header + (unsigned long )st->ip_off;
    tmp___7 = __fswab16((int )((__u16 )ip_length));
    tsoh_iph___0->payload_len = tmp___7;
  }
  rc = efx_tso_put_header(tx_queue, buffer, header);
  tmp___8 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___8 != 0L) {
    return (rc);
  } else {
  }
  tx_queue->tso_packets = tx_queue->tso_packets + 1U;
  return (0);
}
}
static int efx_enqueue_skb_tso(struct efx_tx_queue *tx_queue , struct sk_buff *skb )
{
  struct efx_nic *efx ;
  int frag_i ;
  int rc ;
  struct tso_state state ;
  unsigned char *tmp ;
  unsigned int tmp___0 ;
  int tmp___1 ;
  unsigned char *tmp___2 ;
  unsigned char *tmp___3 ;
  int tmp___4 ;
  {
  efx = tx_queue->efx;
  state.protocol = efx_tso_check_protocol(skb);
  tso_start(& state, (struct sk_buff const *)skb);
  tmp___0 = skb_headlen((struct sk_buff const *)skb);
  if (tmp___0 == state.header_len) {
    frag_i = 0;
    tmp = skb_end_pointer((struct sk_buff const *)skb);
    rc = tso_get_fragment(& state, efx, (skb_frag_t *)(& ((struct skb_shared_info *)tmp)->frags) + (unsigned long )frag_i);
    if (rc != 0) {
      goto mem_err;
    } else {
    }
  } else {
    rc = tso_get_head_fragment(& state, efx, (struct sk_buff const *)skb);
    if (rc != 0) {
      goto mem_err;
    } else {
    }
    frag_i = -1;
  }
  tmp___1 = tso_start_new_packet(tx_queue, (struct sk_buff const *)skb, & state);
  if (tmp___1 < 0) {
    goto mem_err;
  } else {
  }
  ldv_46863:
  tso_fill_packet_with_fragment(tx_queue, (struct sk_buff const *)skb, & state);
  if (state.in_len == 0U) {
    frag_i = frag_i + 1;
    tmp___2 = skb_end_pointer((struct sk_buff const *)skb);
    if (frag_i >= (int )((struct skb_shared_info *)tmp___2)->nr_frags) {
      goto ldv_46862;
    } else {
    }
    tmp___3 = skb_end_pointer((struct sk_buff const *)skb);
    rc = tso_get_fragment(& state, efx, (skb_frag_t *)(& ((struct skb_shared_info *)tmp___3)->frags) + (unsigned long )frag_i);
    if (rc != 0) {
      goto mem_err;
    } else {
    }
  } else {
  }
  if (state.packet_space == 0U) {
    tmp___4 = tso_start_new_packet(tx_queue, (struct sk_buff const *)skb, & state);
    if (tmp___4 < 0) {
      goto mem_err;
    } else {
    }
  } else {
  }
  goto ldv_46863;
  ldv_46862:
  netdev_tx_sent_queue(tx_queue->core_txq, skb->len);
  efx_nic_push_buffers(tx_queue);
  efx_tx_maybe_stop_queue(tx_queue);
  tx_queue->tso_bursts = tx_queue->tso_bursts + 1U;
  return (0);
  mem_err: ;
  if ((efx->msg_enable & 128U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "Out of memory for TSO headers, or DMA mapping error\n");
  } else {
  }
  dev_kfree_skb_any(skb);
  if (state.unmap_len != 0U) {
    if (((int )state.dma_flags & 8) != 0) {
      dma_unmap_single_attrs(& (efx->pci_dev)->dev, state.unmap_addr, (size_t )state.unmap_len,
                             1, 0);
    } else {
      dma_unmap_page(& (efx->pci_dev)->dev, state.unmap_addr, (size_t )state.unmap_len,
                     1);
    }
  } else {
  }
  efx_enqueue_unwind(tx_queue);
  return (0);
}
}
void ldv_mutex_lock_131(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_132(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_133(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_134(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_135(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_136(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_137(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static __u32 __le32_to_cpup(__le32 const *p )
{
  {
  return ((__u32 )*p);
}
}
int ldv_mutex_trylock_148(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_146(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_149(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_151(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_145(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_147(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_150(struct mutex *ldv_func_arg1 ) ;
extern struct page *alloc_pages_current(gfp_t , unsigned int ) ;
__inline static struct page *alloc_pages(gfp_t gfp_mask , unsigned int order )
{
  struct page *tmp ;
  {
  tmp = alloc_pages_current(gfp_mask, order);
  return (tmp);
}
}
extern void __free_pages(struct page * , unsigned int ) ;
extern int net_ratelimit(void) ;
__inline static int PageTail(struct page const *page )
{
  int tmp ;
  {
  tmp = constant_test_bit(15U, (unsigned long const volatile *)(& page->flags));
  return (tmp);
}
}
__inline static struct page *compound_head(struct page *page )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = PageTail((struct page const *)page);
  tmp___0 = ldv__builtin_expect(tmp != 0, 0L);
  if (tmp___0 != 0L) {
    return (page->ldv_16820.first_page);
  } else {
  }
  return (page);
}
}
__inline static int page_count(struct page *page )
{
  struct page *tmp ;
  int tmp___0 ;
  {
  tmp = compound_head(page);
  tmp___0 = atomic_read((atomic_t const *)(& tmp->ldv_16804.ldv_16803.ldv_16802._count));
  return (tmp___0);
}
}
extern bool __get_page_tail(struct page * ) ;
__inline static void get_page(struct page *page )
{
  bool tmp ;
  long tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  int tmp___3 ;
  long tmp___4 ;
  {
  tmp___1 = PageTail((struct page const *)page);
  tmp___2 = ldv__builtin_expect(tmp___1 != 0, 0L);
  if (tmp___2 != 0L) {
    tmp = __get_page_tail(page);
    tmp___0 = ldv__builtin_expect((long )tmp, 1L);
    if (tmp___0 != 0L) {
      return;
    } else {
    }
  } else {
  }
  tmp___3 = atomic_read((atomic_t const *)(& page->ldv_16804.ldv_16803.ldv_16802._count));
  tmp___4 = ldv__builtin_expect(tmp___3 <= 0, 0L);
  if (tmp___4 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/mm.h"),
                         "i" (406), "i" (12UL));
    ldv_16244: ;
    goto ldv_16244;
  } else {
  }
  atomic_inc(& page->ldv_16804.ldv_16803.ldv_16802._count);
  return;
}
}
extern void put_page(struct page * ) ;
__inline static dma_addr_t dma_map_single_attrs___0(struct device *dev , void *ptr ,
                                                    size_t size , enum dma_data_direction dir ,
                                                    struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  int tmp___0 ;
  long tmp___1 ;
  unsigned long tmp___2 ;
  unsigned long tmp___3 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  kmemcheck_mark_initialized(ptr, (unsigned int )size);
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (19), "i" (12UL));
    ldv_18768: ;
    goto ldv_18768;
  } else {
  }
  tmp___2 = __phys_addr((unsigned long )ptr);
  addr = (*(ops->map_page))(dev, 0xffffea0000000000UL + (tmp___2 >> 12), (unsigned long )ptr & 4095UL,
                            size, dir, attrs);
  tmp___3 = __phys_addr((unsigned long )ptr);
  debug_dma_map_page(dev, 0xffffea0000000000UL + (tmp___3 >> 12), (unsigned long )ptr & 4095UL,
                     size, (int )dir, addr, 1);
  return (addr);
}
}
__inline static void dma_unmap_single_attrs___0(struct device *dev , dma_addr_t addr ,
                                                size_t size , enum dma_data_direction dir ,
                                                struct dma_attrs *attrs )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (36), "i" (12UL));
    ldv_18777: ;
    goto ldv_18777;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, attrs);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 1);
  return;
}
}
__inline static dma_addr_t dma_map_page___0(struct device *dev , struct page *page ,
                                            size_t offset , size_t size , enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  dma_addr_t addr ;
  void *tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = lowmem_page_address((struct page const *)page);
  kmemcheck_mark_initialized(tmp___0 + offset, (unsigned int )size);
  tmp___1 = valid_dma_direction((int )dir);
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (79), "i" (12UL));
    ldv_18811: ;
    goto ldv_18811;
  } else {
  }
  addr = (*(ops->map_page))(dev, page, offset, size, dir, 0);
  debug_dma_map_page(dev, page, offset, size, (int )dir, addr, 0);
  return (addr);
}
}
__inline static void dma_unmap_page___0(struct device *dev , dma_addr_t addr , size_t size ,
                                        enum dma_data_direction dir )
{
  struct dma_map_ops *ops ;
  struct dma_map_ops *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = get_dma_ops(dev);
  ops = tmp;
  tmp___0 = valid_dma_direction((int )dir);
  tmp___1 = ldv__builtin_expect(tmp___0 == 0, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/asm-generic/dma-mapping-common.h"),
                         "i" (91), "i" (12UL));
    ldv_18819: ;
    goto ldv_18819;
  } else {
  }
  if ((unsigned long )ops->unmap_page != (unsigned long )((void (*)(struct device * ,
                                                                    dma_addr_t ,
                                                                    size_t , enum dma_data_direction ,
                                                                    struct dma_attrs * ))0)) {
    (*(ops->unmap_page))(dev, addr, size, dir, 0);
  } else {
  }
  debug_dma_unmap_page(dev, addr, size, (int )dir, 0);
  return;
}
}
__inline static void skb_frag_size_set(skb_frag_t *frag , unsigned int size )
{
  {
  frag->size = size;
  return;
}
}
__inline static void __skb_fill_page_desc(struct sk_buff *skb , int i , struct page *page ,
                                          int off , int size )
{
  skb_frag_t *frag ;
  unsigned char *tmp ;
  {
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  frag = (skb_frag_t *)(& ((struct skb_shared_info *)tmp)->frags) + (unsigned long )i;
  if ((int )page->ldv_16804.ldv_16788.pfmemalloc && (unsigned long )page->mapping == (unsigned long )((struct address_space *)0)) {
    skb->pfmemalloc = 1U;
  } else {
  }
  frag->page.p = page;
  frag->page_offset = (__u32 )off;
  skb_frag_size_set(frag, (unsigned int )size);
  return;
}
}
__inline static void skb_fill_page_desc(struct sk_buff *skb , int i , struct page *page ,
                                        int off , int size )
{
  unsigned char *tmp ;
  {
  __skb_fill_page_desc(skb, i, page, off, size);
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  ((struct skb_shared_info *)tmp)->nr_frags = (unsigned int )((unsigned char )i) + 1U;
  return;
}
}
extern unsigned char *skb_put(struct sk_buff * , unsigned int ) ;
__inline static void skb_reserve(struct sk_buff *skb , int len )
{
  {
  skb->data = skb->data + (unsigned long )len;
  skb->tail = skb->tail + (sk_buff_data_t )len;
  return;
}
}
extern struct sk_buff *__netdev_alloc_skb(struct net_device * , unsigned int , gfp_t ) ;
__inline static struct sk_buff *netdev_alloc_skb(struct net_device *dev , unsigned int length )
{
  struct sk_buff *tmp ;
  {
  tmp = __netdev_alloc_skb(dev, length, 32U);
  return (tmp);
}
}
__inline static void skb_record_rx_queue(struct sk_buff *skb , u16 rx_queue )
{
  {
  skb->queue_mapping = (unsigned int )rx_queue + 1U;
  return;
}
}
__inline static void skb_checksum_none_assert(struct sk_buff const *skb )
{
  {
  return;
}
}
extern int netif_receive_skb(struct sk_buff * ) ;
extern gro_result_t napi_gro_receive(struct napi_struct * , struct sk_buff * ) ;
extern struct sk_buff *napi_get_frags(struct napi_struct * ) ;
extern gro_result_t napi_gro_frags(struct napi_struct * ) ;
extern __be16 eth_type_trans(struct sk_buff * , struct net_device * ) ;
void efx_loopback_rx_packet(struct efx_nic *efx , char const *buf_ptr , int pkt_len ) ;
static int rx_alloc_method = 0;
static unsigned int rx_refill_threshold ;
__inline static unsigned int efx_rx_buf_offset(struct efx_nic *efx , struct efx_rx_buffer *buf )
{
  {
  return (((unsigned int )buf->dma_addr & 4095U) + (unsigned int )(efx->type)->rx_buffer_hash_size);
}
}
__inline static unsigned int efx_rx_buf_size(struct efx_nic *efx )
{
  {
  return ((unsigned int )(4096UL << (int )efx->rx_buffer_order));
}
}
static u8 *efx_rx_buf_eh(struct efx_nic *efx , struct efx_rx_buffer *buf )
{
  void *tmp ;
  unsigned int tmp___0 ;
  {
  if ((int )buf->flags & 1) {
    tmp = lowmem_page_address((struct page const *)buf->u.page);
    tmp___0 = efx_rx_buf_offset(efx, buf);
    return ((u8 *)tmp + (unsigned long )tmp___0);
  } else {
    return ((buf->u.skb)->data + (unsigned long )(efx->type)->rx_buffer_hash_size);
  }
}
}
__inline static u32 efx_rx_buf_hash(u8 const *eh )
{
  __u32 tmp ;
  {
  tmp = __le32_to_cpup((__le32 const *)eh + 0xfffffffffffffffcUL);
  return (tmp);
}
}
static int efx_init_rx_buffers_skb(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  struct net_device *net_dev ;
  struct efx_rx_buffer *rx_buf ;
  struct sk_buff *skb ;
  int skb_len ;
  unsigned int index ;
  unsigned int count ;
  long tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  efx = rx_queue->efx;
  net_dev = efx->net_dev;
  skb_len = (int )efx->rx_buffer_len;
  count = 0U;
  goto ldv_46379;
  ldv_46378:
  index = (unsigned int )rx_queue->added_count & rx_queue->ptr_mask;
  rx_buf = efx_rx_buffer(rx_queue, index);
  skb = netdev_alloc_skb(net_dev, (unsigned int )skb_len);
  rx_buf->u.skb = skb;
  tmp = ldv__builtin_expect((unsigned long )skb == (unsigned long )((struct sk_buff *)0),
                         0L);
  if (tmp != 0L) {
    return (-12);
  } else {
  }
  skb_reserve(skb, 0);
  rx_buf->len = (unsigned int )skb_len;
  rx_buf->flags = 0U;
  rx_buf->dma_addr = dma_map_single_attrs___0(& (efx->pci_dev)->dev, (void *)skb->data,
                                              (size_t )rx_buf->len, 2, 0);
  tmp___0 = dma_mapping_error(& (efx->pci_dev)->dev, rx_buf->dma_addr);
  tmp___1 = ldv__builtin_expect(tmp___0 != 0, 0L);
  if (tmp___1 != 0L) {
    dev_kfree_skb_any(skb);
    rx_buf->u.skb = 0;
    return (-5);
  } else {
  }
  rx_queue->added_count = rx_queue->added_count + 1;
  rx_queue->alloc_skb_count = rx_queue->alloc_skb_count + 1U;
  count = count + 1U;
  ldv_46379: ;
  if (count <= 7U) {
    goto ldv_46378;
  } else {
  }
  return (0);
}
}
static int efx_init_rx_buffers_page(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  struct efx_rx_buffer *rx_buf ;
  struct page *page ;
  struct efx_rx_page_state *state ;
  dma_addr_t dma_addr ;
  unsigned int index ;
  unsigned int count ;
  long tmp ;
  unsigned int tmp___0 ;
  int tmp___1 ;
  long tmp___2 ;
  void *tmp___3 ;
  {
  efx = rx_queue->efx;
  count = 0U;
  goto ldv_46393;
  ldv_46392:
  page = alloc_pages(16672U, efx->rx_buffer_order);
  tmp = ldv__builtin_expect((unsigned long )page == (unsigned long )((struct page *)0),
                         0L);
  if (tmp != 0L) {
    return (-12);
  } else {
  }
  tmp___0 = efx_rx_buf_size(efx);
  dma_addr = dma_map_page___0(& (efx->pci_dev)->dev, page, 0UL, (size_t )tmp___0,
                              2);
  tmp___1 = dma_mapping_error(& (efx->pci_dev)->dev, dma_addr);
  tmp___2 = ldv__builtin_expect(tmp___1 != 0, 0L);
  if (tmp___2 != 0L) {
    __free_pages(page, efx->rx_buffer_order);
    return (-5);
  } else {
  }
  tmp___3 = lowmem_page_address((struct page const *)page);
  state = (struct efx_rx_page_state *)tmp___3;
  state->refcnt = 0U;
  state->dma_addr = dma_addr;
  dma_addr = dma_addr + 64ULL;
  split:
  index = (unsigned int )rx_queue->added_count & rx_queue->ptr_mask;
  rx_buf = efx_rx_buffer(rx_queue, index);
  rx_buf->dma_addr = dma_addr;
  rx_buf->u.page = page;
  rx_buf->len = efx->rx_buffer_len;
  rx_buf->flags = 1U;
  rx_queue->added_count = rx_queue->added_count + 1;
  rx_queue->alloc_page_count = rx_queue->alloc_page_count + 1U;
  state->refcnt = state->refcnt + 1U;
  if (((count & 1U) == 0U ? 1 : 0) && efx->rx_buffer_len <= 1984U) {
    get_page(page);
    dma_addr = dma_addr + 2048ULL;
    count = count + 1U;
    goto split;
  } else {
  }
  count = count + 1U;
  ldv_46393: ;
  if (count <= 7U) {
    goto ldv_46392;
  } else {
  }
  return (0);
}
}
static void efx_unmap_rx_buffer(struct efx_nic *efx , struct efx_rx_buffer *rx_buf )
{
  struct efx_rx_page_state *state ;
  void *tmp ;
  unsigned int tmp___0 ;
  {
  if ((int )rx_buf->flags & 1 && (unsigned long )rx_buf->u.page != (unsigned long )((struct page *)0)) {
    tmp = lowmem_page_address((struct page const *)rx_buf->u.page);
    state = (struct efx_rx_page_state *)tmp;
    state->refcnt = state->refcnt - 1U;
    if (state->refcnt == 0U) {
      tmp___0 = efx_rx_buf_size(efx);
      dma_unmap_page___0(& (efx->pci_dev)->dev, state->dma_addr, (size_t )tmp___0,
                         2);
    } else {
    }
  } else
  if (((int )rx_buf->flags & 1) == 0 && (unsigned long )rx_buf->u.skb != (unsigned long )((struct sk_buff *)0)) {
    dma_unmap_single_attrs___0(& (efx->pci_dev)->dev, rx_buf->dma_addr, (size_t )rx_buf->len,
                               2, 0);
  } else {
  }
  return;
}
}
static void efx_free_rx_buffer(struct efx_nic *efx , struct efx_rx_buffer *rx_buf )
{
  {
  if ((int )rx_buf->flags & 1 && (unsigned long )rx_buf->u.page != (unsigned long )((struct page *)0)) {
    __free_pages(rx_buf->u.page, efx->rx_buffer_order);
    rx_buf->u.page = 0;
  } else
  if (((int )rx_buf->flags & 1) == 0 && (unsigned long )rx_buf->u.skb != (unsigned long )((struct sk_buff *)0)) {
    dev_kfree_skb_any(rx_buf->u.skb);
    rx_buf->u.skb = 0;
  } else {
  }
  return;
}
}
static void efx_fini_rx_buffer(struct efx_rx_queue *rx_queue , struct efx_rx_buffer *rx_buf )
{
  {
  efx_unmap_rx_buffer(rx_queue->efx, rx_buf);
  efx_free_rx_buffer(rx_queue->efx, rx_buf);
  return;
}
}
static void efx_resurrect_rx_buffer(struct efx_rx_queue *rx_queue , struct efx_rx_buffer *rx_buf )
{
  struct efx_rx_page_state *state ;
  void *tmp ;
  struct efx_rx_buffer *new_buf ;
  unsigned int fill_level ;
  unsigned int index ;
  long tmp___0 ;
  {
  tmp = lowmem_page_address((struct page const *)rx_buf->u.page);
  state = (struct efx_rx_page_state *)tmp;
  fill_level = (unsigned int )((rx_queue->added_count - rx_queue->removed_count) + 2);
  tmp___0 = ldv__builtin_expect(rx_queue->max_fill < fill_level, 0L);
  if (tmp___0 != 0L) {
    return;
  } else {
  }
  state->refcnt = state->refcnt + 1U;
  get_page(rx_buf->u.page);
  index = (unsigned int )rx_queue->added_count & rx_queue->ptr_mask;
  new_buf = efx_rx_buffer(rx_queue, index);
  new_buf->dma_addr = rx_buf->dma_addr ^ 2048ULL;
  new_buf->u.page = rx_buf->u.page;
  new_buf->len = rx_buf->len;
  new_buf->flags = 1U;
  rx_queue->added_count = rx_queue->added_count + 1;
  return;
}
}
static void efx_recycle_rx_buffer(struct efx_channel *channel , struct efx_rx_buffer *rx_buf )
{
  struct efx_nic *efx ;
  struct efx_rx_queue *rx_queue ;
  struct efx_rx_queue *tmp ;
  struct efx_rx_buffer *new_buf ;
  unsigned int index ;
  int tmp___0 ;
  size_t __len ;
  void *__ret ;
  {
  efx = channel->efx;
  tmp = efx_channel_get_rx_queue(channel);
  rx_queue = tmp;
  rx_buf->flags = (unsigned int )rx_buf->flags & 1U;
  if ((int )rx_buf->flags & 1 && efx->rx_buffer_len <= 1984U) {
    tmp___0 = page_count(rx_buf->u.page);
    if (tmp___0 == 1) {
      efx_resurrect_rx_buffer(rx_queue, rx_buf);
    } else {
    }
  } else {
  }
  index = (unsigned int )rx_queue->added_count & rx_queue->ptr_mask;
  new_buf = efx_rx_buffer(rx_queue, index);
  __len = 24UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)new_buf, (void const *)rx_buf, __len);
  } else {
    __ret = memcpy((void *)new_buf, (void const *)rx_buf, __len);
  }
  rx_buf->u.page = 0;
  rx_queue->added_count = rx_queue->added_count + 1;
  return;
}
}
void efx_fast_push_rx_descriptors(struct efx_rx_queue *rx_queue )
{
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  unsigned int fill_level ;
  int space ;
  int rc ;
  long tmp___0 ;
  long tmp___2 ;
  {
  tmp = efx_rx_queue_channel(rx_queue);
  channel = tmp;
  rc = 0;
  fill_level = (unsigned int )(rx_queue->added_count - rx_queue->removed_count);
  if (rx_queue->fast_fill_trigger <= fill_level) {
    goto out;
  } else {
  }
  tmp___0 = ldv__builtin_expect(rx_queue->min_fill > fill_level, 0L);
  if (tmp___0 != 0L) {
    if (fill_level != 0U) {
      rx_queue->min_fill = fill_level;
    } else {
    }
  } else {
  }
  space = (int )(rx_queue->max_fill - fill_level);
  ldv_46436: ;
  if (channel->rx_alloc_push_pages != 0) {
    rc = efx_init_rx_buffers_page(rx_queue);
  } else {
    rc = efx_init_rx_buffers_skb(rx_queue);
  }
  tmp___2 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___2 != 0L) {
    if (rx_queue->added_count == rx_queue->removed_count) {
      efx_schedule_slow_fill(rx_queue);
    } else {
    }
    goto out;
  } else {
  }
  space = space + -8;
  if (space > 7) {
    goto ldv_46436;
  } else {
  }
  out: ;
  if (rx_queue->notified_count != rx_queue->added_count) {
    efx_nic_notify_rx_desc(rx_queue);
  } else {
  }
  return;
}
}
void efx_rx_slow_fill(unsigned long context )
{
  struct efx_rx_queue *rx_queue ;
  {
  rx_queue = (struct efx_rx_queue *)context;
  efx_nic_generate_fill_event(rx_queue);
  rx_queue->slow_fill_count = rx_queue->slow_fill_count + 1U;
  return;
}
}
static void efx_rx_packet__check_len(struct efx_rx_queue *rx_queue , struct efx_rx_buffer *rx_buf ,
                                     int len , bool *leak_packet )
{
  struct efx_nic *efx ;
  unsigned int max_len ;
  long tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  struct efx_channel *tmp___5 ;
  {
  efx = rx_queue->efx;
  max_len = rx_buf->len - (unsigned int )(efx->type)->rx_buffer_padding;
  tmp = ldv__builtin_expect((unsigned int )len <= max_len, 1L);
  if (tmp != 0L) {
    return;
  } else {
  }
  rx_buf->flags = (u16 )((unsigned int )rx_buf->flags | 4U);
  if ((unsigned int )len > rx_buf->len) {
    tmp___4 = efx_nic_rev(efx);
    if (tmp___4 <= 1) {
      tmp___1 = net_ratelimit();
      if (tmp___1 != 0) {
        if ((efx->msg_enable & 64U) != 0U) {
          tmp___0 = efx_rx_queue_index(rx_queue);
          netdev_err((struct net_device const *)efx->net_dev, " RX queue %d seriously overlength RX event (0x%x > 0x%x+0x%x). Leaking\n",
                     tmp___0, len, max_len, (efx->type)->rx_buffer_padding);
        } else {
        }
      } else {
      }
      *leak_packet = ((int )rx_buf->flags & 1) == 0;
      efx_schedule_reset(efx, 7);
    } else {
      goto _L;
    }
  } else {
    _L:
    tmp___3 = net_ratelimit();
    if (tmp___3 != 0) {
      if ((efx->msg_enable & 64U) != 0U) {
        tmp___2 = efx_rx_queue_index(rx_queue);
        netdev_err((struct net_device const *)efx->net_dev, " RX queue %d overlength RX event (0x%x > 0x%x)\n",
                   tmp___2, len, max_len);
      } else {
      }
    } else {
    }
  }
  tmp___5 = efx_rx_queue_channel(rx_queue);
  tmp___5->n_rx_overlength = tmp___5->n_rx_overlength + 1U;
  return;
}
}
static void efx_rx_packet_gro(struct efx_channel *channel , struct efx_rx_buffer *rx_buf ,
                              u8 const *eh )
{
  struct napi_struct *napi ;
  gro_result_t gro_result ;
  struct efx_nic *efx ;
  struct page *page ;
  struct sk_buff *skb ;
  unsigned int tmp ;
  struct sk_buff *skb___0 ;
  {
  napi = & channel->napi_str;
  if ((int )rx_buf->flags & 1) {
    efx = channel->efx;
    page = rx_buf->u.page;
    rx_buf->u.page = 0;
    skb = napi_get_frags(napi);
    if ((unsigned long )skb == (unsigned long )((struct sk_buff *)0)) {
      put_page(page);
      return;
    } else {
    }
    if (((efx->net_dev)->features & 268435456ULL) != 0ULL) {
      skb->rxhash = efx_rx_buf_hash(eh);
    } else {
    }
    tmp = efx_rx_buf_offset(efx, rx_buf);
    skb_fill_page_desc(skb, 0, page, (int )tmp, (int )rx_buf->len);
    skb->len = rx_buf->len;
    skb->data_len = rx_buf->len;
    skb->truesize = skb->truesize + rx_buf->len;
    skb->ip_summed = ((int )rx_buf->flags & 2) != 0;
    skb_record_rx_queue(skb, (int )((u16 )channel->rx_queue.core_index));
    gro_result = napi_gro_frags(napi);
  } else {
    skb___0 = rx_buf->u.skb;
    rx_buf->u.skb = 0;
    skb___0->ip_summed = 1U;
    gro_result = napi_gro_receive(napi, skb___0);
  }
  if ((unsigned int )gro_result == 3U) {
    channel->rx_alloc_level = channel->rx_alloc_level + -2;
  } else
  if ((unsigned int )gro_result != 4U) {
    channel->rx_alloc_level = channel->rx_alloc_level + 1;
    channel->irq_mod_score = channel->irq_mod_score + 2U;
  } else {
  }
  return;
}
}
void efx_rx_packet(struct efx_rx_queue *rx_queue , unsigned int index , unsigned int len ,
                   u16 flags )
{
  struct efx_nic *efx ;
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  struct efx_rx_buffer *rx_buf ;
  bool leak_packet ;
  long tmp___1 ;
  long tmp___2 ;
  u8 *tmp___3 ;
  {
  efx = rx_queue->efx;
  tmp = efx_rx_queue_channel(rx_queue);
  channel = tmp;
  leak_packet = 0;
  rx_buf = efx_rx_buffer(rx_queue, index);
  rx_buf->flags = (u16 )((int )rx_buf->flags | (int )flags);
  rx_queue->removed_count = rx_queue->removed_count + 1;
  efx_rx_packet__check_len(rx_queue, rx_buf, (int )len, & leak_packet);
  tmp___2 = ldv__builtin_expect(((int )rx_buf->flags & 4) != 0, 0L);
  if (tmp___2 != 0L) {
    tmp___1 = ldv__builtin_expect((long )leak_packet, 0L);
    if (tmp___1 != 0L) {
      channel->n_skbuff_leaks = channel->n_skbuff_leaks + 1U;
    } else {
      efx_recycle_rx_buffer(channel, rx_buf);
    }
    rx_buf = 0;
    goto out;
  } else {
  }
  efx_unmap_rx_buffer(efx, rx_buf);
  tmp___3 = efx_rx_buf_eh(efx, rx_buf);
  __builtin_prefetch((void const *)tmp___3);
  rx_buf->len = len - (unsigned int )(efx->type)->rx_buffer_hash_size;
  out: ;
  if ((unsigned long )channel->rx_pkt != (unsigned long )((struct efx_rx_buffer *)0)) {
    __efx_rx_packet(channel, channel->rx_pkt);
  } else {
  }
  channel->rx_pkt = rx_buf;
  return;
}
}
static void efx_rx_deliver(struct efx_channel *channel , struct efx_rx_buffer *rx_buf )
{
  struct sk_buff *skb ;
  {
  skb = rx_buf->u.skb;
  rx_buf->u.skb = 0;
  skb_checksum_none_assert((struct sk_buff const *)skb);
  skb_record_rx_queue(skb, (int )((u16 )channel->rx_queue.core_index));
  if ((unsigned long )(channel->type)->receive_skb != (unsigned long )((void (* )(struct efx_channel * ,
                                                                                             struct sk_buff * ))0)) {
    (*((channel->type)->receive_skb))(channel, skb);
  } else {
    netif_receive_skb(skb);
  }
  channel->rx_alloc_level = channel->rx_alloc_level + -2;
  return;
}
}
void __efx_rx_packet(struct efx_channel *channel , struct efx_rx_buffer *rx_buf )
{
  struct efx_nic *efx ;
  u8 *eh ;
  u8 *tmp ;
  long tmp___0 ;
  struct sk_buff *skb ;
  unsigned char *tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  {
  efx = channel->efx;
  tmp = efx_rx_buf_eh(efx, rx_buf);
  eh = tmp;
  tmp___0 = ldv__builtin_expect((unsigned long )efx->loopback_selftest != (unsigned long )((void *)0),
                             0L);
  if (tmp___0 != 0L) {
    efx_loopback_rx_packet(efx, (char const *)eh, (int )rx_buf->len);
    efx_free_rx_buffer(efx, rx_buf);
    return;
  } else {
  }
  if (((int )rx_buf->flags & 1) == 0) {
    skb = rx_buf->u.skb;
    tmp___1 = skb_end_pointer((struct sk_buff const *)skb);
    __builtin_prefetch((void const *)tmp___1);
    skb_reserve(skb, (int )(efx->type)->rx_buffer_hash_size);
    skb_put(skb, rx_buf->len);
    if (((efx->net_dev)->features & 268435456ULL) != 0ULL) {
      skb->rxhash = efx_rx_buf_hash((u8 const *)eh);
    } else {
    }
    skb->protocol = eth_type_trans(skb, efx->net_dev);
    skb_record_rx_queue(skb, (int )((u16 )channel->rx_queue.core_index));
  } else {
  }
  tmp___2 = ldv__builtin_expect(((efx->net_dev)->features & 536870912ULL) == 0ULL, 0L);
  if (tmp___2 != 0L) {
    rx_buf->flags = (unsigned int )rx_buf->flags & 65533U;
  } else {
  }
  tmp___3 = ldv__builtin_expect(((int )rx_buf->flags & 3) != 0, 1L);
  if (tmp___3 != 0L && (unsigned long )(channel->type)->receive_skb == (unsigned long )((void (* )(struct efx_channel * ,
                                                                                                              struct sk_buff * ))0)) {
    efx_rx_packet_gro(channel, rx_buf, (u8 const *)eh);
  } else {
    efx_rx_deliver(channel, rx_buf);
  }
  return;
}
}
void efx_rx_strategy(struct efx_channel *channel )
{
  enum efx_rx_alloc_method method ;
  {
  method = (enum efx_rx_alloc_method )rx_alloc_method;
  if ((unsigned long )(channel->type)->receive_skb != (unsigned long )((void (* )(struct efx_channel * ,
                                                                                             struct sk_buff * ))0)) {
    channel->rx_alloc_push_pages = 0;
    return;
  } else {
  }
  if ((((channel->efx)->net_dev)->features & 16384ULL) == 0ULL) {
    method = 1;
  } else
  if ((unsigned int )method == 0U) {
    if (channel->rx_alloc_level < 0) {
      channel->rx_alloc_level = 0;
    } else
    if (channel->rx_alloc_level > 12288) {
      channel->rx_alloc_level = 12288;
    } else {
    }
    method = channel->rx_alloc_level > 8192 ? 2 : 1;
  } else {
  }
  channel->rx_alloc_push_pages = (unsigned int )method == 2U;
  return;
}
}
int efx_probe_rx_queue(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  unsigned int entries ;
  int rc ;
  unsigned long _max1 ;
  unsigned long tmp ;
  unsigned long _max2 ;
  struct _ddebug descriptor ;
  int tmp___0 ;
  long tmp___1 ;
  void *tmp___2 ;
  {
  efx = rx_queue->efx;
  tmp = __roundup_pow_of_two((unsigned long )efx->rxq_entries);
  _max1 = tmp;
  _max2 = 512UL;
  entries = (unsigned int )(_max1 > _max2 ? _max1 : _max2);
  rx_queue->ptr_mask = entries - 1U;
  if ((efx->msg_enable & 2U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_probe_rx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/rx.c.prepared";
    descriptor.format = "creating RX queue %d size %#x mask %#x\n";
    descriptor.lineno = 755U;
    descriptor.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      tmp___0 = efx_rx_queue_index(rx_queue);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "creating RX queue %d size %#x mask %#x\n", tmp___0, efx->rxq_entries,
                           rx_queue->ptr_mask);
    } else {
    }
  } else {
  }
  tmp___2 = kcalloc((size_t )entries, 24UL, 208U);
  rx_queue->buffer = (struct efx_rx_buffer *)tmp___2;
  if ((unsigned long )rx_queue->buffer == (unsigned long )((struct efx_rx_buffer *)0)) {
    return (-12);
  } else {
  }
  rc = efx_nic_probe_rx(rx_queue);
  if (rc != 0) {
    kfree((void const *)rx_queue->buffer);
    rx_queue->buffer = 0;
  } else {
  }
  return (rc);
}
}
void efx_init_rx_queue(struct efx_rx_queue *rx_queue )
{
  struct efx_nic *efx ;
  unsigned int max_fill ;
  unsigned int trigger ;
  unsigned int max_trigger ;
  struct _ddebug descriptor ;
  int tmp ;
  long tmp___0 ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  {
  efx = rx_queue->efx;
  if ((int )(rx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_init_rx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/rx.c.prepared";
    descriptor.format = "initialising RX queue %d\n";
    descriptor.lineno = 777U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      tmp = efx_rx_queue_index(rx_queue);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(rx_queue->efx)->net_dev,
                           "initialising RX queue %d\n", tmp);
    } else {
    }
  } else {
  }
  rx_queue->added_count = 0;
  rx_queue->notified_count = 0;
  rx_queue->removed_count = 0;
  rx_queue->min_fill = 4294967295U;
  max_fill = efx->rxq_entries - 2U;
  max_trigger = max_fill - 8U;
  if (rx_refill_threshold != 0U) {
    _min1 = rx_refill_threshold;
    _min2 = 100U;
    trigger = ((_min1 < _min2 ? _min1 : _min2) * max_fill) / 100U;
    if (trigger > max_trigger) {
      trigger = max_trigger;
    } else {
    }
  } else {
    trigger = max_trigger;
  }
  rx_queue->max_fill = max_fill;
  rx_queue->fast_fill_trigger = trigger;
  rx_queue->enabled = 1;
  efx_nic_init_rx(rx_queue);
  return;
}
}
void efx_fini_rx_queue(struct efx_rx_queue *rx_queue )
{
  int i ;
  struct efx_rx_buffer *rx_buf ;
  struct _ddebug descriptor ;
  int tmp ;
  long tmp___0 ;
  {
  if ((int )(rx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_fini_rx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/rx.c.prepared";
    descriptor.format = "shutting down RX queue %d\n";
    descriptor.lineno = 810U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      tmp = efx_rx_queue_index(rx_queue);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(rx_queue->efx)->net_dev,
                           "shutting down RX queue %d\n", tmp);
    } else {
    }
  } else {
  }
  rx_queue->enabled = 0;
  del_timer_sync(& rx_queue->slow_fill);
  efx_nic_fini_rx(rx_queue);
  if ((unsigned long )rx_queue->buffer != (unsigned long )((struct efx_rx_buffer *)0)) {
    i = 0;
    goto ldv_46521;
    ldv_46520:
    rx_buf = efx_rx_buffer(rx_queue, (unsigned int )i);
    efx_fini_rx_buffer(rx_queue, rx_buf);
    i = i + 1;
    ldv_46521: ;
    if ((unsigned int )i <= rx_queue->ptr_mask) {
      goto ldv_46520;
    } else {
    }
  } else {
  }
  return;
}
}
void efx_remove_rx_queue(struct efx_rx_queue *rx_queue )
{
  struct _ddebug descriptor ;
  int tmp ;
  long tmp___0 ;
  {
  if ((int )(rx_queue->efx)->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_remove_rx_queue";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/rx.c.prepared";
    descriptor.format = "destroying RX queue %d\n";
    descriptor.lineno = 830U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      tmp = efx_rx_queue_index(rx_queue);
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)(rx_queue->efx)->net_dev,
                           "destroying RX queue %d\n", tmp);
    } else {
    }
  } else {
  }
  efx_nic_remove_rx(rx_queue);
  kfree((void const *)rx_queue->buffer);
  rx_queue->buffer = 0;
  return;
}
}
void ldv_mutex_lock_145(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_146(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_147(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_148(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_149(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_150(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_151(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_162(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_160(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_163(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_165(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_159(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_161(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_164(struct mutex *ldv_func_arg1 ) ;
extern int _raw_spin_trylock_bh(raw_spinlock_t * ) ;
__inline static int spin_trylock_bh(spinlock_t *lock )
{
  int tmp ;
  {
  tmp = _raw_spin_trylock_bh(& lock->ldv_5961.rlock);
  return (tmp);
}
}
extern void *vzalloc(unsigned long ) ;
extern void vfree(void const * ) ;
__inline static int skb_network_offset(struct sk_buff const *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_network_header(skb);
  return ((int )((unsigned int )((long )tmp) - (unsigned int )((long )skb->data)));
}
}
__inline static u16 skb_get_rx_queue(struct sk_buff const *skb )
{
  {
  return ((unsigned int )((u16 )skb->queue_mapping) + 65535U);
}
}
extern bool rps_may_expire_flow(struct net_device * , u16 , u32 , u16 ) ;
__inline static bool ip_is_fragment(struct iphdr const *iph )
{
  {
  return (((int )iph->frag_off & 65343) != 0);
}
}
__inline static void efx_filter_init_rx(struct efx_filter_spec *spec , enum efx_filter_priority priority ,
                                        enum efx_filter_flags flags , unsigned int rxq_id )
{
  {
  spec->type = 15U;
  spec->priority = (unsigned char )priority;
  spec->flags = (unsigned int )((u8 )flags) | 8U;
  spec->dmaq_id = (u16 )rxq_id;
  return;
}
}
int efx_filter_set_ipv4_local(struct efx_filter_spec *spec , u8 proto , __be32 host ,
                              __be16 port ) ;
int efx_filter_get_ipv4_local(struct efx_filter_spec const *spec , u8 *proto , __be32 *host ,
                              __be16 *port ) ;
int efx_filter_set_ipv4_full(struct efx_filter_spec *spec , u8 proto , __be32 host ,
                             __be16 port , __be32 rhost , __be16 rport ) ;
int efx_filter_get_ipv4_full(struct efx_filter_spec const *spec , u8 *proto , __be32 *host ,
                             __be16 *port , __be32 *rhost , __be16 *rport ) ;
int efx_filter_set_eth_local(struct efx_filter_spec *spec , u16 vid , u8 const *addr ) ;
int efx_filter_get_eth_local(struct efx_filter_spec const *spec , u16 *vid , u8 *addr ) ;
int efx_filter_set_uc_def(struct efx_filter_spec *spec ) ;
int efx_filter_set_mc_def(struct efx_filter_spec *spec ) ;
s32 efx_filter_insert_filter(struct efx_nic *efx , struct efx_filter_spec *spec ,
                             bool replace ) ;
int efx_filter_remove_id_safe(struct efx_nic *efx , enum efx_filter_priority priority ,
                              u32 filter_id ) ;
int efx_filter_get_filter_safe(struct efx_nic *efx , enum efx_filter_priority priority ,
                               u32 filter_id , struct efx_filter_spec *spec_buf ) ;
u32 efx_filter_count_rx_used(struct efx_nic *efx , enum efx_filter_priority priority ) ;
u32 efx_filter_get_rx_id_limit(struct efx_nic *efx ) ;
s32 efx_filter_get_rx_ids(struct efx_nic *efx , enum efx_filter_priority priority ,
                          u32 *buf , u32 size ) ;
static u16 efx_filter_hash(u32 key )
{
  u16 tmp ;
  {
  tmp = (unsigned int )((u16 )(key >> 16)) ^ 8191U;
  tmp = (u16 )((((int )tmp >> 3) ^ (int )tmp) ^ ((int )tmp >> 6));
  tmp = (u16 )(((int )tmp >> 9) ^ (int )tmp);
  tmp = (int )((u16 )((int )((short )((int )tmp << 13)) ^ (int )((short )tmp))) ^ (int )((u16 )key);
  tmp = (u16 )((((int )tmp >> 3) ^ (int )tmp) ^ ((int )tmp >> 6));
  return ((u16 )(((int )tmp >> 9) ^ (int )tmp));
}
}
static u16 efx_filter_increment(u32 key )
{
  {
  return ((unsigned int )((u16 )key) * 2U - 1U);
}
}
static enum efx_filter_table_id efx_filter_spec_table_id(struct efx_filter_spec const *spec )
{
  {
  return ((enum efx_filter_table_id )(((int )spec->type >> 2) + (((int )spec->flags & 16) != 0 ? 2 : 0)));
}
}
static struct efx_filter_table *efx_filter_spec_table(struct efx_filter_state *state ,
                                                      struct efx_filter_spec const *spec )
{
  enum efx_filter_table_id tmp ;
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) == 15U) {
    return (0);
  } else {
    tmp = efx_filter_spec_table_id(spec);
    return ((struct efx_filter_table *)(& state->table) + (unsigned long )tmp);
  }
}
}
static void efx_filter_table_reset_search_depth(struct efx_filter_table *table )
{
  {
  memset((void *)(& table->search_depth), 0, 40UL);
  return;
}
}
static void efx_filter_push_rx_config(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  efx_oword_t filter_ctl ;
  {
  state = efx->filter_state;
  efx_reado(efx, & filter_ctl, 2064U);
  table = (struct efx_filter_table *)(& state->table);
  filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xffffffffffffff00ULL) | (unsigned long long )(table->search_depth[0] + 1U);
  filter_ctl.u64[1] = filter_ctl.u64[1];
  filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xffffffffffff00ffULL) | ((unsigned long long )(table->search_depth[1] + 3U) << 8);
  filter_ctl.u64[1] = filter_ctl.u64[1];
  filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xffffff00ffffffffULL) | ((unsigned long long )(table->search_depth[2] + 1U) << 32);
  filter_ctl.u64[1] = filter_ctl.u64[1];
  filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xffffffffff00ffffULL) | ((unsigned long long )(table->search_depth[3] + 3U) << 16);
  filter_ctl.u64[1] = filter_ctl.u64[1];
  table = (struct efx_filter_table *)(& state->table) + 1UL;
  if (table->size != 0U) {
    filter_ctl.u64[0] = filter_ctl.u64[0];
    filter_ctl.u64[1] = (filter_ctl.u64[1] & 0xffffffffc03fffffULL) | ((unsigned long long )(table->search_depth[4] + 1U) << 22);
    filter_ctl.u64[0] = filter_ctl.u64[0];
    filter_ctl.u64[1] = (filter_ctl.u64[1] & 0xffffffc03fffffffULL) | ((unsigned long long )(table->search_depth[5] + 3U) << 30);
  } else {
  }
  table = (struct efx_filter_table *)(& state->table) + 2UL;
  if (table->size != 0U) {
    filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xff8007ffffffffffULL) | ((unsigned long long )(table->spec)->dmaq_id << 43);
    filter_ctl.u64[1] = filter_ctl.u64[1];
    filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xfffffbffffffffffULL) | (((unsigned long long )(table->spec)->flags & 1ULL) << 42);
    filter_ctl.u64[1] = filter_ctl.u64[1];
    filter_ctl.u64[0] = (filter_ctl.u64[0] & 144115188075855871ULL) | ((unsigned long long )(table->spec + 1UL)->dmaq_id << 57);
    filter_ctl.u64[1] = (filter_ctl.u64[1] & 0xffffffffffffffe0ULL) | (unsigned long long )((int )(table->spec + 1UL)->dmaq_id >> 7);
    filter_ctl.u64[0] = (filter_ctl.u64[0] & 0xfeffffffffffffffULL) | (((unsigned long long )(table->spec + 1UL)->flags & 1ULL) << 56);
    filter_ctl.u64[1] = filter_ctl.u64[1];
  } else {
  }
  efx_writeo(efx, & filter_ctl, 2064U);
  return;
}
}
static void efx_filter_push_tx_limits(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  efx_oword_t tx_cfg ;
  {
  state = efx->filter_state;
  efx_reado(efx, & tx_cfg, 2640U);
  table = (struct efx_filter_table *)(& state->table) + 3UL;
  if (table->size != 0U) {
    tx_cfg.u64[0] = tx_cfg.u64[0];
    tx_cfg.u64[1] = (tx_cfg.u64[1] & 0xfffffe01ffffffffULL) | ((unsigned long long )(table->search_depth[4] + 1U) << 33);
    tx_cfg.u64[0] = tx_cfg.u64[0];
    tx_cfg.u64[1] = (tx_cfg.u64[1] & 0xfffe01ffffffffffULL) | ((unsigned long long )(table->search_depth[5] + 3U) << 41);
  } else {
  }
  efx_writeo(efx, & tx_cfg, 2640U);
  return;
}
}
__inline static void __efx_filter_set_ipv4(struct efx_filter_spec *spec , __be32 host1 ,
                                           __be16 port1 , __be32 host2 , __be16 port2 )
{
  __u32 tmp ;
  __u16 tmp___0 ;
  __u16 tmp___1 ;
  __u32 tmp___2 ;
  __u32 tmp___3 ;
  {
  tmp = __fswab32(host1);
  tmp___0 = __fswab16((int )port1);
  spec->data[0] = (tmp << 16) | (unsigned int )tmp___0;
  tmp___1 = __fswab16((int )port2);
  tmp___2 = __fswab32(host1);
  spec->data[1] = (unsigned int )((int )tmp___1 << 16) | (tmp___2 >> 16);
  tmp___3 = __fswab32(host2);
  spec->data[2] = tmp___3;
  return;
}
}
__inline static void __efx_filter_get_ipv4(struct efx_filter_spec const *spec ,
                                           __be32 *host1 , __be16 *port1 , __be32 *host2 ,
                                           __be16 *port2 )
{
  __u32 tmp ;
  __u16 tmp___0 ;
  __u32 tmp___1 ;
  __u16 tmp___2 ;
  {
  tmp = __fswab32((spec->data[0] >> 16) | (spec->data[1] << 16));
  *host1 = tmp;
  tmp___0 = __fswab16((int )((__u16 )spec->data[0]));
  *port1 = tmp___0;
  tmp___1 = __fswab32(spec->data[2]);
  *host2 = tmp___1;
  tmp___2 = __fswab16((int )((__u16 )(spec->data[1] >> 16)));
  *port2 = tmp___2;
  return;
}
}
int efx_filter_set_ipv4_local(struct efx_filter_spec *spec , u8 proto , __be32 host ,
                              __be16 port )
{
  __be32 host1 ;
  __be16 port1 ;
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) != 15U) {
    return (-93);
  } else {
  }
  if ((unsigned int )port == 0U) {
    return (-22);
  } else {
  }
  switch ((int )proto) {
  case 6:
  spec->type = 1U;
  goto ldv_46386;
  case 17:
  spec->type = 3U;
  goto ldv_46386;
  default: ;
  return (-93);
  }
  ldv_46386:
  host1 = 0U;
  if ((unsigned int )proto != 17U) {
    port1 = 0U;
  } else {
    port1 = port;
    port = 0U;
  }
  __efx_filter_set_ipv4(spec, host1, (int )port1, host, (int )port);
  return (0);
}
}
int efx_filter_get_ipv4_local(struct efx_filter_spec const *spec , u8 *proto , __be32 *host ,
                              __be16 *port )
{
  __be32 host1 ;
  __be16 port1 ;
  {
  switch ((int )spec->type) {
  case 1:
  *proto = 6U;
  __efx_filter_get_ipv4(spec, & host1, & port1, host, port);
  return (0);
  case 3:
  *proto = 17U;
  __efx_filter_get_ipv4(spec, & host1, port, host, & port1);
  return (0);
  default: ;
  return (-22);
  }
}
}
int efx_filter_set_ipv4_full(struct efx_filter_spec *spec , u8 proto , __be32 host ,
                             __be16 port , __be32 rhost , __be16 rport )
{
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) != 15U) {
    return (-93);
  } else {
  }
  if ((unsigned int )port == 0U || (unsigned int )rport == 0U) {
    return (-22);
  } else {
  }
  switch ((int )proto) {
  case 6:
  spec->type = 0U;
  goto ldv_46409;
  case 17:
  spec->type = 2U;
  goto ldv_46409;
  default: ;
  return (-93);
  }
  ldv_46409:
  __efx_filter_set_ipv4(spec, rhost, (int )rport, host, (int )port);
  return (0);
}
}
int efx_filter_get_ipv4_full(struct efx_filter_spec const *spec , u8 *proto , __be32 *host ,
                             __be16 *port , __be32 *rhost , __be16 *rport )
{
  {
  switch ((int )spec->type) {
  case 0:
  *proto = 6U;
  goto ldv_46421;
  case 2:
  *proto = 17U;
  goto ldv_46421;
  default: ;
  return (-22);
  }
  ldv_46421:
  __efx_filter_get_ipv4(spec, rhost, rport, host, port);
  return (0);
}
}
int efx_filter_set_eth_local(struct efx_filter_spec *spec , u16 vid , u8 const *addr )
{
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) != 15U) {
    return (-93);
  } else {
  }
  if ((unsigned int )vid == 65535U) {
    spec->type = 5U;
    spec->data[0] = 0U;
  } else {
    spec->type = 4U;
    spec->data[0] = (u32 )vid;
  }
  spec->data[1] = (u32 )(((((int )*(addr + 2UL) << 24) | ((int )*(addr + 3UL) << 16)) | ((int )*(addr + 4UL) << 8)) | (int )*(addr + 5UL));
  spec->data[2] = (u32 )(((int )*addr << 8) | (int )*(addr + 1UL));
  return (0);
}
}
int efx_filter_set_uc_def(struct efx_filter_spec *spec )
{
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) != 15U) {
    return (-22);
  } else {
  }
  spec->type = 8U;
  memset((void *)(& spec->data), 0, 12UL);
  return (0);
}
}
int efx_filter_set_mc_def(struct efx_filter_spec *spec )
{
  {
  if ((unsigned int )*((unsigned char *)spec + 0UL) != 15U) {
    return (-22);
  } else {
  }
  spec->type = 9U;
  memset((void *)(& spec->data), 0, 12UL);
  return (0);
}
}
static void efx_filter_reset_rx_def(struct efx_nic *efx , unsigned int filter_idx )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  struct efx_filter_spec *spec ;
  {
  state = efx->filter_state;
  table = (struct efx_filter_table *)(& state->table) + 2UL;
  spec = table->spec + (unsigned long )filter_idx;
  efx_filter_init_rx(spec, 1, 1, 0U);
  spec->type = (unsigned char )((unsigned int )((unsigned char )filter_idx) + 8U);
  *(table->used_bitmap) = *(table->used_bitmap) | (unsigned long )(1 << (int )filter_idx);
  return;
}
}
int efx_filter_get_eth_local(struct efx_filter_spec const *spec , u16 *vid , u8 *addr )
{
  {
  switch ((int )spec->type) {
  case 5:
  *vid = 65535U;
  goto ldv_46448;
  case 4:
  *vid = (u16 )spec->data[0];
  goto ldv_46448;
  default: ;
  return (-22);
  }
  ldv_46448:
  *addr = (u8 )(spec->data[2] >> 8);
  *(addr + 1UL) = (u8 )spec->data[2];
  *(addr + 2UL) = (u8 )(spec->data[1] >> 24);
  *(addr + 3UL) = (u8 )(spec->data[1] >> 16);
  *(addr + 4UL) = (u8 )(spec->data[1] >> 8);
  *(addr + 5UL) = (u8 )spec->data[1];
  return (0);
}
}
static u32 efx_filter_build(efx_oword_t *filter , struct efx_filter_spec *spec )
{
  u32 data3 ;
  enum efx_filter_table_id tmp ;
  bool is_udp ;
  bool is_wild ;
  bool is_wild___0 ;
  {
  tmp = efx_filter_spec_table_id((struct efx_filter_spec const *)spec);
  switch ((unsigned int )tmp) {
  case 0U:
  is_udp = (bool )((unsigned int )*((unsigned char *)spec + 0UL) == 2U || (unsigned int )*((unsigned char *)spec + 0UL) == 3U);
  filter->u64[0] = ((unsigned long long )spec->data[1] << 32) | (unsigned long long )spec->data[0];
  filter->u64[1] = ((((((unsigned long long )spec->flags & 1ULL) << 46) | (((int )spec->flags & 2) != 0 ? 35184372088832ULL : 0ULL)) | ((unsigned long long )is_udp << 44)) | ((unsigned long long )spec->dmaq_id << 32)) | (unsigned long long )spec->data[2];
  data3 = (u32 )is_udp;
  goto ldv_46458;
  case 2U: ;
  return ((u32 )((int )spec->type + -8));
  case 1U:
  is_wild = (unsigned int )*((unsigned char *)spec + 0UL) == 5U;
  filter->u64[0] = (((((unsigned long long )spec->dmaq_id << 61) | ((unsigned long long )is_wild << 60)) | ((unsigned long long )spec->data[2] << 44)) | ((unsigned long long )spec->data[1] << 12)) | (unsigned long long )spec->data[0];
  filter->u64[1] = ((((unsigned long long )spec->flags & 1ULL) << 11) | (((int )spec->flags & 2) != 0 ? 1024ULL : 0ULL)) | (unsigned long long )((int )spec->dmaq_id >> 3);
  data3 = (u32 )is_wild;
  goto ldv_46458;
  case 3U:
  is_wild___0 = (unsigned int )*((unsigned char *)spec + 0UL) == 5U;
  filter->u64[0] = (((((unsigned long long )spec->dmaq_id << 61) | ((unsigned long long )is_wild___0 << 60)) | ((unsigned long long )spec->data[2] << 44)) | ((unsigned long long )spec->data[1] << 12)) | (unsigned long long )spec->data[0];
  filter->u64[1] = (unsigned long long )((int )spec->dmaq_id >> 3);
  data3 = (u32 )((int )is_wild___0 | ((int )spec->dmaq_id << 1));
  goto ldv_46458;
  default:
  __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/filter.c.prepared"),
                       "i" (590), "i" (12UL));
  ldv_46465: ;
  goto ldv_46465;
  }
  ldv_46458: ;
  return (((spec->data[0] ^ spec->data[1]) ^ spec->data[2]) ^ data3);
}
}
static bool efx_filter_equal(struct efx_filter_spec const *left , struct efx_filter_spec const *right )
{
  int tmp ;
  {
  if ((int const )left->type != (int const )right->type) {
    return (0);
  } else {
    tmp = memcmp((void const *)(& left->data), (void const *)(& right->data),
                 12UL);
    if (tmp != 0) {
      return (0);
    } else {
    }
  }
  if (((int )left->flags & 16) != 0 && (int )((unsigned short )left->dmaq_id) != (int )((unsigned short )right->dmaq_id)) {
    return (0);
  } else {
  }
  return (1);
}
}
static int efx_filter_search(struct efx_filter_table *table , struct efx_filter_spec *spec ,
                             u32 key , bool for_insert , unsigned int *depth_required )
{
  unsigned int hash ;
  unsigned int incr ;
  unsigned int filter_idx ;
  unsigned int depth ;
  unsigned int depth_max ;
  u16 tmp ;
  u16 tmp___0 ;
  bool tmp___2 ;
  int tmp___3 ;
  int tmp___4 ;
  {
  tmp = efx_filter_hash(key);
  hash = (unsigned int )tmp;
  tmp___0 = efx_filter_increment(key);
  incr = (unsigned int )tmp___0;
  filter_idx = (table->size - 1U) & hash;
  depth = 1U;
  depth_max = (int )for_insert ? ((int )spec->priority <= 0 ? 5U : 200U) : table->search_depth[(int )spec->type];
  ldv_46482:
  tmp___4 = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp___4 != 0) {
    tmp___2 = efx_filter_equal((struct efx_filter_spec const *)spec, (struct efx_filter_spec const *)table->spec + (unsigned long )filter_idx);
    tmp___3 = (int )tmp___2;
  } else {
    tmp___3 = (int )for_insert;
  }
  if (tmp___3) {
    *depth_required = depth;
    return ((int )filter_idx);
  } else {
  }
  if (depth == depth_max) {
    return ((int )for_insert ? -16 : -2);
  } else {
  }
  filter_idx = (filter_idx + incr) & (table->size - 1U);
  depth = depth + 1U;
  goto ldv_46482;
}
}
static u8 const efx_filter_type_match_pri[10U] =
  { 0U, 1U, 0U, 1U,
        2U, 3U, (unsigned char)0, (unsigned char)0,
        4U, 4U};
static enum efx_filter_table_id const efx_filter_range_table[9U] =
  { 0, 0, 1, 1,
        2, 4, 4, 3,
        3};
__inline static u32 efx_filter_make_id(struct efx_filter_spec const *spec , unsigned int index )
{
  unsigned int range ;
  {
  range = (unsigned int )efx_filter_type_match_pri[(int )spec->type];
  if (((int )spec->flags & 8) == 0) {
    range = range + 5U;
  } else {
  }
  return ((range << 13) | index);
}
}
__inline static enum efx_filter_table_id efx_filter_id_table_id(u32 id )
{
  unsigned int range ;
  {
  range = id >> 13;
  if (range <= 8U) {
    return ((enum efx_filter_table_id )efx_filter_range_table[range]);
  } else {
    return (4);
  }
}
}
__inline static unsigned int efx_filter_id_index(u32 id )
{
  {
  return (id & 8191U);
}
}
__inline static u8 efx_filter_id_flags(u32 id )
{
  unsigned int range ;
  {
  range = id >> 13;
  if (range <= 4U) {
    return (8U);
  } else {
    return (16U);
  }
}
}
u32 efx_filter_get_rx_id_limit(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  unsigned int range ;
  enum efx_filter_table_id table_id ;
  unsigned int tmp ;
  {
  state = efx->filter_state;
  range = 4U;
  ldv_46509:
  table_id = efx_filter_range_table[range];
  if (state->table[(unsigned int )table_id].size != 0U) {
    return ((range << 13) | state->table[(unsigned int )table_id].size);
  } else {
  }
  tmp = range;
  range = range - 1U;
  if (tmp != 0U) {
    goto ldv_46509;
  } else {
  }
  return (0U);
}
}
s32 efx_filter_insert_filter(struct efx_nic *efx , struct efx_filter_spec *spec ,
                             bool replace )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  struct efx_filter_table *tmp ;
  struct efx_filter_spec *saved_spec ;
  efx_oword_t filter ;
  unsigned int filter_idx ;
  unsigned int depth ;
  u32 key ;
  int rc ;
  long tmp___0 ;
  int tmp___1 ;
  u32 tmp___2 ;
  {
  state = efx->filter_state;
  tmp = efx_filter_spec_table(state, (struct efx_filter_spec const *)spec);
  table = tmp;
  depth = 0U;
  if ((unsigned long )table == (unsigned long )((struct efx_filter_table *)0) || table->size == 0U) {
    return (-22);
  } else {
  }
  key = efx_filter_build(& filter, spec);
  spin_lock_bh(& state->lock);
  rc = efx_filter_search(table, spec, key, 1, & depth);
  if (rc < 0) {
    goto out;
  } else {
  }
  filter_idx = (unsigned int )rc;
  tmp___0 = ldv__builtin_expect(table->size <= filter_idx, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/filter.c.prepared"),
                         "i" (772), "i" (12UL));
    ldv_46527: ;
    goto ldv_46527;
  } else {
  }
  saved_spec = table->spec + (unsigned long )filter_idx;
  tmp___1 = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp___1 != 0) {
    if (! replace) {
      rc = -17;
      goto out;
    } else {
    }
    if ((int )spec->priority < (int )saved_spec->priority) {
      rc = -1;
      goto out;
    } else {
    }
  } else {
    __set_bit((int )filter_idx, (unsigned long volatile *)table->used_bitmap);
    table->used = table->used + 1U;
  }
  *saved_spec = *spec;
  if ((unsigned int )table->id == 2U) {
    efx_filter_push_rx_config(efx);
  } else {
    if (table->search_depth[(int )spec->type] < depth) {
      table->search_depth[(int )spec->type] = depth;
      if (((int )spec->flags & 16) != 0) {
        efx_filter_push_tx_limits(efx);
      } else {
        efx_filter_push_rx_config(efx);
      }
    } else {
    }
    efx_writeo(efx, & filter, table->offset + table->step * filter_idx);
  }
  tmp___2 = efx_filter_make_id((struct efx_filter_spec const *)spec, filter_idx);
  rc = (int )tmp___2;
  out:
  spin_unlock_bh(& state->lock);
  return (rc);
}
}
static void efx_filter_table_clear_entry(struct efx_nic *efx , struct efx_filter_table *table ,
                                         unsigned int filter_idx )
{
  efx_oword_t filter ;
  int tmp ;
  {
  if ((unsigned int )table->id == 2U) {
    efx_filter_reset_rx_def(efx, filter_idx);
    efx_filter_push_rx_config(efx);
  } else {
    tmp = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
    if (tmp != 0) {
      __clear_bit((int )filter_idx, (unsigned long volatile *)table->used_bitmap);
      table->used = table->used - 1U;
      memset((void *)table->spec + (unsigned long )filter_idx, 0, 16UL);
      efx_writeo(efx, & filter, table->offset + table->step * filter_idx);
    } else {
    }
  }
  return;
}
}
int efx_filter_remove_id_safe(struct efx_nic *efx , enum efx_filter_priority priority ,
                              u32 filter_id )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  struct efx_filter_table *table ;
  unsigned int filter_idx ;
  struct efx_filter_spec *spec ;
  u8 filter_flags ;
  int rc ;
  int tmp ;
  {
  state = efx->filter_state;
  table_id = efx_filter_id_table_id(filter_id);
  if ((unsigned int )table_id > 3U) {
    return (-2);
  } else {
  }
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  filter_idx = efx_filter_id_index(filter_id);
  if (table->size <= filter_idx) {
    return (-2);
  } else {
  }
  spec = table->spec + (unsigned long )filter_idx;
  filter_flags = efx_filter_id_flags(filter_id);
  spin_lock_bh(& state->lock);
  tmp = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp != 0 && (unsigned int )spec->priority == (unsigned int )priority) {
    efx_filter_table_clear_entry(efx, table, filter_idx);
    if (table->used == 0U) {
      efx_filter_table_reset_search_depth(table);
    } else {
    }
    rc = 0;
  } else {
    rc = -2;
  }
  spin_unlock_bh(& state->lock);
  return (rc);
}
}
int efx_filter_get_filter_safe(struct efx_nic *efx , enum efx_filter_priority priority ,
                               u32 filter_id , struct efx_filter_spec *spec_buf )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  struct efx_filter_table *table ;
  struct efx_filter_spec *spec ;
  unsigned int filter_idx ;
  u8 filter_flags ;
  int rc ;
  int tmp ;
  {
  state = efx->filter_state;
  table_id = efx_filter_id_table_id(filter_id);
  if ((unsigned int )table_id > 3U) {
    return (-2);
  } else {
  }
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  filter_idx = efx_filter_id_index(filter_id);
  if (table->size <= filter_idx) {
    return (-2);
  } else {
  }
  spec = table->spec + (unsigned long )filter_idx;
  filter_flags = efx_filter_id_flags(filter_id);
  spin_lock_bh(& state->lock);
  tmp = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp != 0 && (unsigned int )spec->priority == (unsigned int )priority) {
    *spec_buf = *spec;
    rc = 0;
  } else {
    rc = -2;
  }
  spin_unlock_bh(& state->lock);
  return (rc);
}
}
static void efx_filter_table_clear(struct efx_nic *efx , enum efx_filter_table_id table_id ,
                                   enum efx_filter_priority priority )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  unsigned int filter_idx ;
  {
  state = efx->filter_state;
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  spin_lock_bh(& state->lock);
  filter_idx = 0U;
  goto ldv_46569;
  ldv_46568: ;
  if ((unsigned int )(table->spec + (unsigned long )filter_idx)->priority <= (unsigned int )priority) {
    efx_filter_table_clear_entry(efx, table, filter_idx);
  } else {
  }
  filter_idx = filter_idx + 1U;
  ldv_46569: ;
  if (table->size > filter_idx) {
    goto ldv_46568;
  } else {
  }
  if (table->used == 0U) {
    efx_filter_table_reset_search_depth(table);
  } else {
  }
  spin_unlock_bh(& state->lock);
  return;
}
}
void efx_filter_clear_rx(struct efx_nic *efx , enum efx_filter_priority priority )
{
  {
  efx_filter_table_clear(efx, 0, priority);
  efx_filter_table_clear(efx, 1, priority);
  return;
}
}
u32 efx_filter_count_rx_used(struct efx_nic *efx , enum efx_filter_priority priority )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  struct efx_filter_table *table ;
  unsigned int filter_idx ;
  u32 count ;
  int tmp ;
  {
  state = efx->filter_state;
  count = 0U;
  spin_lock_bh(& state->lock);
  table_id = 0;
  goto ldv_46588;
  ldv_46587:
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  filter_idx = 0U;
  goto ldv_46585;
  ldv_46584:
  tmp = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp != 0 && (unsigned int )(table->spec + (unsigned long )filter_idx)->priority == (unsigned int )priority) {
    count = count + 1U;
  } else {
  }
  filter_idx = filter_idx + 1U;
  ldv_46585: ;
  if (table->size > filter_idx) {
    goto ldv_46584;
  } else {
  }
  table_id = (enum efx_filter_table_id )((unsigned int )table_id + 1U);
  ldv_46588: ;
  if ((unsigned int )table_id <= 2U) {
    goto ldv_46587;
  } else {
  }
  spin_unlock_bh(& state->lock);
  return (count);
}
}
s32 efx_filter_get_rx_ids(struct efx_nic *efx , enum efx_filter_priority priority ,
                          u32 *buf , u32 size )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  struct efx_filter_table *table ;
  unsigned int filter_idx ;
  s32 count ;
  s32 tmp ;
  int tmp___0 ;
  {
  state = efx->filter_state;
  count = 0;
  spin_lock_bh(& state->lock);
  table_id = 0;
  goto ldv_46606;
  ldv_46605:
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  filter_idx = 0U;
  goto ldv_46603;
  ldv_46602:
  tmp___0 = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp___0 != 0 && (unsigned int )(table->spec + (unsigned long )filter_idx)->priority == (unsigned int )priority) {
    if ((u32 )count == size) {
      count = -90;
      goto out;
    } else {
    }
    tmp = count;
    count = count + 1;
    *(buf + (unsigned long )tmp) = efx_filter_make_id((struct efx_filter_spec const *)table->spec + (unsigned long )filter_idx,
                                                      filter_idx);
  } else {
  }
  filter_idx = filter_idx + 1U;
  ldv_46603: ;
  if (table->size > filter_idx) {
    goto ldv_46602;
  } else {
  }
  table_id = (enum efx_filter_table_id )((unsigned int )table_id + 1U);
  ldv_46606: ;
  if ((unsigned int )table_id <= 2U) {
    goto ldv_46605;
  } else {
  }
  out:
  spin_unlock_bh(& state->lock);
  return (count);
}
}
void efx_restore_filters(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  struct efx_filter_table *table ;
  efx_oword_t filter ;
  unsigned int filter_idx ;
  int tmp ;
  {
  state = efx->filter_state;
  spin_lock_bh(& state->lock);
  table_id = 0;
  goto ldv_46622;
  ldv_46621:
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  if (table->step == 0U) {
    goto ldv_46616;
  } else {
  }
  filter_idx = 0U;
  goto ldv_46619;
  ldv_46618:
  tmp = variable_test_bit((int )filter_idx, (unsigned long const volatile *)table->used_bitmap);
  if (tmp == 0) {
    goto ldv_46617;
  } else {
  }
  efx_filter_build(& filter, table->spec + (unsigned long )filter_idx);
  efx_writeo(efx, & filter, table->offset + table->step * filter_idx);
  ldv_46617:
  filter_idx = filter_idx + 1U;
  ldv_46619: ;
  if (table->size > filter_idx) {
    goto ldv_46618;
  } else {
  }
  ldv_46616:
  table_id = (enum efx_filter_table_id )((unsigned int )table_id + 1U);
  ldv_46622: ;
  if ((unsigned int )table_id <= 3U) {
    goto ldv_46621;
  } else {
  }
  efx_filter_push_rx_config(efx);
  efx_filter_push_tx_limits(efx);
  spin_unlock_bh(& state->lock);
  return;
}
}
int efx_probe_filters(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  unsigned int table_id ;
  void *tmp ;
  struct lock_class_key __key ;
  void *tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  void *tmp___3 ;
  void *tmp___4 ;
  unsigned int i ;
  {
  tmp = kzalloc(408UL, 208U);
  state = (struct efx_filter_state *)tmp;
  if ((unsigned long )state == (unsigned long )((struct efx_filter_state *)0)) {
    return (-12);
  } else {
  }
  efx->filter_state = state;
  spinlock_check(& state->lock);
  __raw_spin_lock_init(& state->lock.ldv_5961.rlock, "&(&state->lock)->rlock", & __key);
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 > 1) {
    tmp___0 = kcalloc(8192UL, 4UL, 208U);
    state->rps_flow_id = (u32 *)tmp___0;
    if ((unsigned long )state->rps_flow_id == (unsigned long )((u32 *)0)) {
      goto fail;
    } else {
    }
    table = (struct efx_filter_table *)(& state->table);
    table->id = 0;
    table->offset = 15728640U;
    table->size = 8192U;
    table->step = 32U;
  } else {
  }
  tmp___2 = efx_nic_rev(efx);
  if (tmp___2 > 2) {
    table = (struct efx_filter_table *)(& state->table) + 1UL;
    table->id = 1;
    table->offset = 15728656U;
    table->size = 512U;
    table->step = 32U;
    table = (struct efx_filter_table *)(& state->table) + 2UL;
    table->id = 2;
    table->size = 2U;
    table = (struct efx_filter_table *)(& state->table) + 3UL;
    table->id = 3;
    table->offset = 16646144U;
    table->size = 512U;
    table->step = 16U;
  } else {
  }
  table_id = 0U;
  goto ldv_46634;
  ldv_46633:
  table = (struct efx_filter_table *)(& state->table) + (unsigned long )table_id;
  if (table->size == 0U) {
    goto ldv_46632;
  } else {
  }
  tmp___3 = kcalloc(((unsigned long )table->size + 63UL) / 64UL, 8UL, 208U);
  table->used_bitmap = (unsigned long *)tmp___3;
  if ((unsigned long )table->used_bitmap == (unsigned long )((unsigned long *)0)) {
    goto fail;
  } else {
  }
  tmp___4 = vzalloc((unsigned long )table->size * 16UL);
  table->spec = (struct efx_filter_spec *)tmp___4;
  if ((unsigned long )table->spec == (unsigned long )((struct efx_filter_spec *)0)) {
    goto fail;
  } else {
  }
  ldv_46632:
  table_id = table_id + 1U;
  ldv_46634: ;
  if (table_id <= 3U) {
    goto ldv_46633;
  } else {
  }
  if (state->table[2].size != 0U) {
    i = 0U;
    goto ldv_46638;
    ldv_46637:
    efx_filter_reset_rx_def(efx, i);
    i = i + 1U;
    ldv_46638: ;
    if (i <= 1U) {
      goto ldv_46637;
    } else {
    }
  } else {
  }
  efx_filter_push_rx_config(efx);
  return (0);
  fail:
  efx_remove_filters(efx);
  return (-12);
}
}
void efx_remove_filters(struct efx_nic *efx )
{
  struct efx_filter_state *state ;
  enum efx_filter_table_id table_id ;
  {
  state = efx->filter_state;
  table_id = 0;
  goto ldv_46646;
  ldv_46645:
  kfree((void const *)state->table[(unsigned int )table_id].used_bitmap);
  vfree((void const *)state->table[(unsigned int )table_id].spec);
  table_id = (enum efx_filter_table_id )((unsigned int )table_id + 1U);
  ldv_46646: ;
  if ((unsigned int )table_id <= 3U) {
    goto ldv_46645;
  } else {
  }
  kfree((void const *)state->rps_flow_id);
  kfree((void const *)state);
  return;
}
}
int efx_filter_rfs(struct net_device *net_dev , struct sk_buff const *skb , u16 rxq_index ,
                   u32 flow_id )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_channel *channel ;
  struct efx_filter_state *state ;
  struct efx_filter_spec spec ;
  struct iphdr const *ip ;
  __be16 const *ports ;
  int nhoff ;
  int rc ;
  bool tmp___0 ;
  u16 tmp___1 ;
  __u16 tmp___2 ;
  __u16 tmp___3 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  state = efx->filter_state;
  nhoff = skb_network_offset(skb);
  if ((unsigned int )((unsigned short )skb->protocol) != 8U) {
    return (-93);
  } else {
  }
  ip = (struct iphdr const *)skb->data + (unsigned long )nhoff;
  tmp___0 = ip_is_fragment(ip);
  if ((int )tmp___0) {
    return (-93);
  } else {
  }
  ports = (__be16 const *)(skb->data + ((unsigned long )nhoff + (unsigned long )((int )ip->ihl * 4)));
  efx_filter_init_rx(& spec, 0, 0, (unsigned int )rxq_index);
  rc = efx_filter_set_ipv4_full(& spec, (int )ip->protocol, ip->daddr, (int )*(ports + 1UL),
                                ip->saddr, (int )*ports);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_filter_insert_filter(efx, & spec, 1);
  if (rc < 0) {
    return (rc);
  } else {
  }
  *(state->rps_flow_id + (unsigned long )rc) = flow_id;
  tmp___1 = skb_get_rx_queue(skb);
  channel = efx_get_channel(efx, (unsigned int )tmp___1);
  channel->rfs_filters_added = channel->rfs_filters_added + 1U;
  if ((efx->msg_enable & 2048U) != 0U) {
    tmp___2 = __fswab16((int )*(ports + 1UL));
    tmp___3 = __fswab16((int )*ports);
    netdev_info((struct net_device const *)efx->net_dev, "steering %s %pI4:%u:%pI4:%u to queue %u [flow %u filter %d]\n",
                (unsigned int )((unsigned char )ip->protocol) == 6U ? (char *)"TCP" : (char *)"UDP",
                & ip->saddr, (int )tmp___3, & ip->daddr, (int )tmp___2, (int )rxq_index,
                flow_id, rc);
  } else {
  }
  return (rc);
}
}
bool __efx_filter_rfs_expire(struct efx_nic *efx , unsigned int quota )
{
  struct efx_filter_state *state ;
  struct efx_filter_table *table ;
  unsigned int mask ;
  unsigned int index ;
  unsigned int stop ;
  int tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  state = efx->filter_state;
  table = (struct efx_filter_table *)(& state->table);
  mask = table->size - 1U;
  tmp = spin_trylock_bh(& state->lock);
  if (tmp == 0) {
    return (0);
  } else {
  }
  index = state->rps_expire_index;
  stop = (index + quota) & mask;
  goto ldv_46672;
  ldv_46671:
  tmp___0 = variable_test_bit((int )index, (unsigned long const volatile *)table->used_bitmap);
  if (tmp___0 != 0 && (unsigned int )*((unsigned char *)(table->spec + (unsigned long )index) + 0UL) == 0U) {
    tmp___1 = rps_may_expire_flow(efx->net_dev, (int )(table->spec + (unsigned long )index)->dmaq_id,
                                  *(state->rps_flow_id + (unsigned long )index), (int )((u16 )index));
    if ((int )tmp___1) {
      if ((efx->msg_enable & 2048U) != 0U) {
        netdev_info((struct net_device const *)efx->net_dev, "expiring filter %d [flow %u]\n",
                    index, *(state->rps_flow_id + (unsigned long )index));
      } else {
      }
      efx_filter_table_clear_entry(efx, table, index);
    } else {
    }
  } else {
  }
  index = (index + 1U) & mask;
  ldv_46672: ;
  if (index != stop) {
    goto ldv_46671;
  } else {
  }
  state->rps_expire_index = stop;
  if (table->used == 0U) {
    efx_filter_table_reset_search_depth(table);
  } else {
  }
  spin_unlock_bh(& state->lock);
  return (1);
}
}
void ldv_mutex_lock_159(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_160(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_161(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_162(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_163(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_164(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_165(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_176(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_174(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_177(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_179(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_173(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_175(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_178(struct mutex *ldv_func_arg1 ) ;
__inline static bool efx_phy_mode_disabled(enum efx_phy_mode mode )
{
  {
  return (((unsigned int )mode & 4294967294U) != 0U);
}
}
int falcon_reset_xaui(struct efx_nic *efx ) ;
__inline static int efx_mdio_read(struct efx_nic *efx , int devad , int addr )
{
  int tmp ;
  {
  tmp = (*(efx->mdio.mdio_read))(efx->net_dev, efx->mdio.prtad, devad, (int )((u16 )addr));
  return (tmp);
}
}
__inline static bool efx_mdio_phyxgxs_lane_sync(struct efx_nic *efx )
{
  int i ;
  int lane_status ;
  bool sync ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  i = 0;
  goto ldv_41735;
  ldv_41734:
  lane_status = efx_mdio_read(efx, 4, 24);
  i = i + 1;
  ldv_41735: ;
  if (i <= 1) {
    goto ldv_41734;
  } else {
  }
  sync = (lane_status & 4096) != 0;
  if (! sync) {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_mdio_phyxgxs_lane_sync";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/inst/current/envs/linux-3.8-rc1/linux-3.8-rc1/drivers/net/ethernet/sfc/mdio_10g.h";
      descriptor.format = "XGXS lane status: %x\n";
      descriptor.lineno = 55U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "XGXS lane status: %x\n", lane_status);
      } else {
      }
    } else {
    }
  } else {
  }
  return (sync);
}
}
void falcon_setup_xaui(struct efx_nic *efx )
{
  efx_oword_t sdctl ;
  efx_oword_t txdrv ;
  {
  if (efx->phy_type == 0U) {
    return;
  } else {
  }
  efx_reado(efx, & sdctl, 4880U);
  sdctl.u64[0] = sdctl.u64[0] & 0xffffffffffff7fffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xffffffffffffbfffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xffffffffffffdfffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xffffffffffffefffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xfffffffffffff7ffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xfffffffffffffbffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xfffffffffffffdffULL;
  sdctl.u64[1] = sdctl.u64[1];
  sdctl.u64[0] = sdctl.u64[0] & 0xfffffffffffffeffULL;
  sdctl.u64[1] = sdctl.u64[1];
  efx_writeo(efx, & sdctl, 4880U);
  txdrv.u64[0] = 4008596821ULL;
  txdrv.u64[1] = 0ULL;
  efx_writeo(efx, & txdrv, 4896U);
  return;
}
}
int falcon_reset_xaui(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  int count ;
  int __ret_warn_on ;
  long tmp ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  __ret_warn_on = nic_data->stats_disable_count == 0U;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon_xmac.c.prepared",
                       153);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  reg.u64[0] = 1ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4864U);
  count = 0;
  goto ldv_41793;
  ldv_41792:
  efx_reado(efx, & reg, 4864U);
  if ((reg.u64[0] & 1ULL) == 0ULL && ((reg.u64[0] >> 16) & 1ULL) == 0ULL) {
    falcon_setup_xaui(efx);
    return (0);
  } else {
  }
  __const_udelay(42950UL);
  count = count + 1;
  ldv_41793: ;
  if (count <= 999) {
    goto ldv_41792;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for XAUI/XGXS reset\n");
  } else {
  }
  return (-110);
}
}
static void falcon_ack_status_intr(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  efx_oword_t reg ;
  int tmp ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  tmp = efx_nic_rev(efx);
  if (tmp != 2 || (66600958 >> (int )efx->loopback_mode) & 1) {
    return;
  } else {
  }
  if (! efx->link_state.up) {
    return;
  } else {
  }
  if ((int )nic_data->xmac_poll_required) {
    return;
  } else {
  }
  efx_reado(efx, & reg, 4848U);
  return;
}
}
static bool falcon_xgxs_link_ok(struct efx_nic *efx )
{
  efx_oword_t reg ;
  bool align_done ;
  bool link_ok ;
  int sync_status ;
  {
  link_ok = 0;
  efx_reado(efx, & reg, 4960U);
  align_done = ((reg.u64[0] >> 20) & 1ULL) != 0ULL;
  sync_status = (int )(reg.u64[0] >> 16) & 15;
  if ((int )align_done && sync_status == 15) {
    link_ok = 1;
  } else {
  }
  reg.u64[0] = reg.u64[0] | 61440ULL;
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = reg.u64[0] | 240ULL;
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = reg.u64[0] | 15ULL;
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 4960U);
  return (link_ok);
}
}
static bool falcon_xmac_link_ok(struct efx_nic *efx )
{
  bool tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  if ((unsigned int )efx->loopback_mode == 3U) {
    goto _L;
  } else {
    tmp = falcon_xgxs_link_ok(efx);
    if ((int )tmp) {
      _L:
      if ((efx->mdio.mmds & 16U) == 0U || (66600958 >> (int )efx->loopback_mode) & 1) {
        tmp___1 = 1;
      } else {
        tmp___0 = efx_mdio_phyxgxs_lane_sync(efx);
        if ((int )tmp___0) {
          tmp___1 = 1;
        } else {
          tmp___1 = 0;
        }
      }
    } else {
      tmp___1 = 0;
    }
  }
  return ((bool )tmp___1);
}
}
static void falcon_reconfigure_xmac_core(struct efx_nic *efx )
{
  unsigned int max_frame_len ;
  efx_oword_t reg ;
  bool rx_fc ;
  bool tx_fc ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  {
  rx_fc = ((int )efx->link_state.fc & 2) != 0;
  tx_fc = ((int )efx->link_state.fc & 1) != 0;
  reg.u64[0] = 3136ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4640U);
  reg.u64[0] = ((unsigned long long )tx_fc << 10) | 196902ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4656U);
  reg.u64[0] = ((unsigned long long )efx->promiscuous << 9) | 33556482ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4672U);
  max_frame_len = (((efx->net_dev)->mtu + 29U) & 4294967288U) + 16U;
  reg.u64[0] = (unsigned long long )max_frame_len;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4832U);
  reg.u64[0] = ((unsigned long long )max_frame_len << 16) | 2147483648ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4816U);
  reg.u64[0] = (unsigned long long )(! rx_fc) | 4294836224ULL;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 4720U);
  __len = 4UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& reg), (void const *)(efx->net_dev)->dev_addr, __len);
  } else {
    __ret = memcpy((void *)(& reg), (void const *)(efx->net_dev)->dev_addr,
                             __len);
  }
  efx_writeo(efx, & reg, 4608U);
  __len___0 = 2UL;
  if (__len___0 > 63UL) {
    __ret___0 = memcpy((void *)(& reg), (void const *)(efx->net_dev)->dev_addr + 4U,
                         __len___0);
  } else {
    __ret___0 = memcpy((void *)(& reg), (void const *)(efx->net_dev)->dev_addr + 4U,
                                 __len___0);
  }
  efx_writeo(efx, & reg, 4624U);
  return;
}
}
static void falcon_reconfigure_xgxs_core(struct efx_nic *efx )
{
  efx_oword_t reg ;
  bool xgxs_loopback ;
  bool xaui_loopback ;
  bool xgmii_loopback ;
  bool old_xgmii_loopback ;
  bool old_xgxs_loopback ;
  bool old_xaui_loopback ;
  bool reset_xgxs ;
  {
  xgxs_loopback = (unsigned int )efx->loopback_mode == 4U;
  xaui_loopback = (unsigned int )efx->loopback_mode == 5U;
  xgmii_loopback = (unsigned int )efx->loopback_mode == 3U;
  efx_reado(efx, & reg, 4960U);
  old_xgxs_loopback = ((reg.u64[0] >> 23) & 1ULL) != 0ULL;
  old_xgmii_loopback = ((reg.u64[0] >> 22) & 1ULL) != 0ULL;
  efx_reado(efx, & reg, 4880U);
  old_xaui_loopback = (reg.u64[0] & 1ULL) != 0ULL;
  reset_xgxs = (bool )(((int )xgxs_loopback != (int )old_xgxs_loopback || (int )xaui_loopback != (int )old_xaui_loopback) || (int )xgmii_loopback != (int )old_xgmii_loopback);
  if ((int )reset_xgxs) {
    falcon_reset_xaui(efx);
  } else {
  }
  efx_reado(efx, & reg, 4960U);
  reg.u64[0] = (reg.u64[0] & 0xffffffff00ffffffULL) | ((int )xgxs_loopback || (int )xaui_loopback ? 4278190080ULL : 0ULL);
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = (reg.u64[0] & 0xffffffffff7fffffULL) | ((unsigned long long )xgxs_loopback << 23);
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = (reg.u64[0] & 0xffffffffffbfffffULL) | ((unsigned long long )xgmii_loopback << 22);
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 4960U);
  efx_reado(efx, & reg, 4880U);
  reg.u64[0] = (reg.u64[0] & 0xfffffffffffffff7ULL) | ((unsigned long long )xaui_loopback << 3);
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = (reg.u64[0] & 0xfffffffffffffffbULL) | ((unsigned long long )xaui_loopback << 2);
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = (reg.u64[0] & 0xfffffffffffffffdULL) | ((unsigned long long )xaui_loopback << 1);
  reg.u64[1] = reg.u64[1];
  reg.u64[0] = (reg.u64[0] & 0xfffffffffffffffeULL) | (unsigned long long )xaui_loopback;
  reg.u64[1] = reg.u64[1];
  efx_writeo(efx, & reg, 4880U);
  return;
}
}
static bool falcon_xmac_link_ok_retry(struct efx_nic *efx , int tries )
{
  bool mac_up ;
  bool tmp ;
  bool tmp___0 ;
  struct _ddebug descriptor ;
  long tmp___1 ;
  {
  tmp = falcon_xmac_link_ok(efx);
  mac_up = tmp;
  if ((((u64 )(1 << (int )efx->loopback_mode) & efx->loopback_modes) & 67108864ULL) != 0ULL) {
    return (mac_up);
  } else {
    tmp___0 = efx_phy_mode_disabled(efx->phy_mode);
    if ((int )tmp___0) {
      return (mac_up);
    } else {
    }
  }
  falcon_stop_nic_stats(efx);
  goto ldv_41842;
  ldv_41841: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "falcon_xmac_link_ok_retry";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/falcon_xmac.c.prepared";
    descriptor.format = "bashing xaui\n";
    descriptor.lineno = 346U;
    descriptor.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "bashing xaui\n");
    } else {
    }
  } else {
  }
  falcon_reset_xaui(efx);
  __const_udelay(859000UL);
  mac_up = falcon_xmac_link_ok(efx);
  tries = tries - 1;
  ldv_41842: ;
  if (! mac_up && tries != 0) {
    goto ldv_41841;
  } else {
  }
  falcon_start_nic_stats(efx);
  return (mac_up);
}
}
bool falcon_xmac_check_fault(struct efx_nic *efx )
{
  bool tmp ;
  int tmp___0 ;
  {
  tmp = falcon_xmac_link_ok_retry(efx, 5);
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  return ((bool )tmp___0);
}
}
int falcon_reconfigure_xmac(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  bool tmp ;
  int tmp___0 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  falcon_reconfigure_xgxs_core(efx);
  falcon_reconfigure_xmac_core(efx);
  falcon_reconfigure_mac_wrapper(efx);
  tmp = falcon_xmac_link_ok_retry(efx, 5);
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  nic_data->xmac_poll_required = (bool )tmp___0;
  falcon_ack_status_intr(efx);
  return (0);
}
}
void falcon_update_stats_xmac(struct efx_nic *efx )
{
  struct efx_mac_stats *mac_stats ;
  {
  mac_stats = & efx->mac_stats;
  efx->mac_stats.rx_bytes = efx->mac_stats.rx_bytes + *((__le64 *)efx->stats_buffer.addr);
  efx->mac_stats.rx_good_bytes = efx->mac_stats.rx_good_bytes + *((__le64 *)efx->stats_buffer.addr + 8U);
  efx->mac_stats.rx_packets = efx->mac_stats.rx_packets + (u64 )*((__le32 *)efx->stats_buffer.addr + 16U);
  efx->mac_stats.rx_good = efx->mac_stats.rx_good + (u64 )*((__le32 *)efx->stats_buffer.addr + 20U);
  efx->mac_stats.rx_broadcast = efx->mac_stats.rx_broadcast + (u64 )*((__le32 *)efx->stats_buffer.addr + 24U);
  efx->mac_stats.rx_multicast = efx->mac_stats.rx_multicast + (u64 )*((__le32 *)efx->stats_buffer.addr + 28U);
  efx->mac_stats.rx_unicast = efx->mac_stats.rx_unicast + (u64 )*((__le32 *)efx->stats_buffer.addr + 32U);
  efx->mac_stats.rx_lt64 = efx->mac_stats.rx_lt64 + (u64 )*((__le32 *)efx->stats_buffer.addr + 36U);
  efx->mac_stats.rx_gtjumbo = efx->mac_stats.rx_gtjumbo + (u64 )*((__le32 *)efx->stats_buffer.addr + 40U);
  efx->mac_stats.rx_bad_gtjumbo = efx->mac_stats.rx_bad_gtjumbo + (u64 )*((__le32 *)efx->stats_buffer.addr + 44U);
  efx->mac_stats.rx_bad_lt64 = efx->mac_stats.rx_bad_lt64 + (u64 )*((__le32 *)efx->stats_buffer.addr + 48U);
  efx->mac_stats.rx_overflow = efx->mac_stats.rx_overflow + (u64 )*((__le32 *)efx->stats_buffer.addr + 52U);
  efx->mac_stats.rx_bad = efx->mac_stats.rx_bad + (u64 )*((__le32 *)efx->stats_buffer.addr + 56U);
  efx->mac_stats.rx_align_error = efx->mac_stats.rx_align_error + (u64 )*((__le32 *)efx->stats_buffer.addr + 60U);
  efx->mac_stats.rx_symbol_error = efx->mac_stats.rx_symbol_error + (u64 )*((__le32 *)efx->stats_buffer.addr + 64U);
  efx->mac_stats.rx_internal_error = efx->mac_stats.rx_internal_error + (u64 )*((__le32 *)efx->stats_buffer.addr + 68U);
  efx->mac_stats.rx_control = efx->mac_stats.rx_control + (u64 )*((__le32 *)efx->stats_buffer.addr + 72U);
  efx->mac_stats.rx_pause = efx->mac_stats.rx_pause + (u64 )*((__le32 *)efx->stats_buffer.addr + 76U);
  efx->mac_stats.rx_64 = efx->mac_stats.rx_64 + (u64 )*((__le32 *)efx->stats_buffer.addr + 80U);
  efx->mac_stats.rx_65_to_127 = efx->mac_stats.rx_65_to_127 + (u64 )*((__le32 *)efx->stats_buffer.addr + 84U);
  efx->mac_stats.rx_128_to_255 = efx->mac_stats.rx_128_to_255 + (u64 )*((__le32 *)efx->stats_buffer.addr + 88U);
  efx->mac_stats.rx_256_to_511 = efx->mac_stats.rx_256_to_511 + (u64 )*((__le32 *)efx->stats_buffer.addr + 92U);
  efx->mac_stats.rx_512_to_1023 = efx->mac_stats.rx_512_to_1023 + (u64 )*((__le32 *)efx->stats_buffer.addr + 96U);
  efx->mac_stats.rx_1024_to_15xx = efx->mac_stats.rx_1024_to_15xx + (u64 )*((__le32 *)efx->stats_buffer.addr + 100U);
  efx->mac_stats.rx_15xx_to_jumbo = efx->mac_stats.rx_15xx_to_jumbo + (u64 )*((__le32 *)efx->stats_buffer.addr + 104U);
  efx->mac_stats.rx_length_error = efx->mac_stats.rx_length_error + (u64 )*((__le32 *)efx->stats_buffer.addr + 108U);
  efx->mac_stats.tx_packets = efx->mac_stats.tx_packets + (u64 )*((__le32 *)efx->stats_buffer.addr + 128U);
  efx->mac_stats.tx_bytes = efx->mac_stats.tx_bytes + *((__le64 *)efx->stats_buffer.addr + 136U);
  efx->mac_stats.tx_multicast = efx->mac_stats.tx_multicast + (u64 )*((__le32 *)efx->stats_buffer.addr + 144U);
  efx->mac_stats.tx_broadcast = efx->mac_stats.tx_broadcast + (u64 )*((__le32 *)efx->stats_buffer.addr + 148U);
  efx->mac_stats.tx_unicast = efx->mac_stats.tx_unicast + (u64 )*((__le32 *)efx->stats_buffer.addr + 152U);
  efx->mac_stats.tx_control = efx->mac_stats.tx_control + (u64 )*((__le32 *)efx->stats_buffer.addr + 156U);
  efx->mac_stats.tx_pause = efx->mac_stats.tx_pause + (u64 )*((__le32 *)efx->stats_buffer.addr + 160U);
  efx->mac_stats.tx_64 = efx->mac_stats.tx_64 + (u64 )*((__le32 *)efx->stats_buffer.addr + 164U);
  efx->mac_stats.tx_65_to_127 = efx->mac_stats.tx_65_to_127 + (u64 )*((__le32 *)efx->stats_buffer.addr + 168U);
  efx->mac_stats.tx_128_to_255 = efx->mac_stats.tx_128_to_255 + (u64 )*((__le32 *)efx->stats_buffer.addr + 172U);
  efx->mac_stats.tx_256_to_511 = efx->mac_stats.tx_256_to_511 + (u64 )*((__le32 *)efx->stats_buffer.addr + 176U);
  efx->mac_stats.tx_512_to_1023 = efx->mac_stats.tx_512_to_1023 + (u64 )*((__le32 *)efx->stats_buffer.addr + 180U);
  efx->mac_stats.tx_1024_to_15xx = efx->mac_stats.tx_1024_to_15xx + (u64 )*((__le32 *)efx->stats_buffer.addr + 184U);
  efx->mac_stats.tx_15xx_to_jumbo = efx->mac_stats.tx_15xx_to_jumbo + (u64 )*((__le32 *)efx->stats_buffer.addr + 188U);
  efx->mac_stats.tx_lt64 = efx->mac_stats.tx_lt64 + (u64 )*((__le32 *)efx->stats_buffer.addr + 192U);
  efx->mac_stats.tx_gtjumbo = efx->mac_stats.tx_gtjumbo + (u64 )*((__le32 *)efx->stats_buffer.addr + 196U);
  efx->mac_stats.tx_non_tcpudp = efx->mac_stats.tx_non_tcpudp + (u64 )*((__le16 *)efx->stats_buffer.addr + 200U);
  efx->mac_stats.tx_mac_src_error = efx->mac_stats.tx_mac_src_error + (u64 )*((__le16 *)efx->stats_buffer.addr + 204U);
  efx->mac_stats.tx_ip_src_error = efx->mac_stats.tx_ip_src_error + (u64 )*((__le16 *)efx->stats_buffer.addr + 208U);
  efx_update_diff_stat(& mac_stats->tx_good_bytes, (mac_stats->tx_bytes - mac_stats->tx_bad_bytes) - mac_stats->tx_control * 64ULL);
  efx_update_diff_stat(& mac_stats->rx_bad_bytes, (mac_stats->rx_bytes - mac_stats->rx_good_bytes) - mac_stats->rx_control * 64ULL);
  return;
}
}
void falcon_poll_xmac(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  bool tmp ;
  int tmp___0 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if (! efx->link_state.up || ! nic_data->xmac_poll_required) {
    return;
  } else {
  }
  tmp = falcon_xmac_link_ok_retry(efx, 1);
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  nic_data->xmac_poll_required = (bool )tmp___0;
  falcon_ack_status_intr(efx);
  return;
}
}
void ldv_mutex_lock_173(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_174(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_175(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_176(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_177(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_178(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_179(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int ldv_mutex_is_locked_8(struct mutex *lock ) ;
int ldv_mutex_trylock_190(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_188(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_191(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_193(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_187(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_189(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_192(struct mutex *ldv_func_arg1 ) ;
int efx_mcdi_rpc(struct efx_nic *efx , unsigned int cmd , u8 const *inbuf , size_t inlen ,
                 u8 *outbuf , size_t outlen , size_t *outlen_actual ) ;
int efx_mcdi_set_mac(struct efx_nic *efx )
{
  u32 reject ;
  u32 fcntl ;
  u8 cmdbytes[24U] ;
  size_t __len ;
  void *__ret ;
  int tmp ;
  {
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& cmdbytes) + 8U, (void const *)(efx->net_dev)->dev_addr,
                     __len);
  } else {
    __ret = memcpy((void *)(& cmdbytes) + 8U, (void const *)(efx->net_dev)->dev_addr,
                             __len);
  }
  ((efx_dword_t *)(& cmdbytes))->u32[0] = (((efx->net_dev)->mtu + 29U) & 4294967288U) + 16U;
  ((efx_dword_t *)(& cmdbytes) + 4U)->u32[0] = 0U;
  reject = (int )efx->promiscuous ? 0U : 1U;
  ((efx_dword_t *)(& cmdbytes) + 16U)->u32[0] = reject;
  switch ((int )efx->wanted_fc) {
  case 3:
  fcntl = 2U;
  goto ldv_41221;
  case 2:
  fcntl = 1U;
  goto ldv_41221;
  default:
  fcntl = 0U;
  goto ldv_41221;
  }
  ldv_41221: ;
  if (((int )efx->wanted_fc & 4) != 0) {
    fcntl = 3U;
  } else {
  }
  if (efx->fc_disable != 0U) {
    fcntl = 0U;
  } else {
  }
  ((efx_dword_t *)(& cmdbytes) + 20U)->u32[0] = fcntl;
  tmp = efx_mcdi_rpc(efx, 44U, (u8 const *)(& cmdbytes), 24UL, 0, 0UL, 0);
  return (tmp);
}
}
bool efx_mcdi_mac_check_fault(struct efx_nic *efx )
{
  u8 outbuf[28U] ;
  size_t outlength ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 41U, 0, 0UL, (u8 *)(& outbuf), 28UL, & outlength);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n",
                 "efx_mcdi_mac_check_fault", rc);
    } else {
    }
    return (1);
  } else {
  }
  return (((efx_dword_t *)(& outbuf) + 24U)->u32[0] != 0U);
}
}
int efx_mcdi_mac_stats(struct efx_nic *efx , dma_addr_t dma_addr , u32 dma_len , int enable ,
                       int clear )
{
  u8 inbuf[16U] ;
  int rc ;
  efx_dword_t *cmd_ptr ;
  int period ;
  u32 addr_hi ;
  u32 addr_lo ;
  {
  period = enable != 0 ? 1000 : 0;
  addr_lo = (u32 )dma_addr;
  addr_hi = (u32 )(dma_addr >> 32);
  ((efx_dword_t *)(& inbuf))->u32[0] = addr_lo;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = addr_hi;
  cmd_ptr = (efx_dword_t *)(& inbuf) + 8U;
  cmd_ptr->u32[0] = ((((unsigned int )(enable != 0) | ((unsigned int )clear << 1)) | (enable != 0 ? 8U : 0U)) | ((unsigned int )period << 16)) | 36U;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = dma_len;
  rc = efx_mcdi_rpc(efx, 46U, (u8 const *)(& inbuf), 16UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: %s failed rc=%d\n",
               "efx_mcdi_mac_stats", enable != 0 ? (char *)"enable" : (char *)"disable",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_mac_reconfigure(struct efx_nic *efx )
{
  int rc ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  int tmp___1 ;
  {
  tmp = ldv_mutex_is_locked_8(& efx->mac_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi_mac.c.prepared",
                       207);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  rc = efx_mcdi_set_mac(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  tmp___1 = efx_mcdi_rpc(efx, 53U, (u8 const *)(& efx->multicast_hash.byte), 32UL,
                         0, 0UL, 0);
  return (tmp___1);
}
}
void ldv_mutex_lock_187(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_188(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_189(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_190(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_191(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_192(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_193(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_206(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_204(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_207(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_209(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_211(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_213(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_215(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_217(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_219(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_221(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_203(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_205(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_208(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_210(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_212(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_214(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_216(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_218(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_220(struct mutex *ldv_func_arg1 ) ;
extern bool schedule_delayed_work(struct delayed_work * , unsigned long ) ;
extern void kfree_skb(struct sk_buff * ) ;
extern void consume_skb(struct sk_buff * ) ;
extern struct sk_buff *__alloc_skb(unsigned int , gfp_t , int , int ) ;
__inline static struct sk_buff *alloc_skb(unsigned int size , gfp_t priority )
{
  struct sk_buff *tmp ;
  {
  tmp = __alloc_skb(size, priority, 0, -1);
  return (tmp);
}
}
__inline static struct sk_buff *skb_get(struct sk_buff *skb )
{
  {
  atomic_inc(& skb->users);
  return (skb);
}
}
__inline static int skb_shared(struct sk_buff const *skb )
{
  int tmp ;
  {
  tmp = atomic_read(& skb->users);
  return (tmp != 1);
}
}
__inline static void napi_disable___0(struct napi_struct *n )
{
  int tmp ;
  {
  set_bit(1U, (unsigned long volatile *)(& n->state));
  goto ldv_35551;
  ldv_35550:
  msleep(1U);
  ldv_35551:
  tmp = test_and_set_bit(0, (unsigned long volatile *)(& n->state));
  if (tmp != 0) {
    goto ldv_35550;
  } else {
  }
  clear_bit(1, (unsigned long volatile *)(& n->state));
  return;
}
}
__inline static void napi_enable___0(struct napi_struct *n )
{
  int tmp ;
  long tmp___0 ;
  {
  tmp = constant_test_bit(0U, (unsigned long const volatile *)(& n->state));
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"include/linux/netdevice.h"),
                         "i" (468), "i" (12UL));
    ldv_35556: ;
    goto ldv_35556;
  } else {
  }
  __asm__ volatile ("": : : "memory");
  clear_bit(0, (unsigned long volatile *)(& n->state));
  return;
}
}
__inline static void netif_tx_lock___1(struct net_device *dev )
{
  unsigned int i ;
  int cpu ;
  int pscr_ret__ ;
  void const *__vpp_verify ;
  int pfo_ret__ ;
  int pfo_ret_____0 ;
  int pfo_ret_____1 ;
  int pfo_ret_____2 ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  spin_lock(& dev->tx_global_lock);
  __vpp_verify = 0;
  switch (4UL) {
  case 1UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret__): "m" (cpu_number));
  goto ldv_36774;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_36774;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_36774;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret__): "m" (cpu_number));
  goto ldv_36774;
  default:
  __bad_percpu_size();
  }
  ldv_36774:
  pscr_ret__ = pfo_ret__;
  goto ldv_36780;
  case 2UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_36784;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_36784;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_36784;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____0): "m" (cpu_number));
  goto ldv_36784;
  default:
  __bad_percpu_size();
  }
  ldv_36784:
  pscr_ret__ = pfo_ret_____0;
  goto ldv_36780;
  case 4UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_36793;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_36793;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_36793;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____1): "m" (cpu_number));
  goto ldv_36793;
  default:
  __bad_percpu_size();
  }
  ldv_36793:
  pscr_ret__ = pfo_ret_____1;
  goto ldv_36780;
  case 8UL: ;
  switch (4UL) {
  case 1UL:
  __asm__ ("movb %%gs:%P1,%0": "=q" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_36802;
  case 2UL:
  __asm__ ("movw %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_36802;
  case 4UL:
  __asm__ ("movl %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_36802;
  case 8UL:
  __asm__ ("movq %%gs:%P1,%0": "=r" (pfo_ret_____2): "m" (cpu_number));
  goto ldv_36802;
  default:
  __bad_percpu_size();
  }
  ldv_36802:
  pscr_ret__ = pfo_ret_____2;
  goto ldv_36780;
  default:
  __bad_size_call_parameter();
  goto ldv_36780;
  }
  ldv_36780:
  cpu = pscr_ret__;
  i = 0U;
  goto ldv_36812;
  ldv_36811:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  __netif_tx_lock(txq, cpu);
  set_bit(2U, (unsigned long volatile *)(& txq->state));
  __netif_tx_unlock(txq);
  i = i + 1U;
  ldv_36812: ;
  if (dev->num_tx_queues > i) {
    goto ldv_36811;
  } else {
  }
  return;
}
}
__inline static void netif_tx_lock_bh(struct net_device *dev )
{
  {
  local_bh_disable();
  netif_tx_lock___1(dev);
  return;
}
}
__inline static void netif_tx_unlock___1(struct net_device *dev )
{
  unsigned int i ;
  struct netdev_queue *txq ;
  struct netdev_queue *tmp ;
  {
  i = 0U;
  goto ldv_36823;
  ldv_36822:
  tmp = netdev_get_tx_queue((struct net_device const *)dev, i);
  txq = tmp;
  clear_bit(2, (unsigned long volatile *)(& txq->state));
  netif_schedule_queue(txq);
  i = i + 1U;
  ldv_36823: ;
  if (dev->num_tx_queues > i) {
    goto ldv_36822;
  } else {
  }
  spin_unlock(& dev->tx_global_lock);
  return;
}
}
__inline static void netif_tx_unlock_bh(struct net_device *dev )
{
  {
  netif_tx_unlock___1(dev);
  local_bh_enable();
  return;
}
}
__inline static void efx_device_detach_sync___0(struct efx_nic *efx )
{
  struct net_device *dev ;
  {
  dev = efx->net_dev;
  netif_tx_lock___1(dev);
  netif_device_detach(dev);
  netif_tx_unlock___1(dev);
  return;
}
}
__inline static int efx_nic_event_test_irq_cpu(struct efx_channel *channel )
{
  {
  return ((int )*((int volatile *)(& channel->event_test_cpu)));
}
}
__inline static int efx_nic_irq_test_irq_cpu(struct efx_nic *efx )
{
  {
  return ((int )*((int volatile *)(& efx->last_irq_cpu)));
}
}
int efx_selftest(struct efx_nic *efx , struct efx_self_tests *tests , unsigned int flags ) ;
static unsigned char const payload_source[6U] = { 0U, 15U, 83U, 27U,
        27U, 27U};
static char const payload_msg[55U] =
  { 'H', 'e', 'l', 'l',
        'o', ' ', 'w', 'o',
        'r', 'l', 'd', '!',
        ' ', 'T', 'h', 'i',
        's', ' ', 'i', 's',
        ' ', 'a', 'n', ' ',
        'E', 'f', 'x', ' ',
        'l', 'o', 'o', 'p',
        'b', 'a', 'c', 'k',
        ' ', 't', 'e', 's',
        't', ' ', 'i', 'n',
        ' ', 'p', 'r', 'o',
        'g', 'r', 'e', 's',
        's', '!', '\000'};
static unsigned int const efx_interrupt_mode_max = 3U;
static char const * const efx_interrupt_mode_names[3U] = { "MSI-X", "MSI", "legacy"};
static int efx_test_phy_alive(struct efx_nic *efx , struct efx_self_tests *tests )
{
  int rc ;
  {
  rc = 0;
  if ((unsigned long )(efx->phy_op)->test_alive != (unsigned long )((int (* )(struct efx_nic * ))0)) {
    rc = (*((efx->phy_op)->test_alive))(efx);
    tests->phy_alive = rc != 0 ? -1 : 1;
  } else {
  }
  return (rc);
}
}
static int efx_test_nvram(struct efx_nic *efx , struct efx_self_tests *tests )
{
  int rc ;
  {
  rc = 0;
  if ((unsigned long )(efx->type)->test_nvram != (unsigned long )((int (* )(struct efx_nic * ))0)) {
    rc = (*((efx->type)->test_nvram))(efx);
    tests->nvram = rc != 0 ? -1 : 1;
  } else {
  }
  return (rc);
}
}
static int efx_test_interrupts(struct efx_nic *efx , struct efx_self_tests *tests )
{
  unsigned long timeout ;
  unsigned long wait ;
  int cpu ;
  struct _ddebug descriptor ;
  long tmp ;
  struct _ddebug descriptor___0 ;
  long tmp___0 ;
  struct _ddebug descriptor___1 ;
  long tmp___1 ;
  {
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_test_interrupts";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
    descriptor.format = "testing interrupts\n";
    descriptor.lineno = 223U;
    descriptor.flags = 0U;
    tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "testing interrupts\n");
    } else {
    }
  } else {
  }
  tests->interrupt = -1;
  efx_nic_irq_test_start(efx);
  timeout = (unsigned long )jiffies + 250UL;
  wait = 1UL;
  if ((int )efx->msg_enable & 1) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "efx_test_interrupts";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
    descriptor___0.format = "waiting for test interrupt\n";
    descriptor___0.lineno = 231U;
    descriptor___0.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "waiting for test interrupt\n");
    } else {
    }
  } else {
  }
  ldv_46161:
  schedule_timeout_uninterruptible((long )wait);
  cpu = efx_nic_irq_test_irq_cpu(efx);
  if (cpu >= 0) {
    goto success;
  } else {
  }
  wait = wait * 2UL;
  if ((long )jiffies - (long )timeout < 0L) {
    goto ldv_46161;
  } else {
  }
  if ((int )efx->msg_enable & 1) {
    netdev_err((struct net_device const *)efx->net_dev, "timed out waiting for interrupt\n");
  } else {
  }
  return (-110);
  success: ;
  if ((int )efx->msg_enable & 1) {
    descriptor___1.modname = "sfc";
    descriptor___1.function = "efx_test_interrupts";
    descriptor___1.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
    descriptor___1.format = "%s test interrupt seen on CPU%d\n";
    descriptor___1.lineno = 245U;
    descriptor___1.flags = 0U;
    tmp___1 = ldv__builtin_expect((long )descriptor___1.flags & 1L, 0L);
    if (tmp___1 != 0L) {
      __dynamic_netdev_dbg(& descriptor___1, (struct net_device const *)efx->net_dev,
                           "%s test interrupt seen on CPU%d\n", (unsigned int )efx->interrupt_mode < (unsigned int )efx_interrupt_mode_max ? efx_interrupt_mode_names[(unsigned int )efx->interrupt_mode] : (char const * )"(invalid)",
                           cpu);
    } else {
    }
  } else {
  }
  tests->interrupt = 1;
  return (0);
}
}
static int efx_test_eventq_irq(struct efx_nic *efx , struct efx_self_tests *tests )
{
  struct efx_channel *channel ;
  unsigned int read_ptr[32U] ;
  unsigned long napi_ran ;
  unsigned long dma_pend ;
  unsigned long int_pend ;
  unsigned long timeout ;
  unsigned long wait ;
  bool tmp ;
  int tmp___0 ;
  bool dma_seen ;
  int tmp___1 ;
  bool int_seen ;
  int tmp___2 ;
  struct _ddebug descriptor ;
  int tmp___3 ;
  long tmp___4 ;
  {
  napi_ran = 0UL;
  dma_pend = 0UL;
  int_pend = 0UL;
  channel = efx->channel[0];
  goto ldv_46176;
  ldv_46175:
  read_ptr[channel->channel] = channel->eventq_read_ptr;
  set_bit((unsigned int )channel->channel, (unsigned long volatile *)(& dma_pend));
  set_bit((unsigned int )channel->channel, (unsigned long volatile *)(& int_pend));
  efx_nic_event_test_start(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46176: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46175;
  } else {
  }
  timeout = (unsigned long )jiffies + 250UL;
  wait = 1UL;
  ldv_46187:
  schedule_timeout_uninterruptible((long )wait);
  channel = efx->channel[0];
  goto ldv_46179;
  ldv_46178:
  napi_disable___0(& channel->napi_str);
  if (channel->eventq_read_ptr != read_ptr[channel->channel]) {
    set_bit((unsigned int )channel->channel, (unsigned long volatile *)(& napi_ran));
    clear_bit(channel->channel, (unsigned long volatile *)(& dma_pend));
    clear_bit(channel->channel, (unsigned long volatile *)(& int_pend));
  } else {
    tmp = efx_nic_event_present(channel);
    if ((int )tmp) {
      clear_bit(channel->channel, (unsigned long volatile *)(& dma_pend));
    } else {
    }
    tmp___0 = efx_nic_event_test_irq_cpu(channel);
    if (tmp___0 >= 0) {
      clear_bit(channel->channel, (unsigned long volatile *)(& int_pend));
    } else {
    }
  }
  napi_enable___0(& channel->napi_str);
  efx_nic_eventq_read_ack(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46179: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46178;
  } else {
  }
  wait = wait * 2UL;
  if ((dma_pend != 0UL || int_pend != 0UL) && (long )jiffies - (long )timeout < 0L) {
    goto ldv_46187;
  } else {
  }
  channel = efx->channel[0];
  goto ldv_46194;
  ldv_46193:
  tmp___1 = variable_test_bit(channel->channel, (unsigned long const volatile *)(& dma_pend));
  dma_seen = tmp___1 == 0;
  tmp___2 = variable_test_bit(channel->channel, (unsigned long const volatile *)(& int_pend));
  int_seen = tmp___2 == 0;
  tests->eventq_dma[channel->channel] = (int )dma_seen ? 1 : -1;
  tests->eventq_int[channel->channel] = (int )int_seen ? 1 : -1;
  if ((int )dma_seen && (int )int_seen) {
    if ((int )efx->msg_enable & 1) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_test_eventq_irq";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
      descriptor.format = "channel %d event queue passed (with%s NAPI)\n";
      descriptor.lineno = 309U;
      descriptor.flags = 0U;
      tmp___4 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___4 != 0L) {
        tmp___3 = variable_test_bit(channel->channel, (unsigned long const volatile *)(& napi_ran));
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "channel %d event queue passed (with%s NAPI)\n", channel->channel,
                             tmp___3 != 0 ? (char *)"" : (char *)"out");
      } else {
      }
    } else {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "channel %d timed out waiting for event queue\n",
                   channel->channel);
      } else {
      }
      if ((int )int_seen) {
        if ((int )efx->msg_enable & 1) {
          netdev_err((struct net_device const *)efx->net_dev, "channel %d saw interrupt during event queue test\n",
                     channel->channel);
        } else {
        }
      } else {
      }
      if ((int )dma_seen) {
        if ((int )efx->msg_enable & 1) {
          netdev_err((struct net_device const *)efx->net_dev, "channel %d event was generated, but failed to trigger an interrupt\n",
                     channel->channel);
        } else {
        }
      } else {
      }
    }
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46194: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46193;
  } else {
  }
  return (dma_pend != 0UL || int_pend != 0UL ? -110 : 0);
}
}
static int efx_test_phy(struct efx_nic *efx , struct efx_self_tests *tests , unsigned int flags )
{
  int rc ;
  {
  if ((unsigned long )(efx->phy_op)->run_tests == (unsigned long )((int (* )(struct efx_nic * ,
                                                                                        int * ,
                                                                                        unsigned int ))0)) {
    return (0);
  } else {
  }
  ldv_mutex_lock_210(& efx->mac_lock);
  rc = (*((efx->phy_op)->run_tests))(efx, (int *)(& tests->phy_ext), flags);
  ldv_mutex_unlock_211(& efx->mac_lock);
  return (rc);
}
}
void efx_loopback_rx_packet(struct efx_nic *efx , char const *buf_ptr , int pkt_len )
{
  struct efx_loopback_state *state ;
  struct efx_loopback_payload *received ;
  struct efx_loopback_payload *payload ;
  long tmp ;
  int tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  __u16 tmp___3 ;
  __u16 tmp___4 ;
  {
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  tmp = ldv__builtin_expect((unsigned long )buf_ptr == (unsigned long )((char const *)0),
                         0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared"),
                         "i" (364), "i" (12UL));
    ldv_46210: ;
    goto ldv_46210;
  } else {
  }
  if ((unsigned long )state == (unsigned long )((struct efx_loopback_state *)0) || (int )state->flush) {
    return;
  } else {
  }
  payload = & state->payload;
  received = (struct efx_loopback_payload *)buf_ptr;
  received->ip.saddr = payload->ip.saddr;
  if ((int )state->offload_csum) {
    received->ip.check = payload->ip.check;
  } else {
  }
  if ((unsigned int )pkt_len <= 13U) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "saw runt RX packet (length %d) in %s loopback test\n",
                 pkt_len, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  tmp___0 = memcmp((void const *)(& received->header), (void const *)(& payload->header),
                   14UL);
  if (tmp___0 != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "saw non-loopback RX packet in %s loopback test\n",
                 (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  if (pkt_len != 108) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "saw incorrect RX packet length %d (wanted %d) in %s loopback test\n",
                 pkt_len, 108, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  tmp___1 = memcmp((void const *)(& received->ip), (void const *)(& payload->ip),
                   20UL);
  if (tmp___1 != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "saw corrupted IP header in %s loopback test\n",
                 (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  tmp___2 = memcmp((void const *)(& received->msg), (void const *)(& payload->msg),
                   64UL);
  if (tmp___2 != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "saw corrupted RX packet in %s loopback test\n",
                 (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  if ((int )received->iteration != (int )payload->iteration) {
    if ((int )efx->msg_enable & 1) {
      tmp___3 = __fswab16((int )payload->iteration);
      tmp___4 = __fswab16((int )received->iteration);
      netdev_err((struct net_device const *)efx->net_dev, "saw RX packet from iteration %d (wanted %d) in %s loopback test\n",
                 (int )tmp___4, (int )tmp___3, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto err;
  } else {
  }
  atomic_inc(& state->rx_good);
  return;
  err:
  atomic_inc(& state->rx_bad);
  return;
}
}
static void efx_iterate_state(struct efx_nic *efx )
{
  struct efx_loopback_state *state ;
  struct net_device *net_dev ;
  struct efx_loopback_payload *payload ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  __u16 tmp ;
  __u16 tmp___0 ;
  size_t __len___1 ;
  void *__ret___1 ;
  {
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  net_dev = efx->net_dev;
  payload = & state->payload;
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& payload->header.h_dest), (void const *)net_dev->dev_addr,
                     __len);
  } else {
    __ret = memcpy((void *)(& payload->header.h_dest), (void const *)net_dev->dev_addr,
                             __len);
  }
  __len___0 = 6UL;
  if (__len___0 > 63UL) {
    __ret___0 = memcpy((void *)(& payload->header.h_source), (void const *)(& payload_source),
                         __len___0);
  } else {
    __ret___0 = memcpy((void *)(& payload->header.h_source), (void const *)(& payload_source),
                                 __len___0);
  }
  payload->header.h_proto = 8U;
  payload->ip.daddr = 16777343U;
  payload->ip.ihl = 5U;
  payload->ip.check = 44510U;
  payload->ip.tot_len = 24064U;
  payload->ip.version = 4U;
  payload->ip.protocol = 17U;
  payload->udp.source = 0U;
  payload->udp.len = 18944U;
  payload->udp.check = 0U;
  tmp = __fswab16((int )payload->iteration);
  tmp___0 = __fswab16((int )((unsigned int )tmp + 1U));
  payload->iteration = tmp___0;
  __len___1 = 55UL;
  if (__len___1 > 63UL) {
    __ret___1 = memcpy((void *)(& payload->msg), (void const *)(& payload_msg),
                         __len___1);
  } else {
    __ret___1 = memcpy((void *)(& payload->msg), (void const *)(& payload_msg),
                                 __len___1);
  }
  atomic_set(& state->rx_good, 0);
  atomic_set(& state->rx_bad, 0);
  __asm__ volatile ("": : : "memory");
  return;
}
}
static int efx_begin_loopback(struct efx_tx_queue *tx_queue )
{
  struct efx_nic *efx ;
  struct efx_loopback_state *state ;
  struct efx_loopback_payload *payload ;
  struct sk_buff *skb ;
  int i ;
  netdev_tx_t rc ;
  unsigned char *tmp ;
  size_t __len ;
  void *__ret ;
  __u32 tmp___0 ;
  {
  efx = tx_queue->efx;
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  i = 0;
  goto ldv_46241;
  ldv_46240:
  skb = alloc_skb(108U, 208U);
  if ((unsigned long )skb == (unsigned long )((struct sk_buff *)0)) {
    return (-12);
  } else {
  }
  *(state->skbs + (unsigned long )i) = skb;
  skb_get(skb);
  tmp = skb_put(skb, 108U);
  payload = (struct efx_loopback_payload *)tmp;
  __len = 108UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)payload, (void const *)(& state->payload), __len);
  } else {
    __ret = memcpy((void *)payload, (void const *)(& state->payload),
                             __len);
  }
  tmp___0 = __fswab32((__u32 )((i << 2) | 2130706433));
  payload->ip.saddr = tmp___0;
  __asm__ volatile ("": : : "memory");
  netif_tx_lock_bh(efx->net_dev);
  rc = efx_enqueue_skb(tx_queue, skb);
  netif_tx_unlock_bh(efx->net_dev);
  if ((int )rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "TX queue %d could not transmit packet %d of %d in %s loopback test\n",
                 tx_queue->queue, i + 1, state->packet_count, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    kfree_skb(skb);
    return (-32);
  } else {
  }
  i = i + 1;
  ldv_46241: ;
  if (state->packet_count > i) {
    goto ldv_46240;
  } else {
  }
  return (0);
}
}
static int efx_poll_loopback(struct efx_nic *efx )
{
  struct efx_loopback_state *state ;
  struct efx_channel *channel ;
  int tmp ;
  {
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  channel = efx->channel[0];
  goto ldv_46249;
  ldv_46248: ;
  if ((int )channel->work_pending) {
    efx_process_channel_now(channel);
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46249: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46248;
  } else {
  }
  tmp = atomic_read((atomic_t const *)(& state->rx_good));
  return (tmp == state->packet_count);
}
}
static int efx_end_loopback(struct efx_tx_queue *tx_queue , struct efx_loopback_self_tests *lb_tests )
{
  struct efx_nic *efx ;
  struct efx_loopback_state *state ;
  struct sk_buff *skb ;
  int tx_done ;
  int rx_good ;
  int rx_bad ;
  int i ;
  int rc ;
  int tmp ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  efx = tx_queue->efx;
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  tx_done = 0;
  rc = 0;
  netif_tx_lock_bh(efx->net_dev);
  i = 0;
  goto ldv_46264;
  ldv_46263:
  skb = *(state->skbs + (unsigned long )i);
  if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
    tmp = skb_shared((struct sk_buff const *)skb);
    if (tmp == 0) {
      tx_done = tx_done + 1;
    } else {
    }
  } else {
  }
  consume_skb(skb);
  i = i + 1;
  ldv_46264: ;
  if (state->packet_count > i) {
    goto ldv_46263;
  } else {
  }
  netif_tx_unlock_bh(efx->net_dev);
  rx_good = atomic_read((atomic_t const *)(& state->rx_good));
  rx_bad = atomic_read((atomic_t const *)(& state->rx_bad));
  if (state->packet_count != tx_done) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "TX queue %d saw only %d out of an expected %d TX completion events in %s loopback test\n",
                 tx_queue->queue, tx_done, state->packet_count, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    rc = -110;
  } else {
  }
  if (state->packet_count != rx_good) {
    if ((int )efx->msg_enable & 1) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_end_loopback";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
      descriptor.format = "TX queue %d saw only %d out of an expected %d received packets in %s loopback test\n";
      descriptor.lineno = 592U;
      descriptor.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "TX queue %d saw only %d out of an expected %d received packets in %s loopback test\n",
                             tx_queue->queue, rx_good, state->packet_count, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
      } else {
      }
    } else {
    }
    rc = -110;
  } else {
  }
  lb_tests->tx_sent[tx_queue->queue] = lb_tests->tx_sent[tx_queue->queue] + state->packet_count;
  lb_tests->tx_done[tx_queue->queue] = lb_tests->tx_done[tx_queue->queue] + tx_done;
  lb_tests->rx_good = lb_tests->rx_good + rx_good;
  lb_tests->rx_bad = lb_tests->rx_bad + rx_bad;
  return (rc);
}
}
static int efx_test_loopback(struct efx_tx_queue *tx_queue , struct efx_loopback_self_tests *lb_tests )
{
  struct efx_nic *efx ;
  struct efx_loopback_state *state ;
  int i ;
  int begin_rc ;
  int end_rc ;
  int _min1 ;
  int _min2 ;
  void *tmp ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  int tmp___1 ;
  struct _ddebug descriptor___0 ;
  long tmp___2 ;
  {
  efx = tx_queue->efx;
  state = (struct efx_loopback_state *)efx->loopback_selftest;
  i = 0;
  goto ldv_46283;
  ldv_46282:
  state->packet_count = (int )(efx->txq_entries / 3U);
  _min1 = 1 << (i << 2);
  _min2 = state->packet_count;
  state->packet_count = _min1 < _min2 ? _min1 : _min2;
  tmp = kcalloc((size_t )state->packet_count, 8UL, 208U);
  state->skbs = (struct sk_buff **)tmp;
  if ((unsigned long )state->skbs == (unsigned long )((struct sk_buff **)0)) {
    return (-12);
  } else {
  }
  state->flush = 0;
  if ((int )efx->msg_enable & 1) {
    descriptor.modname = "sfc";
    descriptor.function = "efx_test_loopback";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
    descriptor.format = "TX queue %d testing %s loopback with %d packets\n";
    descriptor.lineno = 627U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "TX queue %d testing %s loopback with %d packets\n", tx_queue->queue,
                           (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)",
                           state->packet_count);
    } else {
    }
  } else {
  }
  efx_iterate_state(efx);
  begin_rc = efx_begin_loopback(tx_queue);
  msleep(1U);
  tmp___1 = efx_poll_loopback(efx);
  if (tmp___1 == 0) {
    msleep(1000U);
    efx_poll_loopback(efx);
  } else {
  }
  end_rc = efx_end_loopback(tx_queue, lb_tests);
  kfree((void const *)state->skbs);
  if (begin_rc != 0 || end_rc != 0) {
    schedule_timeout_uninterruptible(25L);
    return (begin_rc != 0 ? begin_rc : end_rc);
  } else {
  }
  i = i + 1;
  ldv_46283: ;
  if (i <= 2) {
    goto ldv_46282;
  } else {
  }
  if ((int )efx->msg_enable & 1) {
    descriptor___0.modname = "sfc";
    descriptor___0.function = "efx_test_loopback";
    descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
    descriptor___0.format = "TX queue %d passed %s loopback test with a burst length of %d packets\n";
    descriptor___0.lineno = 654U;
    descriptor___0.flags = 0U;
    tmp___2 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
    if (tmp___2 != 0L) {
      __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                           "TX queue %d passed %s loopback test with a burst length of %d packets\n",
                           tx_queue->queue, (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)",
                           state->packet_count);
    } else {
    }
  } else {
  }
  return (0);
}
}
static int efx_wait_for_link(struct efx_nic *efx )
{
  struct efx_link_state *link_state ;
  int count ;
  int link_up_count ;
  bool link_up ;
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  bool tmp___0 ;
  int tmp___1 ;
  {
  link_state = & efx->link_state;
  link_up_count = 0;
  count = 0;
  goto ldv_46295;
  ldv_46294:
  schedule_timeout_uninterruptible(25L);
  if ((unsigned long )(efx->type)->monitor != (unsigned long )((void (* )(struct efx_nic * ))0)) {
    ldv_mutex_lock_212(& efx->mac_lock);
    (*((efx->type)->monitor))(efx);
    ldv_mutex_unlock_213(& efx->mac_lock);
  } else {
    tmp = efx_get_channel(efx, 0U);
    channel = tmp;
    if ((int )channel->work_pending) {
      efx_process_channel_now(channel);
    } else {
    }
  }
  ldv_mutex_lock_214(& efx->mac_lock);
  link_up = link_state->up;
  if ((int )link_up) {
    tmp___0 = (*((efx->type)->check_mac_fault))(efx);
    if ((int )tmp___0 != 0) {
      tmp___1 = 0;
    } else {
      tmp___1 = 1;
    }
    link_up = (bool )tmp___1;
  } else {
  }
  ldv_mutex_unlock_215(& efx->mac_lock);
  if ((int )link_up) {
    link_up_count = link_up_count + 1;
    if (link_up_count == 2) {
      return (0);
    } else {
      link_up_count = 0;
    }
  } else {
  }
  count = count + 1;
  ldv_46295: ;
  if (count <= 39) {
    goto ldv_46294;
  } else {
  }
  return (-110);
}
}
static int efx_test_loopbacks(struct efx_nic *efx , struct efx_self_tests *tests ,
                              unsigned int loopback_modes )
{
  enum efx_loopback_mode mode ;
  struct efx_loopback_state *state ;
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  struct efx_tx_queue *tx_queue ;
  int rc ;
  void *tmp___0 ;
  long tmp___1 ;
  bool tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  {
  tmp = efx_get_channel(efx, efx->tx_channel_offset);
  channel = tmp;
  rc = 0;
  tmp___0 = kzalloc(136UL, 208U);
  state = (struct efx_loopback_state *)tmp___0;
  if ((unsigned long )state == (unsigned long )((struct efx_loopback_state *)0)) {
    return (-12);
  } else {
  }
  tmp___1 = ldv__builtin_expect((unsigned long )efx->loopback_selftest != (unsigned long )((void *)0),
                             0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared"),
                         "i" (715), "i" (12UL));
    ldv_46307: ;
    goto ldv_46307;
  } else {
  }
  state->flush = 1;
  efx->loopback_selftest = (void *)state;
  mode = 0;
  goto ldv_46314;
  ldv_46313: ;
  if (((unsigned int )(1 << (int )mode) & loopback_modes) == 0U) {
    goto ldv_46308;
  } else {
  }
  state->flush = 1;
  ldv_mutex_lock_216(& efx->mac_lock);
  efx->loopback_mode = mode;
  rc = __efx_reconfigure_port(efx);
  ldv_mutex_unlock_217(& efx->mac_lock);
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "unable to move into %s loopback\n",
                 (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto out;
  } else {
  }
  rc = efx_wait_for_link(efx);
  if (rc != 0) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "loopback %s never came up\n",
                 (unsigned int )efx->loopback_mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )efx->loopback_mode] : (char const * )"(invalid)");
    } else {
    }
    goto out;
  } else {
  }
  tmp___3 = efx_channel_has_tx_queues(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_46311;
    ldv_46310:
    state->offload_csum = (tx_queue->queue & 1U) != 0U;
    rc = efx_test_loopback(tx_queue, (struct efx_loopback_self_tests *)(& tests->loopback) + (unsigned long )mode);
    if (rc != 0) {
      goto out;
    } else {
    }
    tx_queue = tx_queue + 1;
    ldv_46311: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___2 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___2) {
        goto ldv_46310;
      } else {
        goto ldv_46312;
      }
    } else {
    }
    ldv_46312: ;
  }
  ldv_46308:
  mode = (enum efx_loopback_mode )((unsigned int )mode + 1U);
  ldv_46314: ;
  if ((unsigned int )mode <= 17U) {
    goto ldv_46313;
  } else {
  }
  out:
  state->flush = 1;
  efx->loopback_selftest = 0;
  __asm__ volatile ("sfence": : : "memory");
  kfree((void const *)state);
  return (rc);
}
}
int efx_selftest(struct efx_nic *efx , struct efx_self_tests *tests , unsigned int flags )
{
  enum efx_loopback_mode loopback_mode ;
  int phy_mode ;
  int rc_test ;
  int rc_reset ;
  int rc ;
  int tmp ;
  {
  loopback_mode = efx->loopback_mode;
  phy_mode = (int )efx->phy_mode;
  rc_test = 0;
  efx_selftest_async_cancel(efx);
  rc = efx_test_phy_alive(efx, tests);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  rc = efx_test_nvram(efx, tests);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  rc = efx_test_interrupts(efx, tests);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  rc = efx_test_eventq_irq(efx, tests);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  if (rc_test != 0) {
    return (rc_test);
  } else {
  }
  if ((flags & 1U) == 0U) {
    tmp = efx_test_phy(efx, tests, flags);
    return (tmp);
  } else {
  }
  efx_device_detach_sync___0(efx);
  if ((unsigned long )(efx->type)->test_chip != (unsigned long )((int (* )(struct efx_nic * ,
                                                                                      struct efx_self_tests * ))0)) {
    rc_reset = (*((efx->type)->test_chip))(efx, tests);
    if (rc_reset != 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "Unable to recover from chip test\n");
      } else {
      }
      efx_schedule_reset(efx, 3);
      return (rc_reset);
    } else {
    }
    if (tests->registers < 0 && rc_test == 0) {
      rc_test = -5;
    } else {
    }
  } else {
  }
  ldv_mutex_lock_218(& efx->mac_lock);
  efx->phy_mode = (enum efx_phy_mode )((unsigned int )efx->phy_mode & 4294967293U);
  efx->loopback_mode = 0;
  __efx_reconfigure_port(efx);
  ldv_mutex_unlock_219(& efx->mac_lock);
  rc = efx_test_phy(efx, tests, flags);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  rc = efx_test_loopbacks(efx, tests, (unsigned int )efx->loopback_modes);
  if (rc != 0 && rc_test == 0) {
    rc_test = rc;
  } else {
  }
  ldv_mutex_lock_220(& efx->mac_lock);
  efx->phy_mode = (enum efx_phy_mode )phy_mode;
  efx->loopback_mode = loopback_mode;
  __efx_reconfigure_port(efx);
  ldv_mutex_unlock_221(& efx->mac_lock);
  netif_device_attach(efx->net_dev);
  return (rc_test);
}
}
void efx_selftest_async_start(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  {
  channel = efx->channel[0];
  goto ldv_46331;
  ldv_46330:
  efx_nic_event_test_start(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46331: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46330;
  } else {
  }
  schedule_delayed_work(& efx->selftest_work, 250UL);
  return;
}
}
void efx_selftest_async_cancel(struct efx_nic *efx )
{
  {
  cancel_delayed_work_sync(& efx->selftest_work);
  return;
}
}
void efx_selftest_async_work(struct work_struct *data )
{
  struct efx_nic *efx ;
  struct work_struct const *__mptr ;
  struct efx_channel *channel ;
  int cpu ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  __mptr = (struct work_struct const *)data;
  efx = (struct efx_nic *)__mptr + 0xfffffffffffff8e0UL;
  channel = efx->channel[0];
  goto ldv_46347;
  ldv_46346:
  cpu = efx_nic_event_test_irq_cpu(channel);
  if (cpu < 0) {
    if ((efx->msg_enable & 32U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "channel %d failed to trigger an interrupt\n",
                 channel->channel);
    } else
    if ((efx->msg_enable & 32U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_selftest_async_work";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/selftest.c.prepared";
      descriptor.format = "channel %d triggered interrupt on CPU %d\n";
      descriptor.lineno = 885U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "channel %d triggered interrupt on CPU %d\n", channel->channel,
                             cpu);
      } else {
      }
    } else {
    }
  } else {
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_46347: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_46346;
  } else {
  }
  return;
}
}
void ldv_mutex_lock_203(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_204(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_205(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_206(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_207(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_208(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_209(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_210(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_211(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_212(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_213(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_214(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_215(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_216(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_217(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_218(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_219(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_220(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_221(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
extern char *strchr(char const * , int ) ;
int ldv_mutex_trylock_244(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_242(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_245(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_247(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_249(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_251(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_253(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_255(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_257(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_241(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_243(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_246(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_248(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_250(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_252(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_254(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_256(struct mutex *ldv_func_arg1 ) ;
__inline static void ethtool_cmd_speed_set(struct ethtool_cmd *ep , __u32 speed )
{
  {
  ep->speed = (unsigned short )speed;
  ep->speed_hi = (unsigned short )(speed >> 16);
  return;
}
}
__inline static __u32 ethtool_cmd_speed(struct ethtool_cmd const *ep )
{
  {
  return ((__u32 )(((int )ep->speed_hi << 16) | (int )ep->speed));
}
}
extern u32 ethtool_op_get_link(struct net_device * ) ;
extern int dev_open(struct net_device * ) ;
__inline static bool is_broadcast_ether_addr(u8 const *addr )
{
  {
  return ((unsigned int )((((((int )((unsigned char )*addr) & (int )((unsigned char )*(addr + 1UL))) & (int )((unsigned char )*(addr + 2UL))) & (int )((unsigned char )*(addr + 3UL))) & (int )((unsigned char )*(addr + 4UL))) & (int )((unsigned char )*(addr + 5UL))) == 255U);
}
}
__inline static unsigned int compare_ether_addr(u8 const *addr1 , u8 const *addr2 )
{
  u16 const *a ;
  u16 const *b ;
  {
  a = (u16 const *)addr1;
  b = (u16 const *)addr2;
  return ((unsigned int )((((int )((unsigned short )*a) ^ (int )((unsigned short )*b)) | ((int )((unsigned short )*(a + 1UL)) ^ (int )((unsigned short )*(b + 1UL)))) | ((int )((unsigned short )*(a + 2UL)) ^ (int )((unsigned short )*(b + 2UL)))) != 0U);
}
}
__inline static bool ether_addr_equal(u8 const *addr1 , u8 const *addr2 )
{
  unsigned int tmp ;
  {
  tmp = compare_ether_addr(addr1, addr2);
  return (tmp == 0U);
}
}
extern int mdio45_nway_restart(struct mdio_if_info const * ) ;
void efx_mcdi_print_fwver(struct efx_nic *efx , char *buf , size_t len ) ;
int efx_ptp_get_ts_info(struct net_device *net_dev , struct ethtool_ts_info *ts_info ) ;
static u64 efx_get_uint_stat(void *field )
{
  {
  return ((u64 )*((unsigned int *)field));
}
}
static u64 efx_get_u64_stat(void *field )
{
  {
  return (*((u64 *)field));
}
}
static u64 efx_get_atomic_stat(void *field )
{
  int tmp ;
  {
  tmp = atomic_read((atomic_t const *)field);
  return ((u64 )tmp);
}
}
static struct efx_ethtool_stat const efx_ethtool_stats[71U] =
  { {"tx_bytes", 0, 0U, & efx_get_u64_stat},
        {"tx_good_bytes", 0, 8U, & efx_get_u64_stat},
        {"tx_bad_bytes", 0, 16U, & efx_get_u64_stat},
        {"tx_packets", 0, 24U, & efx_get_u64_stat},
        {"tx_bad", 0, 32U, & efx_get_u64_stat},
        {"tx_pause", 0, 40U, & efx_get_u64_stat},
        {"tx_control", 0, 48U, & efx_get_u64_stat},
        {"tx_unicast", 0, 56U, & efx_get_u64_stat},
        {"tx_multicast", 0, 64U, & efx_get_u64_stat},
        {"tx_broadcast", 0, 72U, & efx_get_u64_stat},
        {"tx_lt64", 0, 80U, & efx_get_u64_stat},
        {"tx_64", 0, 88U, & efx_get_u64_stat},
        {"tx_65_to_127", 0, 96U, & efx_get_u64_stat},
        {"tx_128_to_255", 0, 104U, & efx_get_u64_stat},
        {"tx_256_to_511", 0, 112U, & efx_get_u64_stat},
        {"tx_512_to_1023", 0, 120U, & efx_get_u64_stat},
        {"tx_1024_to_15xx", 0, 128U, & efx_get_u64_stat},
        {"tx_15xx_to_jumbo", 0, 136U, & efx_get_u64_stat},
        {"tx_gtjumbo", 0, 144U, & efx_get_u64_stat},
        {"tx_collision", 0, 152U, & efx_get_u64_stat},
        {"tx_single_collision", 0, 160U, & efx_get_u64_stat},
        {"tx_multiple_collision", 0, 168U, & efx_get_u64_stat},
        {"tx_excessive_collision", 0, 176U, & efx_get_u64_stat},
        {"tx_deferred", 0, 184U, & efx_get_u64_stat},
        {"tx_late_collision", 0, 192U, & efx_get_u64_stat},
        {"tx_excessive_deferred", 0, 200U, & efx_get_u64_stat},
        {"tx_non_tcpudp", 0, 208U, & efx_get_u64_stat},
        {"tx_mac_src_error", 0, 216U, & efx_get_u64_stat},
        {"tx_ip_src_error", 0, 224U, & efx_get_u64_stat},
        {"tx_tso_bursts", 3, 204U, & efx_get_uint_stat},
        {"tx_tso_long_headers", 3, 208U, & efx_get_uint_stat},
        {"tx_tso_packets", 3, 212U, & efx_get_uint_stat},
        {"tx_pushes", 3, 216U, & efx_get_uint_stat},
        {"rx_bytes", 0, 232U, & efx_get_u64_stat},
        {"rx_good_bytes", 0, 240U, & efx_get_u64_stat},
        {"rx_bad_bytes", 0, 248U, & efx_get_u64_stat},
        {"rx_packets", 0, 256U, & efx_get_u64_stat},
        {"rx_good", 0, 264U, & efx_get_u64_stat},
        {"rx_bad", 0, 272U, & efx_get_u64_stat},
        {"rx_pause", 0, 280U, & efx_get_u64_stat},
        {"rx_control", 0, 288U, & efx_get_u64_stat},
        {"rx_unicast", 0, 296U, & efx_get_u64_stat},
        {"rx_multicast", 0, 304U, & efx_get_u64_stat},
        {"rx_broadcast", 0, 312U, & efx_get_u64_stat},
        {"rx_lt64", 0, 320U, & efx_get_u64_stat},
        {"rx_64", 0, 328U, & efx_get_u64_stat},
        {"rx_65_to_127", 0, 336U, & efx_get_u64_stat},
        {"rx_128_to_255", 0, 344U, & efx_get_u64_stat},
        {"rx_256_to_511", 0, 352U, & efx_get_u64_stat},
        {"rx_512_to_1023", 0, 360U, & efx_get_u64_stat},
        {"rx_1024_to_15xx", 0, 368U, & efx_get_u64_stat},
        {"rx_15xx_to_jumbo", 0, 376U, & efx_get_u64_stat},
        {"rx_gtjumbo", 0, 384U, & efx_get_u64_stat},
        {"rx_bad_lt64", 0, 392U, & efx_get_u64_stat},
        {"rx_bad_64_to_15xx", 0, 400U, & efx_get_u64_stat},
        {"rx_bad_15xx_to_jumbo", 0, 408U, & efx_get_u64_stat},
        {"rx_bad_gtjumbo", 0, 416U, & efx_get_u64_stat},
        {"rx_overflow", 0, 424U, & efx_get_u64_stat},
        {"rx_missed", 0, 432U, & efx_get_u64_stat},
        {"rx_false_carrier", 0, 440U, & efx_get_u64_stat},
        {"rx_symbol_error", 0, 448U, & efx_get_u64_stat},
        {"rx_align_error", 0, 456U, & efx_get_u64_stat},
        {"rx_length_error", 0, 464U, & efx_get_u64_stat},
        {"rx_internal_error", 0, 472U, & efx_get_u64_stat},
        {"rx_nodesc_drop_cnt", 1, 3300U, & efx_get_uint_stat},
        {"rx_reset", 1, 2488U, & efx_get_atomic_stat},
        {"rx_tobe_disc", 2, 280U, & efx_get_uint_stat},
        {"rx_ip_hdr_chksum_err", 2, 284U, & efx_get_uint_stat},
        {"rx_tcp_udp_chksum_err", 2, 288U, & efx_get_uint_stat},
        {"rx_mcast_mismatch", 2, 292U, & efx_get_uint_stat},
        {"rx_frm_trunc", 2, 296U, & efx_get_uint_stat}};
static int efx_ethtool_phys_id(struct net_device *net_dev , enum ethtool_phys_id_state state )
{
  struct efx_nic *efx ;
  void *tmp ;
  enum efx_led_mode mode ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  mode = 2;
  switch ((unsigned int )state) {
  case 2U:
  mode = 1;
  goto ldv_41664;
  case 3U:
  mode = 0;
  goto ldv_41664;
  case 0U:
  mode = 2;
  goto ldv_41664;
  case 1U: ;
  return (1);
  }
  ldv_41664:
  (*((efx->type)->set_id_led))(efx, mode);
  return (0);
}
}
static int efx_ethtool_get_settings(struct net_device *net_dev , struct ethtool_cmd *ecmd )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_link_state *link_state ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  link_state = & efx->link_state;
  ldv_mutex_lock_248(& efx->mac_lock);
  (*((efx->phy_op)->get_settings))(efx, ecmd);
  ldv_mutex_unlock_249(& efx->mac_lock);
  ecmd->supported = ecmd->supported & 4294967279U;
  ecmd->supported = ecmd->supported | 24576U;
  if ((66600958 >> (int )efx->loopback_mode) & 1) {
    ethtool_cmd_speed_set(ecmd, link_state->speed);
    ecmd->duplex = (__u8 )link_state->fd;
  } else {
  }
  return (0);
}
}
static int efx_ethtool_set_settings(struct net_device *net_dev , struct ethtool_cmd *ecmd )
{
  struct efx_nic *efx ;
  void *tmp ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  __u32 tmp___1 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___1 = ethtool_cmd_speed((struct ethtool_cmd const *)ecmd);
  if (tmp___1 == 1000U && (unsigned int )ecmd->duplex != 1U) {
    if ((int )efx->msg_enable & 1) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_ethtool_set_settings";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/ethtool.c.prepared";
      descriptor.format = "rejecting unsupported 1000Mbps HD setting\n";
      descriptor.lineno = 319U;
      descriptor.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "rejecting unsupported 1000Mbps HD setting\n");
      } else {
      }
    } else {
    }
    return (-22);
  } else {
  }
  ldv_mutex_lock_250(& efx->mac_lock);
  rc = (*((efx->phy_op)->set_settings))(efx, ecmd);
  ldv_mutex_unlock_251(& efx->mac_lock);
  return (rc);
}
}
static void efx_ethtool_get_drvinfo(struct net_device *net_dev , struct ethtool_drvinfo *info )
{
  struct efx_nic *efx ;
  void *tmp ;
  int tmp___0 ;
  char const *tmp___1 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  strlcpy((char *)(& info->driver), "sfc", 32UL);
  strlcpy((char *)(& info->version), "3.2", 32UL);
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 > 2) {
    efx_mcdi_print_fwver(efx, (char *)(& info->fw_version), 32UL);
  } else {
  }
  tmp___1 = pci_name((struct pci_dev const *)efx->pci_dev);
  strlcpy((char *)(& info->bus_info), tmp___1, 32UL);
  return;
}
}
static int efx_ethtool_get_regs_len(struct net_device *net_dev )
{
  void *tmp ;
  size_t tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  tmp___0 = efx_nic_get_regs_len((struct efx_nic *)tmp);
  return ((int )tmp___0);
}
}
static void efx_ethtool_get_regs(struct net_device *net_dev , struct ethtool_regs *regs ,
                                 void *buf )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  regs->version = (__u32 )(efx->type)->revision;
  efx_nic_get_regs(efx, buf);
  return;
}
}
static u32 efx_ethtool_get_msglevel(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  return (efx->msg_enable);
}
}
static void efx_ethtool_set_msglevel(struct net_device *net_dev , u32 msg_enable )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  efx->msg_enable = msg_enable;
  return;
}
}
static void efx_fill_test(unsigned int test_index , struct ethtool_string *strings ,
                          u64 *data , int *test , char const *unit_format , int unit_id ,
                          char const *test_format , char const *test_id )
{
  struct ethtool_string unit_str ;
  struct ethtool_string test_str ;
  char *tmp ;
  {
  if ((unsigned long )data != (unsigned long )((u64 *)0)) {
    *(data + (unsigned long )test_index) = (u64 )*test;
  } else {
  }
  if ((unsigned long )strings != (unsigned long )((struct ethtool_string *)0)) {
    tmp = strchr(unit_format, 37);
    if ((unsigned long )tmp != (unsigned long )((char *)0)) {
      snprintf((char *)(& unit_str.name), 32UL, unit_format, unit_id);
    } else {
      strcpy((char *)(& unit_str.name), unit_format);
    }
    snprintf((char *)(& test_str.name), 32UL, test_format, test_id);
    snprintf((char *)(& (strings + (unsigned long )test_index)->name), 32UL, "%-6s %-24s",
             (char *)(& unit_str.name), (char *)(& test_str.name));
  } else {
  }
  return;
}
}
static int efx_fill_loopback_test(struct efx_nic *efx , struct efx_loopback_self_tests *lb_tests ,
                                  enum efx_loopback_mode mode , unsigned int test_index ,
                                  struct ethtool_string *strings , u64 *data )
{
  struct efx_channel *channel ;
  struct efx_channel *tmp ;
  struct efx_tx_queue *tx_queue ;
  unsigned int tmp___0 ;
  unsigned int tmp___1 ;
  bool tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  unsigned int tmp___5 ;
  unsigned int tmp___6 ;
  {
  tmp = efx_get_channel(efx, efx->tx_channel_offset);
  channel = tmp;
  tmp___3 = efx_channel_has_tx_queues(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_41728;
    ldv_41727:
    tmp___0 = test_index;
    test_index = test_index + 1U;
    efx_fill_test(tmp___0, strings, data, (int *)(& lb_tests->tx_sent) + (unsigned long )tx_queue->queue,
                  "txq%d", (int )tx_queue->queue, "loopback.%s.tx_sent", (unsigned int )mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )mode] : (char const * )"(invalid)");
    tmp___1 = test_index;
    test_index = test_index + 1U;
    efx_fill_test(tmp___1, strings, data, (int *)(& lb_tests->tx_done) + (unsigned long )tx_queue->queue,
                  "txq%d", (int )tx_queue->queue, "loopback.%s.tx_done", (unsigned int )mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )mode] : (char const * )"(invalid)");
    tx_queue = tx_queue + 1;
    ldv_41728: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___2 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___2) {
        goto ldv_41727;
      } else {
        goto ldv_41729;
      }
    } else {
    }
    ldv_41729: ;
  }
  tmp___5 = test_index;
  test_index = test_index + 1U;
  efx_fill_test(tmp___5, strings, data, & lb_tests->rx_good, "rx", 0, "loopback.%s.rx_good",
                (unsigned int )mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )mode] : (char const * )"(invalid)");
  tmp___6 = test_index;
  test_index = test_index + 1U;
  efx_fill_test(tmp___6, strings, data, & lb_tests->rx_bad, "rx", 0, "loopback.%s.rx_bad",
                (unsigned int )mode < (unsigned int )efx_loopback_mode_max ? efx_loopback_mode_names[(unsigned int )mode] : (char const * )"(invalid)");
  return ((int )test_index);
}
}
static int efx_ethtool_fill_self_tests(struct efx_nic *efx , struct efx_self_tests *tests ,
                                       struct ethtool_string *strings , u64 *data )
{
  struct efx_channel *channel ;
  unsigned int n ;
  unsigned int i ;
  enum efx_loopback_mode mode ;
  unsigned int tmp ;
  unsigned int tmp___0 ;
  unsigned int tmp___1 ;
  unsigned int tmp___2 ;
  unsigned int tmp___3 ;
  unsigned int tmp___4 ;
  char const *name ;
  unsigned int tmp___5 ;
  int tmp___6 ;
  {
  n = 0U;
  tmp = n;
  n = n + 1U;
  efx_fill_test(tmp, strings, data, & tests->phy_alive, "phy", 0, "alive", 0);
  tmp___0 = n;
  n = n + 1U;
  efx_fill_test(tmp___0, strings, data, & tests->nvram, "core", 0, "nvram", 0);
  tmp___1 = n;
  n = n + 1U;
  efx_fill_test(tmp___1, strings, data, & tests->interrupt, "core", 0, "interrupt",
                0);
  channel = efx->channel[0];
  goto ldv_41741;
  ldv_41740:
  tmp___2 = n;
  n = n + 1U;
  efx_fill_test(tmp___2, strings, data, (int *)(& tests->eventq_dma) + (unsigned long )channel->channel,
                "chan%d", channel->channel, "eventq.dma", 0);
  tmp___3 = n;
  n = n + 1U;
  efx_fill_test(tmp___3, strings, data, (int *)(& tests->eventq_int) + (unsigned long )channel->channel,
                "chan%d", channel->channel, "eventq.int", 0);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_41741: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_41740;
  } else {
  }
  tmp___4 = n;
  n = n + 1U;
  efx_fill_test(tmp___4, strings, data, & tests->registers, "core", 0, "registers",
                0);
  if ((unsigned long )(efx->phy_op)->run_tests != (unsigned long )((int (* )(struct efx_nic * ,
                                                                                        int * ,
                                                                                        unsigned int ))0)) {
    i = 0U;
    ldv_41745:
    name = (*((efx->phy_op)->test_name))(efx, i);
    if ((unsigned long )name == (unsigned long )((char const *)0)) {
      goto ldv_41744;
    } else {
    }
    tmp___5 = n;
    n = n + 1U;
    efx_fill_test(tmp___5, strings, data, (int *)(& tests->phy_ext) + (unsigned long )i,
                  "phy", 0, name, 0);
    i = i + 1U;
    goto ldv_41745;
    ldv_41744: ;
  } else {
  }
  mode = 0;
  goto ldv_41748;
  ldv_41747: ;
  if ((efx->loopback_modes & (u64 )(1 << (int )mode)) == 0ULL) {
    goto ldv_41746;
  } else {
  }
  tmp___6 = efx_fill_loopback_test(efx, (struct efx_loopback_self_tests *)(& tests->loopback) + (unsigned long )mode,
                                   mode, n, strings, data);
  n = (unsigned int )tmp___6;
  ldv_41746:
  mode = (enum efx_loopback_mode )((unsigned int )mode + 1U);
  ldv_41748: ;
  if ((unsigned int )mode <= 17U) {
    goto ldv_41747;
  } else {
  }
  return ((int )n);
}
}
static int efx_ethtool_get_sset_count(struct net_device *net_dev , int string_set )
{
  void *tmp ;
  int tmp___0 ;
  {
  switch (string_set) {
  case 1: ;
  return (71);
  case 0:
  tmp = netdev_priv((struct net_device const *)net_dev);
  tmp___0 = efx_ethtool_fill_self_tests((struct efx_nic *)tmp, 0, 0, 0);
  return (tmp___0);
  default: ;
  return (-22);
  }
}
}
static void efx_ethtool_get_strings(struct net_device *net_dev , u32 string_set ,
                                    u8 *strings )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct ethtool_string *ethtool_strings ;
  int i ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  ethtool_strings = (struct ethtool_string *)strings;
  switch (string_set) {
  case 1U:
  i = 0;
  goto ldv_41771;
  ldv_41770:
  strlcpy((char *)(& (ethtool_strings + (unsigned long )i)->name), efx_ethtool_stats[i].name,
          32UL);
  i = i + 1;
  ldv_41771: ;
  if ((unsigned int )i <= 70U) {
    goto ldv_41770;
  } else {
  }
  goto ldv_41773;
  case 0U:
  efx_ethtool_fill_self_tests(efx, 0, ethtool_strings, 0);
  goto ldv_41773;
  default: ;
  goto ldv_41773;
  }
  ldv_41773: ;
  return;
}
}
static void efx_ethtool_get_stats(struct net_device *net_dev , struct ethtool_stats *stats ,
                                  u64 *data )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_mac_stats *mac_stats ;
  struct efx_ethtool_stat const *stat ;
  struct efx_channel *channel ;
  struct efx_tx_queue *tx_queue ;
  int i ;
  u64 tmp___0 ;
  u64 tmp___1 ;
  bool tmp___2 ;
  bool tmp___3 ;
  int tmp___4 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  mac_stats = & efx->mac_stats;
  spin_lock_bh(& efx->stats_lock);
  (*((efx->type)->update_stats))(efx);
  i = 0;
  goto ldv_41804;
  ldv_41803:
  stat = (struct efx_ethtool_stat const *)(& efx_ethtool_stats) + (unsigned long )i;
  switch ((unsigned int )stat->source) {
  case 0U:
  *(data + (unsigned long )i) = (*(stat->get_stat))((void *)mac_stats + (unsigned long )stat->offset);
  goto ldv_41790;
  case 1U:
  *(data + (unsigned long )i) = (*(stat->get_stat))((void *)efx + (unsigned long )stat->offset);
  goto ldv_41790;
  case 2U:
  *(data + (unsigned long )i) = 0ULL;
  channel = efx->channel[0];
  goto ldv_41794;
  ldv_41793:
  tmp___0 = (*(stat->get_stat))((void *)channel + (unsigned long )stat->offset);
  *(data + (unsigned long )i) = *(data + (unsigned long )i) + tmp___0;
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_41794: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_41793;
  } else {
  }
  goto ldv_41790;
  case 3U:
  *(data + (unsigned long )i) = 0ULL;
  channel = efx->channel[0];
  goto ldv_41801;
  ldv_41800:
  tmp___3 = efx_channel_has_tx_queues(channel);
  if (tmp___3) {
    tmp___4 = 0;
  } else {
    tmp___4 = 1;
  }
  if (tmp___4) {
  } else {
    tx_queue = (struct efx_tx_queue *)(& channel->tx_queue);
    goto ldv_41798;
    ldv_41797:
    tmp___1 = (*(stat->get_stat))((void *)tx_queue + (unsigned long )stat->offset);
    *(data + (unsigned long )i) = *(data + (unsigned long )i) + tmp___1;
    tx_queue = tx_queue + 1;
    ldv_41798: ;
    if ((unsigned long )((struct efx_tx_queue *)(& channel->tx_queue) + 4UL) > (unsigned long )tx_queue) {
      tmp___2 = efx_tx_queue_used(tx_queue);
      if ((int )tmp___2) {
        goto ldv_41797;
      } else {
        goto ldv_41799;
      }
    } else {
    }
    ldv_41799: ;
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_41801: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_41800;
  } else {
  }
  goto ldv_41790;
  }
  ldv_41790:
  i = i + 1;
  ldv_41804: ;
  if ((unsigned int )i <= 70U) {
    goto ldv_41803;
  } else {
  }
  spin_unlock_bh(& efx->stats_lock);
  return;
}
}
static void efx_ethtool_self_test(struct net_device *net_dev , struct ethtool_test *test ,
                                  u64 *data )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_self_tests *efx_tests ;
  int already_up ;
  int rc ;
  void *tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = -12;
  tmp___0 = kzalloc(1072UL, 208U);
  efx_tests = (struct efx_self_tests *)tmp___0;
  if ((unsigned long )efx_tests == (unsigned long )((struct efx_self_tests *)0)) {
    goto fail;
  } else {
  }
  if ((unsigned int )efx->state != 1U) {
    rc = -5;
    goto fail1;
  } else {
  }
  if ((int )efx->msg_enable & 1) {
    netdev_info((struct net_device const *)efx->net_dev, "starting %sline testing\n",
                (int )test->flags & 1 ? (char *)"off" : (char *)"on");
  } else {
  }
  already_up = (int )(efx->net_dev)->flags & 1;
  if (already_up == 0) {
    rc = dev_open(efx->net_dev);
    if (rc != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "failed opening device.\n");
      } else {
      }
      goto fail1;
    } else {
    }
  } else {
  }
  rc = efx_selftest(efx, efx_tests, test->flags);
  if (already_up == 0) {
    dev_close(efx->net_dev);
  } else {
  }
  if ((int )efx->msg_enable & 1) {
    netdev_info((struct net_device const *)efx->net_dev, "%s %sline self-tests\n",
                rc == 0 ? (char *)"passed" : (char *)"failed", (int )test->flags & 1 ? (char *)"off" : (char *)"on");
  } else {
  }
  fail1:
  efx_ethtool_fill_self_tests(efx, efx_tests, 0, data);
  kfree((void const *)efx_tests);
  fail: ;
  if (rc != 0) {
    test->flags = test->flags | 2U;
  } else {
  }
  return;
}
}
static int efx_ethtool_nway_reset(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = mdio45_nway_restart((struct mdio_if_info const *)(& efx->mdio));
  return (tmp___0);
}
}
static int efx_ethtool_get_coalesce(struct net_device *net_dev , struct ethtool_coalesce *coalesce )
{
  struct efx_nic *efx ;
  void *tmp ;
  unsigned int tx_usecs ;
  unsigned int rx_usecs ;
  bool rx_adaptive ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  efx_get_irq_moderation(efx, & tx_usecs, & rx_usecs, & rx_adaptive);
  coalesce->tx_coalesce_usecs = tx_usecs;
  coalesce->tx_coalesce_usecs_irq = tx_usecs;
  coalesce->rx_coalesce_usecs = rx_usecs;
  coalesce->rx_coalesce_usecs_irq = rx_usecs;
  coalesce->use_adaptive_rx_coalesce = (__u32 )rx_adaptive;
  return (0);
}
}
static int efx_ethtool_set_coalesce(struct net_device *net_dev , struct ethtool_coalesce *coalesce )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_channel *channel ;
  unsigned int tx_usecs ;
  unsigned int rx_usecs ;
  bool adaptive ;
  bool rx_may_override_tx ;
  int rc ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if (coalesce->use_adaptive_tx_coalesce != 0U) {
    return (-22);
  } else {
  }
  efx_get_irq_moderation(efx, & tx_usecs, & rx_usecs, & adaptive);
  if (coalesce->rx_coalesce_usecs != rx_usecs) {
    rx_usecs = coalesce->rx_coalesce_usecs;
  } else {
    rx_usecs = coalesce->rx_coalesce_usecs_irq;
  }
  adaptive = coalesce->use_adaptive_rx_coalesce != 0U;
  rx_may_override_tx = (bool )(coalesce->tx_coalesce_usecs == tx_usecs && coalesce->tx_coalesce_usecs_irq == tx_usecs);
  if (coalesce->tx_coalesce_usecs != tx_usecs) {
    tx_usecs = coalesce->tx_coalesce_usecs;
  } else {
    tx_usecs = coalesce->tx_coalesce_usecs_irq;
  }
  rc = efx_init_irq_moderation(efx, tx_usecs, rx_usecs, (int )adaptive, (int )rx_may_override_tx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  channel = efx->channel[0];
  goto ldv_41841;
  ldv_41840:
  (*((efx->type)->push_irq_moderation))(channel);
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_41841: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_41840;
  } else {
  }
  return (0);
}
}
static void efx_ethtool_get_ringparam(struct net_device *net_dev , struct ethtool_ringparam *ring )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  ring->rx_max_pending = 4096U;
  ring->tx_max_pending = 4096U;
  ring->rx_pending = efx->rxq_entries;
  ring->tx_pending = efx->txq_entries;
  return;
}
}
static int efx_ethtool_set_ringparam(struct net_device *net_dev , struct ethtool_ringparam *ring )
{
  struct efx_nic *efx ;
  void *tmp ;
  u32 txq_entries ;
  __u32 _max1 ;
  unsigned int _max2 ;
  unsigned int tmp___0 ;
  int tmp___1 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if (((ring->rx_mini_pending != 0U || ring->rx_jumbo_pending != 0U) || ring->rx_pending > 4096U) || ring->tx_pending > 4096U) {
    return (-22);
  } else {
  }
  if (ring->rx_pending <= 127U) {
    if ((int )efx->msg_enable & 1) {
      netdev_err((struct net_device const *)efx->net_dev, "RX queues cannot be smaller than %u\n",
                 128U);
    } else {
    }
    return (-22);
  } else {
  }
  _max1 = ring->tx_pending;
  tmp___0 = efx_tx_max_skb_descs(efx);
  _max2 = tmp___0 * 2U;
  txq_entries = _max1 > _max2 ? _max1 : _max2;
  if (ring->tx_pending != txq_entries) {
    if ((int )efx->msg_enable & 1) {
      netdev_warn((struct net_device const *)efx->net_dev, "increasing TX queue size to minimum of %u\n",
                  txq_entries);
    } else {
    }
  } else {
  }
  tmp___1 = efx_realloc_channels(efx, ring->rx_pending, txq_entries);
  return (tmp___1);
}
}
static int efx_ethtool_set_pauseparam(struct net_device *net_dev , struct ethtool_pauseparam *pause )
{
  struct efx_nic *efx ;
  void *tmp ;
  u8 wanted_fc ;
  u8 old_fc ;
  u32 old_adv ;
  bool reset ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  struct _ddebug descriptor___0 ;
  long tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = 0;
  ldv_mutex_lock_252(& efx->mac_lock);
  wanted_fc = (u8 )(((pause->rx_pause != 0U ? 2 : 0) | (pause->tx_pause != 0U)) | (pause->autoneg != 0U ? 4 : 0));
  if ((int )wanted_fc & 1 && ((int )wanted_fc & 2) == 0) {
    if ((int )efx->msg_enable & 1) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_ethtool_set_pauseparam";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/ethtool.c.prepared";
      descriptor.format = "Flow control unsupported: tx ON rx OFF\n";
      descriptor.lineno = 810U;
      descriptor.flags = 0U;
      tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___0 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "Flow control unsupported: tx ON rx OFF\n");
      } else {
      }
    } else {
    }
    rc = -22;
    goto out;
  } else {
  }
  if (((int )wanted_fc & 4) != 0 && efx->link_advertising == 0U) {
    if ((int )efx->msg_enable & 1) {
      descriptor___0.modname = "sfc";
      descriptor___0.function = "efx_ethtool_set_pauseparam";
      descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/ethtool.c.prepared";
      descriptor___0.format = "Autonegotiation is disabled\n";
      descriptor___0.lineno = 817U;
      descriptor___0.flags = 0U;
      tmp___1 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
      if (tmp___1 != 0L) {
        __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                             "Autonegotiation is disabled\n");
      } else {
      }
    } else {
    }
    rc = -22;
    goto out;
  } else {
  }
  reset = (bool )((int )wanted_fc & 1 && ((int )efx->wanted_fc & 1) == 0);
  tmp___3 = efx_nic_rev(efx);
  if (tmp___3 <= 2 && (int )reset) {
    tmp___2 = efx_nic_rev(efx);
    if (tmp___2 == 2) {
      falcon_stop_nic_stats(efx);
      falcon_drain_tx_fifo(efx);
      falcon_reconfigure_xmac(efx);
      falcon_start_nic_stats(efx);
    } else {
      efx_schedule_reset(efx, 0);
    }
  } else {
  }
  old_adv = efx->link_advertising;
  old_fc = efx->wanted_fc;
  efx_link_set_wanted_fc(efx, (int )wanted_fc);
  if (efx->link_advertising != old_adv || (((int )efx->wanted_fc ^ (int )old_fc) & 4) != 0) {
    rc = (*((efx->phy_op)->reconfigure))(efx);
    if (rc != 0) {
      if ((int )efx->msg_enable & 1) {
        netdev_err((struct net_device const *)efx->net_dev, "Unable to advertise requested flow control setting\n");
      } else {
      }
      goto out;
    } else {
    }
  } else {
  }
  (*((efx->type)->reconfigure_mac))(efx);
  out:
  ldv_mutex_unlock_253(& efx->mac_lock);
  return (rc);
}
}
static void efx_ethtool_get_pauseparam(struct net_device *net_dev , struct ethtool_pauseparam *pause )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  pause->rx_pause = ((int )efx->wanted_fc & 2) != 0;
  pause->tx_pause = (__u32 )efx->wanted_fc & 1U;
  pause->autoneg = ((int )efx->wanted_fc & 4) != 0;
  return;
}
}
static void efx_ethtool_get_wol(struct net_device *net_dev , struct ethtool_wolinfo *wol )
{
  struct efx_nic *efx ;
  void *tmp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  return;
}
}
static int efx_ethtool_set_wol(struct net_device *net_dev , struct ethtool_wolinfo *wol )
{
  struct efx_nic *efx ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = (*((efx->type)->set_wol))(efx, wol->wolopts);
  return (tmp___0);
}
}
static int efx_ethtool_reset(struct net_device *net_dev , u32 *flags )
{
  struct efx_nic *efx ;
  void *tmp ;
  int rc ;
  int tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  rc = (*((efx->type)->map_reset_flags))(flags);
  if (rc < 0) {
    return (rc);
  } else {
  }
  tmp___0 = efx_reset(efx, (enum reset_type )rc);
  return (tmp___0);
}
}
static u8 const mac_addr_mc_mask[6U] = { 1U, 0U, 0U, 0U,
        0U, 0U};
static int efx_ethtool_get_class_rule(struct efx_nic *efx , struct ethtool_rx_flow_spec *rule )
{
  struct ethtool_tcpip4_spec *ip_entry ;
  struct ethtool_tcpip4_spec *ip_mask ;
  struct ethhdr *mac_entry ;
  struct ethhdr *mac_mask ;
  struct efx_filter_spec spec ;
  u16 vid ;
  u8 proto ;
  int rc ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  __u16 tmp ;
  {
  ip_entry = & rule->h_u.tcp_ip4_spec;
  ip_mask = & rule->m_u.tcp_ip4_spec;
  mac_entry = & rule->h_u.ether_spec;
  mac_mask = & rule->m_u.ether_spec;
  rc = efx_filter_get_filter_safe(efx, 1, rule->location, & spec);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((unsigned int )spec.dmaq_id == 4095U) {
    rule->ring_cookie = 0xffffffffffffffffULL;
  } else {
    rule->ring_cookie = (__u64 )spec.dmaq_id;
  }
  if ((unsigned int )*((unsigned char *)(& spec) + 0UL) == 9U || (unsigned int )*((unsigned char *)(& spec) + 0UL) == 8U) {
    rule->flow_type = 18U;
    __len = 6UL;
    if (__len > 63UL) {
      __ret = memcpy((void *)(& mac_mask->h_dest), (void const *)(& mac_addr_mc_mask),
                       __len);
    } else {
      __ret = memcpy((void *)(& mac_mask->h_dest), (void const *)(& mac_addr_mc_mask),
                               __len);
    }
    if ((unsigned int )*((unsigned char *)(& spec) + 0UL) == 9U) {
      __len___0 = 6UL;
      if (__len___0 > 63UL) {
        __ret___0 = memcpy((void *)(& mac_entry->h_dest), (void const *)(& mac_addr_mc_mask),
                             __len___0);
      } else {
        __ret___0 = memcpy((void *)(& mac_entry->h_dest), (void const *)(& mac_addr_mc_mask),
                                     __len___0);
      }
    } else {
    }
    return (0);
  } else {
  }
  rc = efx_filter_get_eth_local((struct efx_filter_spec const *)(& spec), & vid,
                                (u8 *)(& mac_entry->h_dest));
  if (rc == 0) {
    rule->flow_type = 18U;
    memset((void *)(& mac_mask->h_dest), -1, 6UL);
    if ((unsigned int )vid != 65535U) {
      rule->flow_type = rule->flow_type | 2147483648U;
      tmp = __fswab16((int )vid);
      rule->h_ext.vlan_tci = tmp;
      rule->m_ext.vlan_tci = 65295U;
    } else {
    }
    return (0);
  } else {
  }
  rc = efx_filter_get_ipv4_local((struct efx_filter_spec const *)(& spec), & proto,
                                 & ip_entry->ip4dst, & ip_entry->pdst);
  if (rc != 0) {
    rc = efx_filter_get_ipv4_full((struct efx_filter_spec const *)(& spec), & proto,
                                  & ip_entry->ip4dst, & ip_entry->pdst, & ip_entry->ip4src,
                                  & ip_entry->psrc);
    ip_mask->ip4src = 4294967295U;
    ip_mask->psrc = 65535U;
  } else {
  }
  rule->flow_type = (unsigned int )proto == 6U ? 1U : 2U;
  ip_mask->ip4dst = 4294967295U;
  ip_mask->pdst = 65535U;
  return (rc);
}
}
static int efx_ethtool_get_rxnfc(struct net_device *net_dev , struct ethtool_rxnfc *info ,
                                 u32 *rule_locs )
{
  struct efx_nic *efx ;
  void *tmp ;
  unsigned int min_revision ;
  int tmp___0 ;
  u32 tmp___1 ;
  u32 tmp___2 ;
  int tmp___3 ;
  s32 rc ;
  u32 tmp___4 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  switch (info->cmd) {
  case 45U:
  info->data = (__u64 )efx->n_rx_channels;
  return (0);
  case 41U:
  min_revision = 0U;
  info->data = 0ULL;
  switch (info->flow_type) {
  case 1U:
  info->data = info->data | 192ULL;
  case 2U: ;
  case 3U: ;
  case 4U: ;
  case 16U:
  info->data = info->data | 48ULL;
  min_revision = 2U;
  goto ldv_41925;
  case 5U:
  info->data = info->data | 192ULL;
  case 6U: ;
  case 7U: ;
  case 8U: ;
  case 17U:
  info->data = info->data | 48ULL;
  min_revision = 3U;
  goto ldv_41925;
  default: ;
  goto ldv_41925;
  }
  ldv_41925:
  tmp___0 = efx_nic_rev(efx);
  if ((unsigned int )tmp___0 < min_revision) {
    info->data = 0ULL;
  } else {
  }
  return (0);
  case 46U:
  tmp___1 = efx_filter_get_rx_id_limit(efx);
  info->data = (__u64 )tmp___1;
  if (info->data == 0ULL) {
    return (-95);
  } else {
  }
  info->data = info->data | 2147483648ULL;
  info->rule_cnt = efx_filter_count_rx_used(efx, 1);
  return (0);
  case 47U:
  tmp___2 = efx_filter_get_rx_id_limit(efx);
  if (tmp___2 == 0U) {
    return (-95);
  } else {
  }
  tmp___3 = efx_ethtool_get_class_rule(efx, & info->fs);
  return (tmp___3);
  case 48U:
  tmp___4 = efx_filter_get_rx_id_limit(efx);
  info->data = (__u64 )tmp___4;
  if (info->data == 0ULL) {
    return (-95);
  } else {
  }
  rc = efx_filter_get_rx_ids(efx, 1, rule_locs, info->rule_cnt);
  if (rc < 0) {
    return (rc);
  } else {
  }
  info->rule_cnt = (__u32 )rc;
  return (0);
  default: ;
  return (-95);
  }
}
}
static int efx_ethtool_set_class_rule(struct efx_nic *efx , struct ethtool_rx_flow_spec *rule )
{
  struct ethtool_tcpip4_spec *ip_entry ;
  struct ethtool_tcpip4_spec *ip_mask ;
  struct ethhdr *mac_entry ;
  struct ethhdr *mac_mask ;
  struct efx_filter_spec spec ;
  int rc ;
  u8 proto ;
  u16 vlan_tag_mask ;
  __u16 tmp ;
  unsigned int tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  bool tmp___3 ;
  __u16 tmp___4 ;
  int tmp___5 ;
  bool tmp___6 ;
  bool tmp___7 ;
  {
  ip_entry = & rule->h_u.tcp_ip4_spec;
  ip_mask = & rule->m_u.tcp_ip4_spec;
  mac_entry = & rule->h_u.ether_spec;
  mac_mask = & rule->m_u.ether_spec;
  if (rule->location != 4294967295U) {
    return (-22);
  } else {
  }
  if (rule->ring_cookie >= (__u64 )efx->n_rx_channels && rule->ring_cookie != 0xffffffffffffffffULL) {
    return (-22);
  } else {
  }
  if ((int )rule->flow_type < 0 && (((unsigned int )rule->m_ext.vlan_etype != 0U || rule->m_ext.data[0] != 0U) || rule->m_ext.data[1] != 0U)) {
    return (-22);
  } else {
  }
  efx_filter_init_rx(& spec, 1, 0, rule->ring_cookie != 0xffffffffffffffffULL ? (unsigned int )rule->ring_cookie : 4095U);
  switch (rule->flow_type) {
  case 1U: ;
  case 2U:
  proto = rule->flow_type == 1U ? 6U : 17U;
  if (ip_mask->ip4dst != 4294967295U || (unsigned int )ip_mask->pdst != 65535U) {
    return (-22);
  } else {
  }
  if ((ip_mask->ip4src != 0U || (unsigned int )ip_mask->psrc != 0U) && (ip_mask->ip4src != 4294967295U || (unsigned int )ip_mask->psrc != 65535U)) {
    return (-22);
  } else {
  }
  if ((unsigned int )ip_mask->tos != 0U || (unsigned int )rule->m_ext.vlan_tci != 0U) {
    return (-22);
  } else {
  }
  if (ip_mask->ip4src != 0U) {
    rc = efx_filter_set_ipv4_full(& spec, (int )proto, ip_entry->ip4dst, (int )ip_entry->pdst,
                                  ip_entry->ip4src, (int )ip_entry->psrc);
  } else {
    rc = efx_filter_set_ipv4_local(& spec, (int )proto, ip_entry->ip4dst, (int )ip_entry->pdst);
  }
  if (rc != 0) {
    return (rc);
  } else {
  }
  goto ldv_41950;
  case 2147483666U: ;
  case 18U: ;
  if ((int )rule->flow_type < 0) {
    tmp = __fswab16((int )rule->m_ext.vlan_tci);
    tmp___0 = tmp;
  } else {
    tmp___0 = 0U;
  }
  vlan_tag_mask = tmp___0;
  tmp___1 = is_zero_ether_addr((u8 const *)(& mac_mask->h_source));
  if (tmp___1) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  if (tmp___2 || (unsigned int )mac_mask->h_proto != 0U) {
    return (-22);
  } else {
  }
  tmp___7 = ether_addr_equal((u8 const *)(& mac_mask->h_dest), (u8 const *)(& mac_addr_mc_mask));
  if ((int )tmp___7 && (unsigned int )vlan_tag_mask == 0U) {
    tmp___3 = is_multicast_ether_addr((u8 const *)(& mac_entry->h_dest));
    if ((int )tmp___3) {
      rc = efx_filter_set_mc_def(& spec);
    } else {
      rc = efx_filter_set_uc_def(& spec);
    }
  } else {
    tmp___6 = is_broadcast_ether_addr((u8 const *)(& mac_mask->h_dest));
    if ((int )tmp___6 && ((unsigned int )vlan_tag_mask == 4095U || (unsigned int )vlan_tag_mask == 0U)) {
      if ((unsigned int )vlan_tag_mask != 0U) {
        tmp___4 = __fswab16((int )rule->h_ext.vlan_tci);
        tmp___5 = (int )tmp___4;
      } else {
        tmp___5 = 65535;
      }
      rc = efx_filter_set_eth_local(& spec, tmp___5, (u8 const *)(& mac_entry->h_dest));
    } else {
      rc = -22;
    }
  }
  if (rc != 0) {
    return (rc);
  } else {
  }
  goto ldv_41950;
  default: ;
  return (-22);
  }
  ldv_41950:
  rc = efx_filter_insert_filter(efx, & spec, 1);
  if (rc < 0) {
    return (rc);
  } else {
  }
  rule->location = (__u32 )rc;
  return (0);
}
}
static int efx_ethtool_set_rxnfc(struct net_device *net_dev , struct ethtool_rxnfc *info )
{
  struct efx_nic *efx ;
  void *tmp ;
  u32 tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = efx_filter_get_rx_id_limit(efx);
  if (tmp___0 == 0U) {
    return (-95);
  } else {
  }
  switch (info->cmd) {
  case 50U:
  tmp___1 = efx_ethtool_set_class_rule(efx, & info->fs);
  return (tmp___1);
  case 49U:
  tmp___2 = efx_filter_remove_id_safe(efx, 1, info->fs.location);
  return (tmp___2);
  default: ;
  return (-95);
  }
}
}
static u32 efx_ethtool_get_rxfh_indir_size(struct net_device *net_dev )
{
  struct efx_nic *efx ;
  void *tmp ;
  int tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = efx_nic_rev(efx);
  return (tmp___0 <= 1 || efx->n_rx_channels == 1U ? 0U : 128U);
}
}
static int efx_ethtool_get_rxfh_indir(struct net_device *net_dev , u32 *indir )
{
  struct efx_nic *efx ;
  void *tmp ;
  size_t __len ;
  void *__ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  __len = 512UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)indir, (void const *)(& efx->rx_indir_table), __len);
  } else {
    __ret = memcpy((void *)indir, (void const *)(& efx->rx_indir_table),
                             __len);
  }
  return (0);
}
}
static int efx_ethtool_set_rxfh_indir(struct net_device *net_dev , u32 const *indir )
{
  struct efx_nic *efx ;
  void *tmp ;
  size_t __len ;
  void *__ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  __len = 512UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& efx->rx_indir_table), (void const *)indir, __len);
  } else {
    __ret = memcpy((void *)(& efx->rx_indir_table), (void const *)indir,
                             __len);
  }
  efx_nic_push_rx_indir_table(efx);
  return (0);
}
}
static int efx_ethtool_get_module_eeprom(struct net_device *net_dev , struct ethtool_eeprom *ee ,
                                         u8 *data )
{
  struct efx_nic *efx ;
  void *tmp ;
  int ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned long )efx->phy_op == (unsigned long )((struct efx_phy_operations const *)0) || (unsigned long )(efx->phy_op)->get_module_eeprom == (unsigned long )((int (* )(struct efx_nic * ,
                                                                                                                                                                                           struct ethtool_eeprom * ,
                                                                                                                                                                                           u8 * ))0)) {
    return (-95);
  } else {
  }
  ldv_mutex_lock_254(& efx->mac_lock);
  ret = (*((efx->phy_op)->get_module_eeprom))(efx, ee, data);
  ldv_mutex_unlock_255(& efx->mac_lock);
  return (ret);
}
}
static int efx_ethtool_get_module_info(struct net_device *net_dev , struct ethtool_modinfo *modinfo )
{
  struct efx_nic *efx ;
  void *tmp ;
  int ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned long )efx->phy_op == (unsigned long )((struct efx_phy_operations const *)0) || (unsigned long )(efx->phy_op)->get_module_info == (unsigned long )((int (* )(struct efx_nic * ,
                                                                                                                                                                                         struct ethtool_modinfo * ))0)) {
    return (-95);
  } else {
  }
  ldv_mutex_lock_256(& efx->mac_lock);
  ret = (*((efx->phy_op)->get_module_info))(efx, modinfo);
  ldv_mutex_unlock_257(& efx->mac_lock);
  return (ret);
}
}
struct ethtool_ops const efx_ethtool_ops =
     {& efx_ethtool_get_settings, & efx_ethtool_set_settings, & efx_ethtool_get_drvinfo,
    & efx_ethtool_get_regs_len, & efx_ethtool_get_regs, & efx_ethtool_get_wol, & efx_ethtool_set_wol,
    & efx_ethtool_get_msglevel, & efx_ethtool_set_msglevel, & efx_ethtool_nway_reset,
    & ethtool_op_get_link, 0, 0, 0, & efx_ethtool_get_coalesce, & efx_ethtool_set_coalesce,
    & efx_ethtool_get_ringparam, & efx_ethtool_set_ringparam, & efx_ethtool_get_pauseparam,
    & efx_ethtool_set_pauseparam, & efx_ethtool_self_test, & efx_ethtool_get_strings,
    & efx_ethtool_phys_id, & efx_ethtool_get_stats, 0, 0, 0, 0, & efx_ethtool_get_sset_count,
    & efx_ethtool_get_rxnfc, & efx_ethtool_set_rxnfc, 0, & efx_ethtool_reset, & efx_ethtool_get_rxfh_indir_size,
    & efx_ethtool_get_rxfh_indir, & efx_ethtool_set_rxfh_indir, 0, 0, 0, 0, 0, & efx_ptp_get_ts_info,
    & efx_ethtool_get_module_info, & efx_ethtool_get_module_eeprom, 0, 0};
void ldv_main10_sequence_infinite_withcheck_stateful(void)
{
  struct net_device *var_group1 ;
  struct ethtool_cmd *var_group2 ;
  struct ethtool_drvinfo *var_group3 ;
  struct ethtool_regs *var_group4 ;
  void *var_efx_ethtool_get_regs_8_p2 ;
  u32 var_efx_ethtool_set_msglevel_10_p1 ;
  struct ethtool_coalesce *var_group5 ;
  struct ethtool_ringparam *var_group6 ;
  struct ethtool_pauseparam *var_group7 ;
  int var_efx_ethtool_get_sset_count_14_p1 ;
  struct ethtool_test *var_group8 ;
  u64 *var_efx_ethtool_self_test_17_p2 ;
  u32 var_efx_ethtool_get_strings_15_p1 ;
  u8 *var_efx_ethtool_get_strings_15_p2 ;
  enum ethtool_phys_id_state var_efx_ethtool_phys_id_3_p1 ;
  struct ethtool_stats *var_group9 ;
  u64 *var_efx_ethtool_get_stats_16_p2 ;
  struct ethtool_wolinfo *var_group10 ;
  u32 *var_efx_ethtool_reset_27_p1 ;
  struct ethtool_rxnfc *var_group11 ;
  u32 *var_efx_ethtool_get_rxnfc_29_p2 ;
  u32 *var_efx_ethtool_get_rxfh_indir_33_p1 ;
  u32 const *var_efx_ethtool_set_rxfh_indir_34_p1 ;
  struct ethtool_modinfo *var_group12 ;
  struct ethtool_eeprom *var_group13 ;
  u8 *var_efx_ethtool_get_module_eeprom_35_p2 ;
  int tmp ;
  int tmp___0 ;
  {
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_42073;
  ldv_42072:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0:
  ldv_handler_precall();
  efx_ethtool_get_settings(var_group1, var_group2);
  goto ldv_42042;
  case 1:
  ldv_handler_precall();
  efx_ethtool_set_settings(var_group1, var_group2);
  goto ldv_42042;
  case 2:
  ldv_handler_precall();
  efx_ethtool_get_drvinfo(var_group1, var_group3);
  goto ldv_42042;
  case 3:
  ldv_handler_precall();
  efx_ethtool_get_regs_len(var_group1);
  goto ldv_42042;
  case 4:
  ldv_handler_precall();
  efx_ethtool_get_regs(var_group1, var_group4, var_efx_ethtool_get_regs_8_p2);
  goto ldv_42042;
  case 5:
  ldv_handler_precall();
  efx_ethtool_get_msglevel(var_group1);
  goto ldv_42042;
  case 6:
  ldv_handler_precall();
  efx_ethtool_set_msglevel(var_group1, var_efx_ethtool_set_msglevel_10_p1);
  goto ldv_42042;
  case 7:
  ldv_handler_precall();
  efx_ethtool_nway_reset(var_group1);
  goto ldv_42042;
  case 8:
  ldv_handler_precall();
  efx_ethtool_get_coalesce(var_group1, var_group5);
  goto ldv_42042;
  case 9:
  ldv_handler_precall();
  efx_ethtool_set_coalesce(var_group1, var_group5);
  goto ldv_42042;
  case 10:
  ldv_handler_precall();
  efx_ethtool_get_ringparam(var_group1, var_group6);
  goto ldv_42042;
  case 11:
  ldv_handler_precall();
  efx_ethtool_set_ringparam(var_group1, var_group6);
  goto ldv_42042;
  case 12:
  ldv_handler_precall();
  efx_ethtool_get_pauseparam(var_group1, var_group7);
  goto ldv_42042;
  case 13:
  ldv_handler_precall();
  efx_ethtool_set_pauseparam(var_group1, var_group7);
  goto ldv_42042;
  case 14:
  ldv_handler_precall();
  efx_ethtool_get_sset_count(var_group1, var_efx_ethtool_get_sset_count_14_p1);
  goto ldv_42042;
  case 15:
  ldv_handler_precall();
  efx_ethtool_self_test(var_group1, var_group8, var_efx_ethtool_self_test_17_p2);
  goto ldv_42042;
  case 16:
  ldv_handler_precall();
  efx_ethtool_get_strings(var_group1, var_efx_ethtool_get_strings_15_p1, var_efx_ethtool_get_strings_15_p2);
  goto ldv_42042;
  case 17:
  ldv_handler_precall();
  efx_ethtool_phys_id(var_group1, var_efx_ethtool_phys_id_3_p1);
  goto ldv_42042;
  case 18:
  ldv_handler_precall();
  efx_ethtool_get_stats(var_group1, var_group9, var_efx_ethtool_get_stats_16_p2);
  goto ldv_42042;
  case 19:
  ldv_handler_precall();
  efx_ethtool_get_wol(var_group1, var_group10);
  goto ldv_42042;
  case 20:
  ldv_handler_precall();
  efx_ethtool_set_wol(var_group1, var_group10);
  goto ldv_42042;
  case 21:
  ldv_handler_precall();
  efx_ethtool_reset(var_group1, var_efx_ethtool_reset_27_p1);
  goto ldv_42042;
  case 22:
  ldv_handler_precall();
  efx_ethtool_get_rxnfc(var_group1, var_group11, var_efx_ethtool_get_rxnfc_29_p2);
  goto ldv_42042;
  case 23:
  ldv_handler_precall();
  efx_ethtool_set_rxnfc(var_group1, var_group11);
  goto ldv_42042;
  case 24:
  ldv_handler_precall();
  efx_ethtool_get_rxfh_indir_size(var_group1);
  goto ldv_42042;
  case 25:
  ldv_handler_precall();
  efx_ethtool_get_rxfh_indir(var_group1, var_efx_ethtool_get_rxfh_indir_33_p1);
  goto ldv_42042;
  case 26:
  ldv_handler_precall();
  efx_ethtool_set_rxfh_indir(var_group1, var_efx_ethtool_set_rxfh_indir_34_p1);
  goto ldv_42042;
  case 27:
  ldv_handler_precall();
  efx_ethtool_get_module_info(var_group1, var_group12);
  goto ldv_42042;
  case 28:
  ldv_handler_precall();
  efx_ethtool_get_module_eeprom(var_group1, var_group13, var_efx_ethtool_get_module_eeprom_35_p2);
  goto ldv_42042;
  default: ;
  goto ldv_42042;
  }
  ldv_42042: ;
  ldv_42073:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    goto ldv_42072;
  } else {
  }
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_241(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_242(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_243(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_244(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_245(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_246(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_247(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_248(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_249(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_250(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_251(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_252(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_253(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_254(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_255(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_256(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_257(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_278(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_276(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_279(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_281(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_275(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_277(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_280(struct mutex *ldv_func_arg1 ) ;
extern int mdio_set_flag(struct mdio_if_info const * , int , int , u16 , int ,
                         bool ) ;
extern void mdio45_ethtool_gset_npage(struct mdio_if_info const * , struct ethtool_cmd * ,
                                      u32 , u32 ) ;
__inline static void mdio45_ethtool_gset(struct mdio_if_info const *mdio , struct ethtool_cmd *ecmd )
{
  {
  mdio45_ethtool_gset_npage(mdio, ecmd, 0U, 0U);
  return;
}
}
__inline static unsigned int efx_mdio_id_rev(u32 id )
{
  {
  return (id & 15U);
}
}
__inline static unsigned int efx_mdio_id_model(u32 id )
{
  {
  return ((id >> 4) & 63U);
}
}
unsigned int efx_mdio_id_oui(u32 id ) ;
__inline static void efx_mdio_write(struct efx_nic *efx , int devad , int addr , int value )
{
  {
  (*(efx->mdio.mdio_write))(efx->net_dev, efx->mdio.prtad, devad, (int )((u16 )addr),
                            (int )((u16 )value));
  return;
}
}
__inline static u32 efx_mdio_read_id(struct efx_nic *efx , int mmd )
{
  u16 id_low ;
  int tmp ;
  u16 id_hi ;
  int tmp___0 ;
  {
  tmp = efx_mdio_read(efx, mmd, 3);
  id_low = (u16 )tmp;
  tmp___0 = efx_mdio_read(efx, mmd, 2);
  id_hi = (u16 )tmp___0;
  return ((u32 )(((int )id_hi << 16) | (int )id_low));
}
}
int efx_mdio_reset_mmd(struct efx_nic *port , int mmd , int spins , int spintime ) ;
bool efx_mdio_links_ok(struct efx_nic *efx , unsigned int mmd_mask ) ;
void efx_mdio_transmit_disable(struct efx_nic *efx ) ;
void efx_mdio_phy_reconfigure(struct efx_nic *efx ) ;
int efx_mdio_set_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd ) ;
__inline static void efx_mdio_set_flag(struct efx_nic *efx , int devad , int addr ,
                                       int mask , bool state )
{
  {
  mdio_set_flag((struct mdio_if_info const *)(& efx->mdio), efx->mdio.prtad, devad,
                (int )((u16 )addr), mask, (int )state);
  return;
}
}
int efx_mdio_test_alive(struct efx_nic *efx ) ;
void falcon_qt202x_set_led(struct efx_nic *p , int led , int mode ) ;
void falcon_qt202x_set_led(struct efx_nic *p , int led , int mode )
{
  int addr ;
  {
  addr = led + 53254;
  efx_mdio_write(p, 1, addr, mode);
  return;
}
}
static int qt2025c_wait_heartbeat(struct efx_nic *efx )
{
  unsigned long timeout ;
  int reg ;
  int old_counter ;
  int counter ;
  {
  timeout = (unsigned long )jiffies + 1250UL;
  old_counter = 0;
  ldv_41714:
  reg = efx_mdio_read(efx, 3, 55278);
  if (reg < 0) {
    return (reg);
  } else {
  }
  counter = reg & 255;
  if (old_counter == 0) {
    old_counter = counter;
  } else
  if (counter != old_counter) {
    goto ldv_41707;
  } else {
  }
  if ((long )timeout - (long )jiffies < 0L) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "If an SFP+ direct attach cable is connected, please check that it complies with the SFP+ specification\n");
    } else {
    }
    return (-110);
  } else {
  }
  msleep(100U);
  goto ldv_41714;
  ldv_41707: ;
  return (0);
}
}
static int qt2025c_wait_fw_status_good(struct efx_nic *efx )
{
  unsigned long timeout ;
  int reg ;
  {
  timeout = (unsigned long )jiffies + 625UL;
  ldv_41727:
  reg = efx_mdio_read(efx, 3, 55293);
  if (reg < 0) {
    return (reg);
  } else {
  }
  if ((reg & 255) > 31) {
    goto ldv_41720;
  } else {
  }
  if ((long )timeout - (long )jiffies < 0L) {
    return (-110);
  } else {
  }
  msleep(100U);
  goto ldv_41727;
  ldv_41720: ;
  return (0);
}
}
static void qt2025c_restart_firmware(struct efx_nic *efx )
{
  {
  efx_mdio_write(efx, 3, 59476, 192);
  efx_mdio_write(efx, 3, 59476, 64);
  msleep(50U);
  return;
}
}
static int qt2025c_wait_reset(struct efx_nic *efx )
{
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  {
  rc = qt2025c_wait_heartbeat(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = qt2025c_wait_fw_status_good(efx);
  if (rc == -110) {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "qt2025c_wait_reset";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/qt202x_phy.c.prepared";
      descriptor.format = "bashing QT2025C microcontroller\n";
      descriptor.lineno = 240U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "bashing QT2025C microcontroller\n");
      } else {
      }
    } else {
    }
    qt2025c_restart_firmware(efx);
    rc = qt2025c_wait_heartbeat(efx);
    if (rc != 0) {
      return (rc);
    } else {
    }
    rc = qt2025c_wait_fw_status_good(efx);
  } else {
  }
  return (rc);
}
}
static void qt2025c_firmware_id(struct efx_nic *efx )
{
  struct qt202x_phy_data *phy_data ;
  u8 firmware_id[9U] ;
  size_t i ;
  int tmp ;
  {
  phy_data = (struct qt202x_phy_data *)efx->phy_data;
  i = 0UL;
  goto ldv_41744;
  ldv_41743:
  tmp = efx_mdio_read(efx, 3, (int )((unsigned int )i + 55280U));
  firmware_id[i] = (u8 )tmp;
  i = i + 1UL;
  ldv_41744: ;
  if (i <= 8UL) {
    goto ldv_41743;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "QT2025C firmware %xr%d v%d.%d.%d.%d [20%02d-%02d-%02d]\n",
                ((int )firmware_id[0] << 8) | (int )firmware_id[1], (int )firmware_id[2],
                (int )firmware_id[3] >> 4, (int )firmware_id[3] & 15, (int )firmware_id[4],
                (int )firmware_id[5], (int )firmware_id[6], (int )firmware_id[7],
                (int )firmware_id[8]);
  } else {
  }
  phy_data->firmware_ver = (u32 )((((((int )firmware_id[3] & 240) << 20) | (((int )firmware_id[3] & 15) << 16)) | ((int )firmware_id[4] << 8)) | (int )firmware_id[5]);
  return;
}
}
static void qt2025c_bug17190_workaround(struct efx_nic *efx )
{
  struct qt202x_phy_data *phy_data ;
  bool tmp ;
  int tmp___0 ;
  struct _ddebug descriptor ;
  long tmp___1 ;
  {
  phy_data = (struct qt202x_phy_data *)efx->phy_data;
  if ((int )efx->link_state.up) {
    phy_data->bug17190_in_bad_state = 0;
    return;
  } else {
    tmp = efx_mdio_links_ok(efx, 18U);
    if (tmp) {
      tmp___0 = 0;
    } else {
      tmp___0 = 1;
    }
    if (tmp___0) {
      phy_data->bug17190_in_bad_state = 0;
      return;
    } else {
    }
  }
  if (! phy_data->bug17190_in_bad_state) {
    phy_data->bug17190_in_bad_state = 1;
    phy_data->bug17190_timer = (unsigned long )jiffies + 500UL;
    return;
  } else {
  }
  if ((long )jiffies - (long )phy_data->bug17190_timer >= 0L) {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "qt2025c_bug17190_workaround";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/qt202x_phy.c.prepared";
      descriptor.format = "bashing QT2025C PMA/PMD\n";
      descriptor.lineno = 294U;
      descriptor.flags = 0U;
      tmp___1 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___1 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "bashing QT2025C PMA/PMD\n");
      } else {
      }
    } else {
    }
    efx_mdio_set_flag(efx, 1, 0, 1, 1);
    msleep(100U);
    efx_mdio_set_flag(efx, 1, 0, 1, 0);
    phy_data->bug17190_timer = (unsigned long )jiffies + 500UL;
  } else {
  }
  return;
}
}
static int qt2025c_select_phy_mode(struct efx_nic *efx )
{
  struct qt202x_phy_data *phy_data ;
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  int reg ;
  int rc ;
  int i ;
  uint16_t phy_op_mode ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  {
  phy_data = (struct qt202x_phy_data *)efx->phy_data;
  tmp = falcon_board(efx);
  board = tmp;
  if (phy_data->firmware_ver <= 33554687U) {
    return (0);
  } else {
  }
  phy_op_mode = (unsigned int )efx->loopback_mode == 0U ? 56U : 32U;
  reg = efx_mdio_read(efx, 1, 49945);
  if ((reg & 56) == (int )phy_op_mode) {
    return (0);
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    descriptor.modname = "sfc";
    descriptor.function = "qt2025c_select_phy_mode";
    descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/qt202x_phy.c.prepared";
    descriptor.format = "Switching PHY to mode 0x%04x\n";
    descriptor.lineno = 328U;
    descriptor.flags = 0U;
    tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
    if (tmp___0 != 0L) {
      __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                           "Switching PHY to mode 0x%04x\n", (int )phy_op_mode);
    } else {
    }
  } else {
  }
  efx_mdio_write(efx, 1, 49920, 0);
  if (board->major == 0 && board->minor <= 1) {
    efx_mdio_write(efx, 1, 49923, 17560);
    i = 0;
    goto ldv_41770;
    ldv_41769:
    efx_mdio_write(efx, 1, 49923, 17544);
    efx_mdio_write(efx, 1, 49923, 17536);
    efx_mdio_write(efx, 1, 49923, 17552);
    efx_mdio_write(efx, 1, 49923, 17560);
    i = i + 1;
    ldv_41770: ;
    if (i <= 8) {
      goto ldv_41769;
    } else {
    }
  } else {
    efx_mdio_write(efx, 1, 49923, 2336);
    efx_mdio_write(efx, 1, 53256, 4);
    i = 0;
    goto ldv_41773;
    ldv_41772:
    efx_mdio_write(efx, 1, 49923, 2304);
    efx_mdio_write(efx, 1, 53256, 5);
    efx_mdio_write(efx, 1, 49923, 2336);
    efx_mdio_write(efx, 1, 53256, 4);
    i = i + 1;
    ldv_41773: ;
    if (i <= 8) {
      goto ldv_41772;
    } else {
    }
    efx_mdio_write(efx, 1, 49923, 18688);
  }
  efx_mdio_write(efx, 1, 49923, 18688);
  efx_mdio_write(efx, 1, 49922, 4);
  efx_mdio_write(efx, 1, 49942, 19);
  efx_mdio_write(efx, 1, 49944, 84);
  efx_mdio_write(efx, 1, 49945, (int )phy_op_mode);
  efx_mdio_write(efx, 1, 49946, 152);
  efx_mdio_write(efx, 3, 38, 3584);
  efx_mdio_write(efx, 3, 39, 19);
  efx_mdio_write(efx, 3, 40, 42280);
  efx_mdio_write(efx, 1, 53254, 10);
  efx_mdio_write(efx, 1, 53255, 9);
  efx_mdio_write(efx, 1, 53256, 4);
  efx_mdio_write(efx, 1, 49943, 255);
  efx_mdio_set_flag(efx, 1, 49921, 64, 0);
  efx_mdio_write(efx, 1, 49920, 2);
  msleep(20U);
  qt2025c_restart_firmware(efx);
  rc = qt2025c_wait_reset(efx);
  if (rc < 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "PHY microcontroller reset during mode switch timed out\n");
    } else {
    }
    return (rc);
  } else {
  }
  return (0);
}
}
static int qt202x_reset_phy(struct efx_nic *efx )
{
  int rc ;
  struct falcon_board *tmp ;
  {
  if (efx->phy_type == 9U) {
    rc = qt2025c_wait_reset(efx);
    if (rc < 0) {
      goto fail;
    } else {
    }
  } else {
    rc = efx_mdio_reset_mmd(efx, 4, 50, 10);
    if (rc < 0) {
      goto fail;
    } else {
    }
  }
  msleep(250U);
  tmp = falcon_board(efx);
  (*((tmp->type)->init_phy))(efx);
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "PHY reset timed out\n");
  } else {
  }
  return (rc);
}
}
static int qt202x_phy_probe(struct efx_nic *efx )
{
  struct qt202x_phy_data *phy_data ;
  void *tmp ;
  {
  tmp = kzalloc(24UL, 208U);
  phy_data = (struct qt202x_phy_data *)tmp;
  if ((unsigned long )phy_data == (unsigned long )((struct qt202x_phy_data *)0)) {
    return (-12);
  } else {
  }
  efx->phy_data = (void *)phy_data;
  phy_data->phy_mode = efx->phy_mode;
  phy_data->bug17190_in_bad_state = 0;
  phy_data->bug17190_timer = 0UL;
  efx->mdio.mmds = 26U;
  efx->mdio.mode_support = 6U;
  efx->loopback_modes = 67305528ULL;
  return (0);
}
}
static int qt202x_phy_init(struct efx_nic *efx )
{
  u32 devid ;
  int rc ;
  unsigned int tmp ;
  unsigned int tmp___0 ;
  unsigned int tmp___1 ;
  {
  rc = qt202x_reset_phy(efx);
  if (rc != 0) {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "PHY init failed\n");
    } else {
    }
    return (rc);
  } else {
  }
  devid = efx_mdio_read_id(efx, 4);
  if ((efx->msg_enable & 2U) != 0U) {
    tmp = efx_mdio_id_rev(devid);
    tmp___0 = efx_mdio_id_model(devid);
    tmp___1 = efx_mdio_id_oui(devid);
    netdev_info((struct net_device const *)efx->net_dev, "PHY ID reg %x (OUI %06x model %02x revision %x)\n",
                devid, tmp___1, tmp___0, tmp);
  } else {
  }
  if (efx->phy_type == 9U) {
    qt2025c_firmware_id(efx);
  } else {
  }
  return (0);
}
}
static int qt202x_link_ok(struct efx_nic *efx )
{
  bool tmp ;
  {
  tmp = efx_mdio_links_ok(efx, 26U);
  return ((int )tmp);
}
}
static bool qt202x_phy_poll(struct efx_nic *efx )
{
  bool was_up ;
  int tmp ;
  {
  was_up = efx->link_state.up;
  tmp = qt202x_link_ok(efx);
  efx->link_state.up = tmp != 0;
  efx->link_state.speed = 10000U;
  efx->link_state.fd = 1;
  efx->link_state.fc = efx->wanted_fc;
  if (efx->phy_type == 9U) {
    qt2025c_bug17190_workaround(efx);
  } else {
  }
  return ((int )efx->link_state.up != (int )was_up);
}
}
static int qt202x_phy_reconfigure(struct efx_nic *efx )
{
  struct qt202x_phy_data *phy_data ;
  int rc ;
  int tmp ;
  {
  phy_data = (struct qt202x_phy_data *)efx->phy_data;
  if (efx->phy_type == 9U) {
    tmp = qt2025c_select_phy_mode(efx);
    rc = tmp;
    if (rc != 0) {
      return (rc);
    } else {
    }
    mdio_set_flag((struct mdio_if_info const *)(& efx->mdio), efx->mdio.prtad, 1,
                  49929, 8192, (int )((bool )((((int )efx->phy_mode & 1 || ((unsigned int )efx->phy_mode & 2U) != 0U) || (unsigned int )efx->loopback_mode == 16U) || (unsigned int )efx->loopback_mode == 17U)));
  } else {
    if (((unsigned int )efx->phy_mode & 1U) == 0U && (int )phy_data->phy_mode & 1) {
      qt202x_reset_phy(efx);
    } else {
    }
    efx_mdio_transmit_disable(efx);
  }
  efx_mdio_phy_reconfigure(efx);
  phy_data->phy_mode = efx->phy_mode;
  return (0);
}
}
static void qt202x_phy_get_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  {
  mdio45_ethtool_gset((struct mdio_if_info const *)(& efx->mdio), ecmd);
  return;
}
}
static void qt202x_phy_remove(struct efx_nic *efx )
{
  {
  kfree((void const *)efx->phy_data);
  efx->phy_data = 0;
  return;
}
}
static int qt202x_phy_get_module_info(struct efx_nic *efx , struct ethtool_modinfo *modinfo )
{
  {
  modinfo->type = 1U;
  modinfo->eeprom_len = 256U;
  return (0);
}
}
static int qt202x_phy_get_module_eeprom(struct efx_nic *efx , struct ethtool_eeprom *ee ,
                                        u8 *data )
{
  int mmd ;
  int reg_base ;
  int rc ;
  int i ;
  {
  if (efx->phy_type == 9U) {
    mmd = 3;
    reg_base = 53248;
  } else {
    mmd = 1;
    reg_base = 32775;
  }
  i = 0;
  goto ldv_41822;
  ldv_41821:
  rc = efx_mdio_read(efx, mmd, (int )((ee->offset + (__u32 )reg_base) + (__u32 )i));
  if (rc < 0) {
    return (rc);
  } else {
  }
  *(data + (unsigned long )i) = (u8 )rc;
  i = i + 1;
  ldv_41822: ;
  if ((__u32 )i < ee->len) {
    goto ldv_41821;
  } else {
  }
  return (0);
}
}
struct efx_phy_operations const falcon_qt202x_phy_ops =
     {& qt202x_phy_probe, & qt202x_phy_init, & efx_port_dummy_op_void, & qt202x_phy_remove,
    & qt202x_phy_reconfigure, & qt202x_phy_poll, & qt202x_phy_get_settings, & efx_mdio_set_settings,
    0, & efx_mdio_test_alive, 0, 0, & qt202x_phy_get_module_eeprom, & qt202x_phy_get_module_info};
void ldv_main11_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  int res_qt202x_phy_probe_9 ;
  struct ethtool_cmd *var_group2 ;
  struct ethtool_eeprom *var_group3 ;
  u8 *var_qt202x_phy_get_module_eeprom_17_p2 ;
  struct ethtool_modinfo *var_group4 ;
  int ldv_s_falcon_qt202x_phy_ops_efx_phy_operations ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_falcon_qt202x_phy_ops_efx_phy_operations = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_41860;
  ldv_41859:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_s_falcon_qt202x_phy_ops_efx_phy_operations == 0) {
    res_qt202x_phy_probe_9 = qt202x_phy_probe(var_group1);
    ldv_check_return_value(res_qt202x_phy_probe_9);
    ldv_check_return_value_probe(res_qt202x_phy_probe_9);
    if (res_qt202x_phy_probe_9 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_falcon_qt202x_phy_ops_efx_phy_operations = ldv_s_falcon_qt202x_phy_ops_efx_phy_operations + 1;
  } else {
  }
  goto ldv_41850;
  case 1: ;
  if (ldv_s_falcon_qt202x_phy_ops_efx_phy_operations == 1) {
    ldv_handler_precall();
    qt202x_phy_remove(var_group1);
    ldv_s_falcon_qt202x_phy_ops_efx_phy_operations = 0;
  } else {
  }
  goto ldv_41850;
  case 2:
  ldv_handler_precall();
  qt202x_phy_init(var_group1);
  goto ldv_41850;
  case 3:
  ldv_handler_precall();
  qt202x_phy_reconfigure(var_group1);
  goto ldv_41850;
  case 4:
  ldv_handler_precall();
  qt202x_phy_poll(var_group1);
  goto ldv_41850;
  case 5:
  ldv_handler_precall();
  qt202x_phy_get_settings(var_group1, var_group2);
  goto ldv_41850;
  case 6:
  ldv_handler_precall();
  qt202x_phy_get_module_eeprom(var_group1, var_group3, var_qt202x_phy_get_module_eeprom_17_p2);
  goto ldv_41850;
  case 7:
  ldv_handler_precall();
  qt202x_phy_get_module_info(var_group1, var_group4);
  goto ldv_41850;
  default: ;
  goto ldv_41850;
  }
  ldv_41850: ;
  ldv_41860:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0 || ldv_s_falcon_qt202x_phy_ops_efx_phy_operations != 0) {
    goto ldv_41859;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_275(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_276(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_277(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_278(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_279(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_280(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_281(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_294(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_290(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_292(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_295(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_297(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_289(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_291(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_293(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_296(struct mutex *ldv_func_arg1 ) ;
__inline static u16 mii_advertise_flowctrl(int cap )
{
  u16 adv ;
  {
  adv = 0U;
  if ((cap & 2) != 0) {
    adv = 3072U;
  } else {
  }
  if (cap & 1) {
    adv = (u16 )((unsigned int )adv ^ 2048U);
  } else {
  }
  return (adv);
}
}
__inline static u8 mii_resolve_flowctrl_fdx(u16 lcladv , u16 rmtadv )
{
  u8 cap ;
  {
  cap = 0U;
  if ((((int )lcladv & (int )rmtadv) & 1024) != 0) {
    cap = 3U;
  } else
  if ((((int )lcladv & (int )rmtadv) & 2048) != 0) {
    if (((int )lcladv & 1024) != 0) {
      cap = 2U;
    } else
    if (((int )rmtadv & 1024) != 0) {
      cap = 1U;
    } else {
    }
  } else {
  }
  return (cap);
}
}
extern int mdio45_links_ok(struct mdio_if_info const * , u32 ) ;
int efx_mdio_check_mmds(struct efx_nic *efx , unsigned int mmd_mask ) ;
void efx_mdio_set_mmds_lpower(struct efx_nic *efx , int low_power , unsigned int mmd_mask ) ;
void efx_mdio_an_reconfigure(struct efx_nic *efx ) ;
u8 efx_mdio_get_pause(struct efx_nic *efx ) ;
int efx_mdio_wait_reset_mmds(struct efx_nic *efx , unsigned int mmd_mask ) ;
unsigned int efx_mdio_id_oui(u32 id )
{
  unsigned int oui ;
  int i ;
  {
  oui = 0U;
  i = 0;
  goto ldv_41147;
  ldv_41146: ;
  if (((u32 )(1 << (i + 10)) & id) != 0U) {
    oui = (unsigned int )(1 << (i ^ 7)) | oui;
  } else {
  }
  i = i + 1;
  ldv_41147: ;
  if (i <= 21) {
    goto ldv_41146;
  } else {
  }
  return (oui);
}
}
int efx_mdio_reset_mmd(struct efx_nic *port , int mmd , int spins , int spintime )
{
  u32 ctrl ;
  int tmp ;
  {
  efx_mdio_write(port, mmd, 0, 32768);
  ldv_41156:
  msleep((unsigned int )spintime);
  tmp = efx_mdio_read(port, mmd, 0);
  ctrl = (u32 )tmp;
  spins = spins - 1;
  if (spins != 0 && (ctrl & 32768U) != 0U) {
    goto ldv_41156;
  } else {
  }
  return (spins != 0 ? spins : -110);
}
}
static int efx_mdio_check_mmd(struct efx_nic *efx , int mmd )
{
  int status ;
  {
  if (mmd != 7) {
    status = efx_mdio_read(efx, mmd, 8);
    if ((status & 49152) != 32768) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "PHY MMD %d not responding.\n",
                   mmd);
      } else {
      }
      return (-5);
    } else {
    }
  } else {
  }
  return (0);
}
}
int efx_mdio_wait_reset_mmds(struct efx_nic *efx , unsigned int mmd_mask )
{
  int spintime ;
  int tries ;
  int rc ;
  int in_reset ;
  int mask ;
  int mmd ;
  int stat ;
  {
  spintime = 10;
  tries = 100;
  rc = 0;
  goto ldv_41179;
  ldv_41178:
  mask = (int )mmd_mask;
  mmd = 0;
  in_reset = 0;
  goto ldv_41175;
  ldv_41174: ;
  if (mask & 1) {
    stat = efx_mdio_read(efx, mmd, 0);
    if (stat < 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "failed to read status of MMD %d\n",
                   mmd);
      } else {
      }
      return (-5);
    } else {
    }
    if ((stat & 32768) != 0) {
      in_reset = (1 << mmd) | in_reset;
    } else {
    }
  } else {
  }
  mask = mask >> 1;
  mmd = mmd + 1;
  ldv_41175: ;
  if (mask != 0) {
    goto ldv_41174;
  } else {
  }
  if (in_reset == 0) {
    goto ldv_41177;
  } else {
  }
  tries = tries - 1;
  msleep((unsigned int )spintime);
  ldv_41179: ;
  if (tries != 0) {
    goto ldv_41178;
  } else {
  }
  ldv_41177: ;
  if (in_reset != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "not all MMDs came out of reset in time. MMDs still in reset: %x\n",
                 in_reset);
    } else {
    }
    rc = -110;
  } else {
  }
  return (rc);
}
}
int efx_mdio_check_mmds(struct efx_nic *efx , unsigned int mmd_mask )
{
  int mmd ;
  int probe_mmd ;
  int devs1 ;
  int devs2 ;
  u32 devices ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  mmd = 0;
  if ((mmd_mask & 16U) == 0U) {
    tmp = __ffs((unsigned long )mmd_mask);
    probe_mmd = (int )tmp;
  } else {
    probe_mmd = 4;
  }
  devs1 = efx_mdio_read(efx, probe_mmd, 5);
  devs2 = efx_mdio_read(efx, probe_mmd, 6);
  if (devs1 < 0 || devs2 < 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "failed to read devices present\n");
    } else {
    }
    return (-5);
  } else {
  }
  devices = (u32 )((devs2 << 16) | devs1);
  if ((devices & mmd_mask) != mmd_mask) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "required MMDs not present: got %x, wanted %x\n",
                 devices, mmd_mask);
    } else {
    }
    return (-19);
  } else {
  }
  goto ldv_41191;
  ldv_41190: ;
  if ((int )mmd_mask & 1) {
    tmp___0 = efx_mdio_check_mmd(efx, mmd);
    if (tmp___0 != 0) {
      return (-5);
    } else {
    }
  } else {
  }
  mmd_mask = mmd_mask >> 1;
  mmd = mmd + 1;
  ldv_41191: ;
  if (mmd_mask != 0U) {
    goto ldv_41190;
  } else {
  }
  return (0);
}
}
bool efx_mdio_links_ok(struct efx_nic *efx , unsigned int mmd_mask )
{
  bool tmp ;
  int tmp___0 ;
  {
  if ((66600958 >> (int )efx->loopback_mode) & 1) {
    return (1);
  } else
  if ((133693440 >> (int )efx->loopback_mode) & 1) {
    return (0);
  } else {
    tmp = efx_phy_mode_disabled(efx->phy_mode);
    if ((int )tmp) {
      return (0);
    } else
    if ((unsigned int )efx->loopback_mode == 15U) {
      mmd_mask = mmd_mask & 4294967141U;
    } else
    if ((unsigned int )efx->loopback_mode == 16U) {
      mmd_mask = mmd_mask & 4294967157U;
    } else
    if ((unsigned int )efx->loopback_mode == 17U) {
      mmd_mask = mmd_mask & 4294967165U;
    } else {
    }
  }
  tmp___0 = mdio45_links_ok((struct mdio_if_info const *)(& efx->mdio), mmd_mask);
  return (tmp___0 != 0);
}
}
void efx_mdio_transmit_disable(struct efx_nic *efx )
{
  {
  efx_mdio_set_flag(efx, 1, 9, 1, (int )efx->phy_mode & 1);
  return;
}
}
void efx_mdio_phy_reconfigure(struct efx_nic *efx )
{
  {
  efx_mdio_set_flag(efx, 1, 0, 1, (unsigned int )efx->loopback_mode == 17U);
  efx_mdio_set_flag(efx, 3, 0, 16384, (unsigned int )efx->loopback_mode == 16U);
  efx_mdio_set_flag(efx, 4, 0, 16384, (unsigned int )efx->loopback_mode == 26U);
  return;
}
}
static void efx_mdio_set_mmd_lpower(struct efx_nic *efx , int lpower , int mmd )
{
  int stat ;
  int tmp ;
  {
  tmp = efx_mdio_read(efx, mmd, 1);
  stat = tmp;
  if ((stat & 2) != 0) {
    efx_mdio_set_flag(efx, mmd, 0, 2048, lpower != 0);
  } else {
  }
  return;
}
}
void efx_mdio_set_mmds_lpower(struct efx_nic *efx , int low_power , unsigned int mmd_mask )
{
  int mmd ;
  {
  mmd = 0;
  mmd_mask = mmd_mask & 4294967167U;
  goto ldv_41217;
  ldv_41216: ;
  if ((int )mmd_mask & 1) {
    efx_mdio_set_mmd_lpower(efx, low_power, mmd);
  } else {
  }
  mmd_mask = mmd_mask >> 1;
  mmd = mmd + 1;
  ldv_41217: ;
  if (mmd_mask != 0U) {
    goto ldv_41216;
  } else {
  }
  return;
}
}
int efx_mdio_set_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  struct ethtool_cmd prev ;
  __u32 tmp ;
  __u32 tmp___0 ;
  {
  prev.cmd = 1U;
  prev.supported = 0U;
  prev.advertising = 0U;
  prev.speed = (unsigned short)0;
  prev.duplex = (unsigned char)0;
  prev.port = (unsigned char)0;
  prev.phy_address = (unsigned char)0;
  prev.transceiver = (unsigned char)0;
  prev.autoneg = (unsigned char)0;
  prev.mdio_support = (unsigned char)0;
  prev.maxtxpkt = 0U;
  prev.maxrxpkt = 0U;
  prev.speed_hi = (unsigned short)0;
  prev.eth_tp_mdix = (unsigned char)0;
  prev.eth_tp_mdix_ctrl = (unsigned char)0;
  prev.lp_advertising = 0U;
  prev.reserved[0] = 0U;
  prev.reserved[1] = 0U;
  (*((efx->phy_op)->get_settings))(efx, & prev);
  if (ecmd->advertising == prev.advertising) {
    tmp = ethtool_cmd_speed((struct ethtool_cmd const *)ecmd);
    tmp___0 = ethtool_cmd_speed((struct ethtool_cmd const *)(& prev));
    if (tmp == tmp___0) {
      if ((int )ecmd->duplex == (int )prev.duplex) {
        if ((int )ecmd->port == (int )prev.port) {
          if ((int )ecmd->autoneg == (int )prev.autoneg) {
            return (0);
          } else {
          }
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
  }
  if ((unsigned int )prev.port != 0U || (unsigned int )ecmd->port != 0U) {
    return (-22);
  } else {
  }
  if ((unsigned int )ecmd->autoneg == 0U || ((ecmd->advertising | 64U) & ~ prev.supported) != 0U) {
    return (-22);
  } else {
  }
  efx_link_set_advertising(efx, ecmd->advertising | 64U);
  efx_mdio_an_reconfigure(efx);
  return (0);
}
}
void efx_mdio_an_reconfigure(struct efx_nic *efx )
{
  int reg ;
  int __ret_warn_on ;
  long tmp ;
  {
  __ret_warn_on = (efx->mdio.mmds & 128U) == 0U;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mdio_10g.c.prepared",
                       356);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  reg = 4097;
  if ((efx->link_advertising & 8192U) != 0U) {
    reg = reg | 1024;
  } else {
  }
  if ((efx->link_advertising & 16384U) != 0U) {
    reg = reg | 2048;
  } else {
  }
  efx_mdio_write(efx, 7, 16, reg);
  (*((efx->phy_op)->set_npage_adv))(efx, efx->link_advertising);
  reg = efx_mdio_read(efx, 7, 0);
  reg = reg | 12800;
  efx_mdio_write(efx, 7, 0, reg);
  return;
}
}
u8 efx_mdio_get_pause(struct efx_nic *efx )
{
  int __ret_warn_on ;
  long tmp ;
  int tmp___0 ;
  u16 tmp___1 ;
  u8 tmp___2 ;
  {
  if (((int )efx->wanted_fc & 4) == 0) {
    return (efx->wanted_fc);
  } else {
  }
  __ret_warn_on = (efx->mdio.mmds & 128U) == 0U;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mdio_10g.c.prepared",
                       382);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  tmp___0 = efx_mdio_read(efx, 7, 19);
  tmp___1 = mii_advertise_flowctrl((int )efx->wanted_fc);
  tmp___2 = mii_resolve_flowctrl_fdx((int )tmp___1, (int )((u16 )tmp___0));
  return (tmp___2);
}
}
int efx_mdio_test_alive(struct efx_nic *efx )
{
  int rc ;
  int devad ;
  unsigned long tmp ;
  u16 physid1 ;
  u16 physid2 ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp = __ffs((unsigned long )efx->mdio.mmds);
  devad = (int )tmp;
  ldv_mutex_lock_296(& efx->mac_lock);
  tmp___0 = efx_mdio_read(efx, devad, 2);
  physid1 = (u16 )tmp___0;
  tmp___1 = efx_mdio_read(efx, devad, 3);
  physid2 = (u16 )tmp___1;
  if ((((unsigned int )physid1 == 0U || (unsigned int )physid1 == 65535U) || (unsigned int )physid2 == 0U) || (unsigned int )physid2 == 65535U) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "no MDIO PHY present with ID %d\n",
                 efx->mdio.prtad);
    } else {
    }
    rc = -22;
  } else {
    rc = efx_mdio_check_mmds(efx, efx->mdio.mmds);
  }
  ldv_mutex_unlock_297(& efx->mac_lock);
  return (rc);
}
}
void ldv_main12_sequence_infinite_withcheck_stateful(void)
{
  int tmp ;
  int tmp___0 ;
  {
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_41261;
  ldv_41260:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  default: ;
  goto ldv_41259;
  }
  ldv_41259: ;
  ldv_41261:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    goto ldv_41260;
  } else {
  }
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_289(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_290(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_291(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_292(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_293(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_294(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___4 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_295(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_296(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mac_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_297(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mac_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_310(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_308(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_311(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_313(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_307(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_309(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_312(struct mutex *ldv_func_arg1 ) ;
void tenxpress_set_id_led(struct efx_nic *efx , enum efx_led_mode mode ) ;
static int tenxpress_init(struct efx_nic *efx )
{
  {
  efx_mdio_write(efx, 3, 55303, 8);
  efx_mdio_set_flag(efx, 1, 49159, 8, 1);
  efx_mdio_write(efx, 1, 49161, 128);
  return (0);
}
}
static int tenxpress_phy_probe(struct efx_nic *efx )
{
  struct tenxpress_phy_data *phy_data ;
  void *tmp ;
  {
  tmp = kzalloc(12UL, 208U);
  phy_data = (struct tenxpress_phy_data *)tmp;
  if ((unsigned long )phy_data == (unsigned long )((struct tenxpress_phy_data *)0)) {
    return (-12);
  } else {
  }
  efx->phy_data = (void *)phy_data;
  phy_data->phy_mode = efx->phy_mode;
  efx->mdio.mmds = 154U;
  efx->mdio.mode_support = 2U;
  efx->loopback_modes = 67338296ULL;
  efx->link_advertising = 4288U;
  return (0);
}
}
static int tenxpress_phy_init(struct efx_nic *efx )
{
  int rc ;
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  (*((tmp->type)->init_phy))(efx);
  if (((unsigned int )efx->phy_mode & 8U) == 0U) {
    rc = efx_mdio_wait_reset_mmds(efx, 154U);
    if (rc < 0) {
      return (rc);
    } else {
    }
    rc = efx_mdio_check_mmds(efx, 154U);
    if (rc < 0) {
      return (rc);
    } else {
    }
  } else {
  }
  rc = tenxpress_init(efx);
  if (rc < 0) {
    return (rc);
  } else {
  }
  efx_link_set_wanted_fc(efx, (int )efx->wanted_fc);
  efx_mdio_an_reconfigure(efx);
  schedule_timeout_uninterruptible(50L);
  falcon_reset_xaui(efx);
  return (0);
}
}
static int tenxpress_special_reset(struct efx_nic *efx )
{
  int rc ;
  int reg ;
  unsigned long __ms ;
  unsigned long tmp ;
  unsigned long __ms___0 ;
  unsigned long tmp___0 ;
  {
  falcon_stop_nic_stats(efx);
  reg = efx_mdio_read(efx, 1, 49152);
  reg = reg | 32768;
  efx_mdio_write(efx, 1, 49152, reg);
  __ms = 200UL;
  goto ldv_41711;
  ldv_41710:
  __const_udelay(4295000UL);
  ldv_41711:
  tmp = __ms;
  __ms = __ms - 1UL;
  if (tmp != 0UL) {
    goto ldv_41710;
  } else {
  }
  rc = efx_mdio_wait_reset_mmds(efx, 154U);
  if (rc < 0) {
    goto out;
  } else {
  }
  rc = tenxpress_init(efx);
  if (rc < 0) {
    goto out;
  } else {
  }
  __ms___0 = 10UL;
  goto ldv_41716;
  ldv_41715:
  __const_udelay(4295000UL);
  ldv_41716:
  tmp___0 = __ms___0;
  __ms___0 = __ms___0 - 1UL;
  if (tmp___0 != 0UL) {
    goto ldv_41715;
  } else {
  }
  out:
  falcon_start_nic_stats(efx);
  return (rc);
}
}
static void sfx7101_check_bad_lp(struct efx_nic *efx , bool link_ok )
{
  struct tenxpress_phy_data *pd ;
  bool bad_lp ;
  int reg ;
  {
  pd = (struct tenxpress_phy_data *)efx->phy_data;
  if ((int )link_ok) {
    bad_lp = 0;
  } else {
    reg = efx_mdio_read(efx, 7, 1);
    if ((reg & 1) == 0) {
      return;
    } else {
    }
    bad_lp = (reg & 32) == 0;
    if ((int )bad_lp) {
      pd->bad_lp_tries = pd->bad_lp_tries + 1;
    } else {
    }
  }
  if (pd->bad_lp_tries == 0) {
    return;
  } else {
  }
  if (! bad_lp || pd->bad_lp_tries == 5) {
    reg = efx_mdio_read(efx, 1, 49161);
    reg = reg & -193;
    if (! bad_lp) {
      reg = reg | 128;
    } else {
      reg = reg | 192;
      if ((efx->msg_enable & 4U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "appears to be plugged into a port that is not 10GBASE-T capable. The PHY supports 10GBASE-T ONLY, so no link can be established\n");
      } else {
      }
    }
    efx_mdio_write(efx, 1, 49161, reg);
    pd->bad_lp_tries = (int )bad_lp;
  } else {
  }
  return;
}
}
static bool sfx7101_link_ok(struct efx_nic *efx )
{
  bool tmp ;
  {
  tmp = efx_mdio_links_ok(efx, 26U);
  return (tmp);
}
}
static void tenxpress_ext_loopback(struct efx_nic *efx )
{
  {
  efx_mdio_set_flag(efx, 4, 49162, 256, (unsigned int )efx->loopback_mode == 15U);
  return;
}
}
static void tenxpress_low_power(struct efx_nic *efx )
{
  {
  efx_mdio_set_mmds_lpower(efx, ((unsigned int )efx->phy_mode & 2U) != 0U, 154U);
  return;
}
}
static int tenxpress_phy_reconfigure(struct efx_nic *efx )
{
  struct tenxpress_phy_data *phy_data ;
  bool phy_mode_change ;
  bool loop_reset ;
  {
  phy_data = (struct tenxpress_phy_data *)efx->phy_data;
  if (((unsigned int )efx->phy_mode & 12U) != 0U) {
    phy_data->phy_mode = efx->phy_mode;
    return (0);
  } else {
  }
  phy_mode_change = (bool )((unsigned int )efx->phy_mode == 0U && (unsigned int )phy_data->phy_mode != 0U);
  loop_reset = (bool )(((((u64 )(1 << (int )phy_data->loopback_mode) & efx->loopback_modes) & 0xfffffffffc07c000ULL) != 0ULL && (((u64 )(1 << (int )efx->loopback_mode) & efx->loopback_modes) & 0xfffffffffc07c000ULL) == 0ULL) || (((1 << (int )phy_data->loopback_mode) ^ (1 << (int )efx->loopback_mode)) & 16384) != 0);
  if ((int )loop_reset || (int )phy_mode_change) {
    tenxpress_special_reset(efx);
    falcon_reset_xaui(efx);
  } else {
  }
  tenxpress_low_power(efx);
  efx_mdio_transmit_disable(efx);
  efx_mdio_phy_reconfigure(efx);
  tenxpress_ext_loopback(efx);
  efx_mdio_an_reconfigure(efx);
  phy_data->loopback_mode = efx->loopback_mode;
  phy_data->phy_mode = efx->phy_mode;
  return (0);
}
}
static void tenxpress_get_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd ) ;
static bool tenxpress_phy_poll(struct efx_nic *efx )
{
  struct efx_link_state old_state ;
  bool tmp ;
  int tmp___0 ;
  {
  old_state = efx->link_state;
  efx->link_state.up = sfx7101_link_ok(efx);
  efx->link_state.speed = 10000U;
  efx->link_state.fd = 1;
  efx->link_state.fc = efx_mdio_get_pause(efx);
  sfx7101_check_bad_lp(efx, (int )efx->link_state.up);
  tmp = efx_link_state_equal((struct efx_link_state const *)(& efx->link_state),
                             (struct efx_link_state const *)(& old_state));
  if ((int )tmp != 0) {
    tmp___0 = 0;
  } else {
    tmp___0 = 1;
  }
  return ((bool )tmp___0);
}
}
static void sfx7101_phy_fini(struct efx_nic *efx )
{
  int reg ;
  {
  reg = 256;
  efx_mdio_write(efx, 1, 49152, reg);
  schedule_timeout_uninterruptible(50L);
  return;
}
}
static void tenxpress_phy_remove(struct efx_nic *efx )
{
  {
  kfree((void const *)efx->phy_data);
  efx->phy_data = 0;
  return;
}
}
void tenxpress_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  int reg ;
  {
  switch ((unsigned int )mode) {
  case 0U:
  reg = 162;
  goto ldv_41760;
  case 1U:
  reg = 81;
  goto ldv_41760;
  default:
  reg = 128;
  goto ldv_41760;
  }
  ldv_41760:
  efx_mdio_write(efx, 1, 49161, reg);
  return;
}
}
static char const * const sfx7101_test_names[1U] = { "bist"};
static char const *sfx7101_test_name(struct efx_nic *efx , unsigned int index )
{
  {
  if (index == 0U) {
    return ((char const *)sfx7101_test_names[index]);
  } else {
  }
  return (0);
}
}
static int sfx7101_run_tests(struct efx_nic *efx , int *results , unsigned int flags )
{
  int rc ;
  {
  if ((flags & 1U) == 0U) {
    return (0);
  } else {
  }
  rc = tenxpress_special_reset(efx);
  *results = rc != 0 ? -1 : 1;
  efx_mdio_an_reconfigure(efx);
  return (rc);
}
}
static void tenxpress_get_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  u32 adv ;
  u32 lpa ;
  int reg ;
  {
  adv = 0U;
  lpa = 0U;
  reg = efx_mdio_read(efx, 7, 32);
  if ((reg & 4096) != 0) {
    adv = adv | 4096U;
  } else {
  }
  reg = efx_mdio_read(efx, 7, 33);
  if ((reg & 2048) != 0) {
    lpa = lpa | 4096U;
  } else {
  }
  mdio45_ethtool_gset_npage((struct mdio_if_info const *)(& efx->mdio), ecmd, adv,
                            lpa);
  if ((((u64 )(1 << (int )efx->loopback_mode) & efx->loopback_modes) & 0xfffffffffc07c000ULL) != 0ULL) {
    ethtool_cmd_speed_set(ecmd, 10000U);
  } else {
  }
  return;
}
}
static int tenxpress_set_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  int tmp ;
  {
  if ((unsigned int )ecmd->autoneg == 0U) {
    return (-22);
  } else {
  }
  tmp = efx_mdio_set_settings(efx, ecmd);
  return (tmp);
}
}
static void sfx7101_set_npage_adv(struct efx_nic *efx , u32 advertising )
{
  {
  efx_mdio_set_flag(efx, 7, 32, 4096, (advertising & 4096U) != 0U);
  return;
}
}
struct efx_phy_operations const falcon_sfx7101_phy_ops =
     {& tenxpress_phy_probe, & tenxpress_phy_init, & sfx7101_phy_fini, & tenxpress_phy_remove,
    & tenxpress_phy_reconfigure, & tenxpress_phy_poll, & tenxpress_get_settings, & tenxpress_set_settings,
    & sfx7101_set_npage_adv, & efx_mdio_test_alive, & sfx7101_test_name, & sfx7101_run_tests,
    0, 0};
void ldv_main13_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  int res_tenxpress_phy_probe_1 ;
  struct ethtool_cmd *var_group2 ;
  u32 var_sfx7101_set_npage_adv_17_p1 ;
  unsigned int var_sfx7101_test_name_13_p1 ;
  int *var_sfx7101_run_tests_14_p1 ;
  unsigned int var_sfx7101_run_tests_14_p2 ;
  int ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_41831;
  ldv_41830:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations == 0) {
    res_tenxpress_phy_probe_1 = tenxpress_phy_probe(var_group1);
    ldv_check_return_value(res_tenxpress_phy_probe_1);
    ldv_check_return_value_probe(res_tenxpress_phy_probe_1);
    if (res_tenxpress_phy_probe_1 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations = ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations + 1;
  } else {
  }
  goto ldv_41818;
  case 1: ;
  if (ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations == 1) {
    ldv_handler_precall();
    tenxpress_phy_remove(var_group1);
    ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations = 0;
  } else {
  }
  goto ldv_41818;
  case 2:
  ldv_handler_precall();
  tenxpress_phy_init(var_group1);
  goto ldv_41818;
  case 3:
  ldv_handler_precall();
  tenxpress_phy_reconfigure(var_group1);
  goto ldv_41818;
  case 4:
  ldv_handler_precall();
  tenxpress_phy_poll(var_group1);
  goto ldv_41818;
  case 5:
  ldv_handler_precall();
  sfx7101_phy_fini(var_group1);
  goto ldv_41818;
  case 6:
  ldv_handler_precall();
  tenxpress_get_settings(var_group1, var_group2);
  goto ldv_41818;
  case 7:
  ldv_handler_precall();
  tenxpress_set_settings(var_group1, var_group2);
  goto ldv_41818;
  case 8:
  ldv_handler_precall();
  sfx7101_set_npage_adv(var_group1, var_sfx7101_set_npage_adv_17_p1);
  goto ldv_41818;
  case 9:
  ldv_handler_precall();
  sfx7101_test_name(var_group1, var_sfx7101_test_name_13_p1);
  goto ldv_41818;
  case 10:
  ldv_handler_precall();
  sfx7101_run_tests(var_group1, var_sfx7101_run_tests_14_p1, var_sfx7101_run_tests_14_p2);
  goto ldv_41818;
  default: ;
  goto ldv_41818;
  }
  ldv_41818: ;
  ldv_41831:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0 || ldv_s_falcon_sfx7101_phy_ops_efx_phy_operations != 0) {
    goto ldv_41830;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_307(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_308(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_309(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_310(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_311(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_312(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_313(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_324(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_322(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_325(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_327(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_321(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_323(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_326(struct mutex *ldv_func_arg1 ) ;
void falcon_txc_set_gpio_dir(struct efx_nic *efx , int pin , int dir ) ;
void falcon_txc_set_gpio_val(struct efx_nic *efx , int pin , int on ) ;
static void txc_reset_logic(struct efx_nic *efx ) ;
void falcon_txc_set_gpio_val(struct efx_nic *efx , int pin , int on )
{
  {
  efx_mdio_set_flag(efx, 4, 49990, 1 << pin, on != 0);
  return;
}
}
void falcon_txc_set_gpio_dir(struct efx_nic *efx , int pin , int dir )
{
  {
  efx_mdio_set_flag(efx, 4, 49992, 1 << pin, dir != 0);
  return;
}
}
static int txc_reset_phy(struct efx_nic *efx )
{
  int rc ;
  int tmp ;
  {
  tmp = efx_mdio_reset_mmd(efx, 1, 50, 10);
  rc = tmp;
  if (rc < 0) {
    goto fail;
  } else {
  }
  rc = efx_mdio_check_mmds(efx, 26U);
  if (rc < 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "TXC43128: reset timed out!\n");
  } else {
  }
  return (rc);
}
}
static int txc_bist_one(struct efx_nic *efx , int mmd , int test )
{
  int ctrl ;
  int bctl ;
  int lane ;
  int rc ;
  int count ;
  int tmp ;
  {
  rc = 0;
  ctrl = efx_mdio_read(efx, 3, 49999);
  ctrl = ctrl | 1024;
  efx_mdio_write(efx, 3, 49999, ctrl);
  bctl = test << 10;
  efx_mdio_write(efx, mmd, 49792, bctl);
  bctl = bctl | 8192;
  efx_mdio_write(efx, mmd, 49792, bctl);
  efx_mdio_write(efx, mmd, 49792, bctl | 32768);
  __const_udelay(214750UL);
  bctl = bctl | 16384;
  efx_mdio_write(efx, mmd, 49792, bctl);
  goto ldv_41720;
  ldv_41719:
  bctl = efx_mdio_read(efx, mmd, 49792);
  ldv_41720: ;
  if ((bctl & 16384) != 0) {
    goto ldv_41719;
  } else {
  }
  lane = 0;
  goto ldv_41724;
  ldv_41723:
  tmp = efx_mdio_read(efx, mmd, lane + 49798);
  count = tmp;
  if (count != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "TXC43128: BIST error. Lane %d had %d errs\n",
                 lane, count);
    } else {
    }
    rc = -5;
  } else {
  }
  count = efx_mdio_read(efx, mmd, lane + 49794);
  if (count == 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "TXC43128: BIST error. Lane %d got 0 frames\n",
                 lane);
    } else {
    }
    rc = -5;
  } else {
  }
  lane = lane + 1;
  ldv_41724: ;
  if (lane <= 3) {
    goto ldv_41723;
  } else {
  }
  if (rc == 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "TXC43128: BIST pass\n");
    } else {
    }
  } else {
  }
  efx_mdio_write(efx, mmd, 49792, 0);
  ctrl = ctrl & -1025;
  efx_mdio_write(efx, 3, 49999, ctrl);
  return (rc);
}
}
static int txc_bist(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = txc_bist_one(efx, 3, 0);
  return (tmp);
}
}
static void txc_apply_defaults(struct efx_nic *efx )
{
  int mctrl ;
  struct falcon_board *tmp ;
  {
  efx_mdio_write(efx, 4, 49219, 0);
  efx_mdio_write(efx, 4, 49220, 0);
  efx_mdio_write(efx, 4, 49217, 51400);
  efx_mdio_write(efx, 4, 49218, 51400);
  efx_mdio_write(efx, 1, 49219, 4112);
  efx_mdio_write(efx, 1, 49220, 4112);
  efx_mdio_write(efx, 1, 49217, 24672);
  efx_mdio_write(efx, 1, 49218, 24672);
  mctrl = efx_mdio_read(efx, 4, 49984);
  mctrl = mctrl & -24577;
  efx_mdio_write(efx, 4, 49984, mctrl);
  txc_reset_logic(efx);
  tmp = falcon_board(efx);
  (*((tmp->type)->init_phy))(efx);
  return;
}
}
static int txc43128_phy_probe(struct efx_nic *efx )
{
  struct txc43128_data *phy_data ;
  void *tmp ;
  {
  tmp = kzalloc(16UL, 208U);
  phy_data = (struct txc43128_data *)tmp;
  if ((unsigned long )phy_data == (unsigned long )((struct txc43128_data *)0)) {
    return (-12);
  } else {
  }
  efx->phy_data = (void *)phy_data;
  phy_data->phy_mode = efx->phy_mode;
  efx->mdio.mmds = 26U;
  efx->mdio.mode_support = 6U;
  efx->loopback_modes = 67305528ULL;
  return (0);
}
}
static int txc43128_phy_init(struct efx_nic *efx )
{
  int rc ;
  {
  rc = txc_reset_phy(efx);
  if (rc < 0) {
    return (rc);
  } else {
  }
  rc = txc_bist(efx);
  if (rc < 0) {
    return (rc);
  } else {
  }
  txc_apply_defaults(efx);
  return (0);
}
}
static void txc_glrgs_lane_power(struct efx_nic *efx , int mmd )
{
  int pd ;
  int ctl ;
  int tmp ;
  {
  pd = 96;
  tmp = efx_mdio_read(efx, mmd, 49156);
  ctl = tmp;
  if (((unsigned int )efx->phy_mode & 2U) == 0U) {
    ctl = ~ pd & ctl;
  } else {
    ctl = ctl | pd;
  }
  efx_mdio_write(efx, mmd, 49156, ctl);
  return;
}
}
static void txc_analog_lane_power(struct efx_nic *efx , int mmd )
{
  int txpd ;
  int rxpd ;
  int txctl ;
  int tmp ;
  int rxctl ;
  int tmp___0 ;
  {
  txpd = 61440;
  rxpd = 61440;
  tmp = efx_mdio_read(efx, mmd, 49216);
  txctl = tmp;
  tmp___0 = efx_mdio_read(efx, mmd, 49221);
  rxctl = tmp___0;
  if (((unsigned int )efx->phy_mode & 2U) == 0U) {
    txctl = ~ txpd & txctl;
    rxctl = ~ rxpd & rxctl;
  } else {
    txctl = txctl | txpd;
    rxctl = rxctl | rxpd;
  }
  efx_mdio_write(efx, mmd, 49216, txctl);
  efx_mdio_write(efx, mmd, 49221, rxctl);
  return;
}
}
static void txc_set_power(struct efx_nic *efx )
{
  {
  efx_mdio_set_mmds_lpower(efx, ((unsigned int )efx->phy_mode & 2U) != 0U, 26U);
  txc_glrgs_lane_power(efx, 3);
  txc_glrgs_lane_power(efx, 4);
  txc_analog_lane_power(efx, 1);
  txc_analog_lane_power(efx, 4);
  return;
}
}
static void txc_reset_logic_mmd(struct efx_nic *efx , int mmd )
{
  int val ;
  int tmp ;
  int tries ;
  int tmp___0 ;
  {
  tmp = efx_mdio_read(efx, mmd, 49156);
  val = tmp;
  tries = 50;
  val = val | 16384;
  efx_mdio_write(efx, mmd, 49156, val);
  goto ldv_41766;
  ldv_41765:
  val = efx_mdio_read(efx, mmd, 49156);
  if ((val & 16384) == 0) {
    goto ldv_41764;
  } else {
  }
  __const_udelay(4295UL);
  ldv_41766:
  tmp___0 = tries;
  tries = tries - 1;
  if (tmp___0 != 0) {
    goto ldv_41765;
  } else {
  }
  ldv_41764: ;
  if (tries == 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "TXC43128 Logic reset timed out!\n");
    } else {
    }
  } else {
  }
  return;
}
}
static void txc_reset_logic(struct efx_nic *efx )
{
  {
  txc_reset_logic_mmd(efx, 3);
  return;
}
}
static bool txc43128_phy_read_link(struct efx_nic *efx )
{
  bool tmp ;
  {
  tmp = efx_mdio_links_ok(efx, 26U);
  return (tmp);
}
}
static int txc43128_phy_reconfigure(struct efx_nic *efx )
{
  struct txc43128_data *phy_data ;
  enum efx_phy_mode mode_change ;
  bool loop_change ;
  {
  phy_data = (struct txc43128_data *)efx->phy_data;
  mode_change = (unsigned int )efx->phy_mode ^ (unsigned int )phy_data->phy_mode;
  loop_change = (((1 << (int )phy_data->loopback_mode) ^ (1 << (int )efx->loopback_mode)) & 67305472) != 0;
  if ((int )((unsigned int )efx->phy_mode & (unsigned int )mode_change) & 1) {
    txc_reset_phy(efx);
    txc_apply_defaults(efx);
    falcon_reset_xaui(efx);
    mode_change = (enum efx_phy_mode )((unsigned int )mode_change & 4294967294U);
  } else {
  }
  efx_mdio_transmit_disable(efx);
  efx_mdio_phy_reconfigure(efx);
  if (((unsigned int )mode_change & 2U) != 0U) {
    txc_set_power(efx);
  } else {
  }
  if ((int )loop_change || (unsigned int )mode_change != 0U) {
    txc_reset_logic(efx);
  } else {
  }
  phy_data->phy_mode = efx->phy_mode;
  phy_data->loopback_mode = efx->loopback_mode;
  return (0);
}
}
static void txc43128_phy_fini(struct efx_nic *efx )
{
  {
  efx_mdio_write(efx, 1, 36866, 0);
  return;
}
}
static void txc43128_phy_remove(struct efx_nic *efx )
{
  {
  kfree((void const *)efx->phy_data);
  efx->phy_data = 0;
  return;
}
}
static bool txc43128_phy_poll(struct efx_nic *efx )
{
  struct txc43128_data *data ;
  bool was_up ;
  {
  data = (struct txc43128_data *)efx->phy_data;
  was_up = efx->link_state.up;
  efx->link_state.up = txc43128_phy_read_link(efx);
  efx->link_state.speed = 10000U;
  efx->link_state.fd = 1;
  efx->link_state.fc = efx->wanted_fc;
  if ((int )efx->link_state.up || (unsigned int )efx->loopback_mode != 0U) {
    data->bug10934_timer = jiffies;
  } else
  if ((long )jiffies - (long )(data->bug10934_timer + 1250UL) >= 0L) {
    data->bug10934_timer = jiffies;
    txc_reset_logic(efx);
  } else {
  }
  return ((int )efx->link_state.up != (int )was_up);
}
}
static char const * const txc43128_test_names[1U] = { "bist"};
static char const *txc43128_test_name(struct efx_nic *efx , unsigned int index )
{
  {
  if (index == 0U) {
    return ((char const *)txc43128_test_names[index]);
  } else {
  }
  return (0);
}
}
static int txc43128_run_tests(struct efx_nic *efx , int *results , unsigned int flags )
{
  int rc ;
  {
  if ((flags & 1U) == 0U) {
    return (0);
  } else {
  }
  rc = txc_reset_phy(efx);
  if (rc < 0) {
    return (rc);
  } else {
  }
  rc = txc_bist(efx);
  txc_apply_defaults(efx);
  *results = rc != 0 ? -1 : 1;
  return (rc);
}
}
static void txc43128_get_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  {
  mdio45_ethtool_gset((struct mdio_if_info const *)(& efx->mdio), ecmd);
  return;
}
}
struct efx_phy_operations const falcon_txc_phy_ops =
     {& txc43128_phy_probe, & txc43128_phy_init, & txc43128_phy_fini, & txc43128_phy_remove,
    & txc43128_phy_reconfigure, & txc43128_phy_poll, & txc43128_get_settings, & efx_mdio_set_settings,
    0, & efx_mdio_test_alive, & txc43128_test_name, & txc43128_run_tests, 0, 0};
void ldv_main14_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  int res_txc43128_phy_probe_6 ;
  struct ethtool_cmd *var_group2 ;
  int *var_txc43128_run_tests_19_p1 ;
  unsigned int var_txc43128_run_tests_19_p2 ;
  unsigned int var_txc43128_test_name_18_p1 ;
  int ldv_s_falcon_txc_phy_ops_efx_phy_operations ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_falcon_txc_phy_ops_efx_phy_operations = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_41850;
  ldv_41849:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_s_falcon_txc_phy_ops_efx_phy_operations == 0) {
    res_txc43128_phy_probe_6 = txc43128_phy_probe(var_group1);
    ldv_check_return_value(res_txc43128_phy_probe_6);
    ldv_check_return_value_probe(res_txc43128_phy_probe_6);
    if (res_txc43128_phy_probe_6 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_falcon_txc_phy_ops_efx_phy_operations = ldv_s_falcon_txc_phy_ops_efx_phy_operations + 1;
  } else {
  }
  goto ldv_41839;
  case 1: ;
  if (ldv_s_falcon_txc_phy_ops_efx_phy_operations == 1) {
    ldv_handler_precall();
    txc43128_phy_remove(var_group1);
    ldv_s_falcon_txc_phy_ops_efx_phy_operations = 0;
  } else {
  }
  goto ldv_41839;
  case 2:
  ldv_handler_precall();
  txc43128_phy_init(var_group1);
  goto ldv_41839;
  case 3:
  ldv_handler_precall();
  txc43128_phy_reconfigure(var_group1);
  goto ldv_41839;
  case 4:
  ldv_handler_precall();
  txc43128_phy_poll(var_group1);
  goto ldv_41839;
  case 5:
  ldv_handler_precall();
  txc43128_phy_fini(var_group1);
  goto ldv_41839;
  case 6:
  ldv_handler_precall();
  txc43128_get_settings(var_group1, var_group2);
  goto ldv_41839;
  case 7:
  ldv_handler_precall();
  txc43128_run_tests(var_group1, var_txc43128_run_tests_19_p1, var_txc43128_run_tests_19_p2);
  goto ldv_41839;
  case 8:
  ldv_handler_precall();
  txc43128_test_name(var_group1, var_txc43128_test_name_18_p1);
  goto ldv_41839;
  default: ;
  goto ldv_41839;
  }
  ldv_41839: ;
  ldv_41850:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0 || ldv_s_falcon_txc_phy_ops_efx_phy_operations != 0) {
    goto ldv_41849;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_321(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_322(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_323(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_324(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_325(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_326(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_327(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_338(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_336(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_339(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_341(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_335(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_337(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_340(struct mutex *ldv_func_arg1 ) ;
extern s32 i2c_smbus_read_byte_data(struct i2c_client const * , u8 ) ;
extern s32 i2c_smbus_write_byte_data(struct i2c_client const * , u8 , u8 ) ;
extern struct i2c_client *i2c_new_device(struct i2c_adapter * , struct i2c_board_info const * ) ;
extern struct i2c_client *i2c_new_dummy(struct i2c_adapter * , u16 ) ;
extern void i2c_unregister_device(struct i2c_client * ) ;
static int efx_poke_lm87(struct i2c_client *client , u8 const *reg_values )
{
  u8 reg ;
  u8 const *tmp ;
  u8 value ;
  u8 const *tmp___0 ;
  int rc ;
  s32 tmp___1 ;
  {
  goto ldv_41621;
  ldv_41620:
  tmp = reg_values;
  reg_values = reg_values + 1;
  reg = *tmp;
  tmp___0 = reg_values;
  reg_values = reg_values + 1;
  value = *tmp___0;
  tmp___1 = i2c_smbus_write_byte_data((struct i2c_client const *)client, (int )reg,
                                      (int )value);
  rc = tmp___1;
  if (rc != 0) {
    return (rc);
  } else {
  }
  ldv_41621: ;
  if ((unsigned int )((unsigned char )*reg_values) != 0U) {
    goto ldv_41620;
  } else {
  }
  return (0);
}
}
static u8 const falcon_lm87_common_regs[13U] =
  { 19U, 95U, 23U, 95U,
        55U, 90U, 56U, 0U,
        20U, 125U, 24U, 125U,
        0U};
static int efx_init_lm87(struct efx_nic *efx , struct i2c_board_info const *info ,
                         u8 const *reg_values )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  struct i2c_client *client ;
  struct i2c_client *tmp___0 ;
  int rc ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  tmp___0 = i2c_new_device(& board->i2c_adap, info);
  client = tmp___0;
  if ((unsigned long )client == (unsigned long )((struct i2c_client *)0)) {
    return (-5);
  } else {
  }
  i2c_smbus_read_byte_data((struct i2c_client const *)client, 65);
  i2c_smbus_read_byte_data((struct i2c_client const *)client, 66);
  rc = efx_poke_lm87(client, reg_values);
  if (rc != 0) {
    goto err;
  } else {
  }
  rc = efx_poke_lm87(client, (u8 const *)(& falcon_lm87_common_regs));
  if (rc != 0) {
    goto err;
  } else {
  }
  board->hwmon_client = client;
  return (0);
  err:
  i2c_unregister_device(client);
  return (rc);
}
}
static void efx_fini_lm87(struct efx_nic *efx )
{
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  i2c_unregister_device(tmp->hwmon_client);
  return;
}
}
static int efx_check_lm87(struct efx_nic *efx , unsigned int mask )
{
  struct i2c_client *client ;
  struct falcon_board *tmp ;
  bool temp_crit ;
  bool elec_fault ;
  bool is_failure ;
  u16 alarms ;
  s32 reg ;
  {
  tmp = falcon_board(efx);
  client = tmp->hwmon_client;
  if ((int )efx->link_state.up) {
    return (0);
  } else {
  }
  reg = i2c_smbus_read_byte_data((struct i2c_client const *)client, 65);
  if (reg < 0) {
    return (reg);
  } else {
  }
  alarms = (u16 )reg;
  reg = i2c_smbus_read_byte_data((struct i2c_client const *)client, 66);
  if (reg < 0) {
    return (reg);
  } else {
  }
  alarms = (u16 )((int )((short )(reg << 8)) | (int )((short )alarms));
  alarms = (int )((u16 )mask) & (int )alarms;
  temp_crit = 0;
  if (((int )alarms & 16) != 0) {
    reg = i2c_smbus_read_byte_data((struct i2c_client const *)client, 39);
    if (reg < 0) {
      return (reg);
    } else {
    }
    if (reg > 95) {
      temp_crit = 1;
    } else {
    }
  } else {
  }
  if (((int )alarms & 32) != 0) {
    reg = i2c_smbus_read_byte_data((struct i2c_client const *)client, 38);
    if (reg < 0) {
      return (reg);
    } else {
    }
    if (reg > 125) {
      temp_crit = 1;
    } else {
    }
  } else {
  }
  elec_fault = ((int )alarms & -49) != 0;
  is_failure = (bool )((int )temp_crit || (int )elec_fault);
  if ((unsigned int )alarms != 0U) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "LM87 detected a hardware %s (status %02x:%02x)%s%s%s%s\n",
                 (int )is_failure ? (char *)"failure" : (char *)"problem", (int )alarms & 255,
                 (int )alarms >> 8, ((int )alarms & 16) != 0 ? (char *)"; board is overheating" : (char *)"",
                 ((int )alarms & 32) != 0 ? (char *)"; controller is overheating" : (char *)"",
                 (int )temp_crit ? (char *)"; reached critical temperature" : (char *)"",
                 (int )elec_fault ? (char *)"; electrical fault" : (char *)"");
    } else {
    }
  } else {
  }
  return ((int )is_failure ? -34 : 0);
}
}
static void sfe4001_poweroff(struct efx_nic *efx )
{
  struct i2c_client *ioexp_client ;
  struct falcon_board *tmp ;
  struct i2c_client *hwmon_client ;
  struct falcon_board *tmp___0 ;
  {
  tmp = falcon_board(efx);
  ioexp_client = tmp->ioexp_client;
  tmp___0 = falcon_board(efx);
  hwmon_client = tmp___0->hwmon_client;
  i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 2, 255);
  i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 7, 255);
  i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 6, 255);
  i2c_smbus_read_byte_data((struct i2c_client const *)hwmon_client, 2);
  return;
}
}
static int sfe4001_poweron(struct efx_nic *efx )
{
  struct i2c_client *ioexp_client ;
  struct falcon_board *tmp ;
  struct i2c_client *hwmon_client ;
  struct falcon_board *tmp___0 ;
  unsigned int i ;
  unsigned int j ;
  int rc ;
  u8 out ;
  {
  tmp = falcon_board(efx);
  ioexp_client = tmp->ioexp_client;
  tmp___0 = falcon_board(efx);
  hwmon_client = tmp___0->hwmon_client;
  rc = i2c_smbus_read_byte_data((struct i2c_client const *)hwmon_client, 2);
  if (rc < 0) {
    return (rc);
  } else {
  }
  rc = i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 6, 0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 7, 239);
  if (rc != 0) {
    goto fail_on;
  } else {
  }
  rc = i2c_smbus_read_byte_data((struct i2c_client const *)ioexp_client, 2);
  if (rc < 0) {
    goto fail_on;
  } else {
  }
  out = 255U;
  if ((int )out != rc) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_info((struct net_device const *)efx->net_dev, "power-cycling PHY\n");
    } else {
    }
    rc = i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 2, (int )out);
    if (rc != 0) {
      goto fail_on;
    } else {
    }
    schedule_timeout_uninterruptible(250L);
  } else {
  }
  i = 0U;
  goto ldv_41665;
  ldv_41664:
  out = 161U;
  if (((unsigned int )efx->phy_mode & 8U) != 0U) {
    out = (u8 )((unsigned int )out | 8U);
  } else {
  }
  rc = i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 2, (int )out);
  if (rc != 0) {
    goto fail_on;
  } else {
  }
  msleep(10U);
  out = (unsigned int )out & 254U;
  rc = i2c_smbus_write_byte_data((struct i2c_client const *)ioexp_client, 2, (int )out);
  if (rc != 0) {
    goto fail_on;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "waiting for DSP boot (attempt %d)...\n",
                i);
  } else {
  }
  if (((unsigned int )efx->phy_mode & 8U) != 0U) {
    schedule_timeout_uninterruptible(250L);
    return (0);
  } else {
  }
  j = 0U;
  goto ldv_41662;
  ldv_41661:
  msleep(100U);
  rc = i2c_smbus_read_byte_data((struct i2c_client const *)ioexp_client, 1);
  if (rc < 0) {
    goto fail_on;
  } else {
  }
  if (rc & 1) {
    return (0);
  } else {
  }
  j = j + 1U;
  ldv_41662: ;
  if (j <= 9U) {
    goto ldv_41661;
  } else {
  }
  i = i + 1U;
  ldv_41665: ;
  if (i <= 19U) {
    goto ldv_41664;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "timed out waiting for DSP boot\n");
  } else {
  }
  rc = -110;
  fail_on:
  sfe4001_poweroff(efx);
  return (rc);
}
}
static ssize_t show_phy_flash_cfg(struct device *dev , struct device_attribute *attr ,
                                  char *buf )
{
  struct efx_nic *efx ;
  struct device const *__mptr ;
  void *tmp ;
  int tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  tmp = pci_get_drvdata((struct pci_dev *)__mptr + 0xffffffffffffff68UL);
  efx = (struct efx_nic *)tmp;
  tmp___0 = sprintf(buf, "%d\n", ((unsigned int )efx->phy_mode & 8U) != 0U);
  return ((ssize_t )tmp___0);
}
}
static ssize_t set_phy_flash_cfg(struct device *dev , struct device_attribute *attr ,
                                 char const *buf , size_t count )
{
  struct efx_nic *efx ;
  struct device const *__mptr ;
  void *tmp ;
  enum efx_phy_mode old_mode ;
  enum efx_phy_mode new_mode ;
  int err ;
  bool tmp___0 ;
  {
  __mptr = (struct device const *)dev;
  tmp = pci_get_drvdata((struct pci_dev *)__mptr + 0xffffffffffffff68UL);
  efx = (struct efx_nic *)tmp;
  rtnl_lock();
  old_mode = efx->phy_mode;
  if (count == 0UL || (int )((signed char )*buf) == 48) {
    new_mode = (enum efx_phy_mode )((unsigned int )old_mode & 4294967287U);
  } else {
    new_mode = 8;
  }
  if ((((unsigned int )old_mode ^ (unsigned int )new_mode) & 8U) == 0U) {
    err = 0;
  } else
  if ((unsigned int )efx->state != 1U) {
    err = -16;
  } else {
    tmp___0 = netif_running((struct net_device const *)efx->net_dev);
    if ((int )tmp___0) {
      err = -16;
    } else {
      efx->phy_mode = new_mode;
      if (((unsigned int )new_mode & 8U) != 0U) {
        falcon_stop_nic_stats(efx);
      } else {
      }
      err = sfe4001_poweron(efx);
      if (err == 0) {
        err = efx_reconfigure_port(efx);
      } else {
      }
      if (((unsigned int )new_mode & 8U) == 0U) {
        falcon_start_nic_stats(efx);
      } else {
      }
    }
  }
  rtnl_unlock();
  return ((ssize_t )(err != 0 ? (size_t )err : count));
}
}
static struct device_attribute dev_attr_phy_flash_cfg = {{"phy_flash_cfg", 420U, (_Bool)0, 0, {{{(char)0}, {(char)0}, {(char)0}, {(char)0},
                                           {(char)0}, {(char)0}, {(char)0}, {(char)0}}}},
    & show_phy_flash_cfg, & set_phy_flash_cfg};
static void sfe4001_fini(struct efx_nic *efx )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  if ((int )efx->msg_enable & 1) {
    netdev_info((struct net_device const *)efx->net_dev, "%s\n", "sfe4001_fini");
  } else {
  }
  device_remove_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& dev_attr_phy_flash_cfg));
  sfe4001_poweroff(efx);
  i2c_unregister_device(board->ioexp_client);
  i2c_unregister_device(board->hwmon_client);
  return;
}
}
static int sfe4001_check_hw(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  s32 status ;
  struct falcon_board *tmp ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  if (! nic_data->xmac_poll_required) {
    return (0);
  } else {
  }
  tmp = falcon_board(efx);
  status = i2c_smbus_read_byte_data((struct i2c_client const *)tmp->ioexp_client,
                                    1);
  if (status >= 0 && (status & 3) != 0) {
    return (0);
  } else {
  }
  sfe4001_poweroff(efx);
  efx->phy_mode = 4;
  return (status < 0 ? -5 : -34);
}
}
static struct i2c_board_info const sfe4001_hwmon_info =
     {{'m', 'a', 'x', '6', '6', '4', '7', '\000'}, (unsigned short)0, 78U, 0, 0, 0,
    {0}, 0};
static int sfe4001_init(struct efx_nic *efx )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  int rc ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  board->hwmon_client = i2c_new_device(& board->i2c_adap, & sfe4001_hwmon_info);
  if ((unsigned long )board->hwmon_client == (unsigned long )((struct i2c_client *)0)) {
    return (-5);
  } else {
  }
  rc = i2c_smbus_write_byte_data((struct i2c_client const *)board->hwmon_client,
                                 11, 90);
  if (rc != 0) {
    goto fail_hwmon;
  } else {
  }
  board->ioexp_client = i2c_new_dummy(& board->i2c_adap, 116);
  if ((unsigned long )board->ioexp_client == (unsigned long )((struct i2c_client *)0)) {
    rc = -5;
    goto fail_hwmon;
  } else {
  }
  if (((unsigned int )efx->phy_mode & 8U) != 0U) {
    falcon_stop_nic_stats(efx);
  } else {
  }
  rc = sfe4001_poweron(efx);
  if (rc != 0) {
    goto fail_ioexp;
  } else {
  }
  rc = device_create_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& dev_attr_phy_flash_cfg));
  if (rc != 0) {
    goto fail_on;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "PHY is powered on\n");
  } else {
  }
  return (0);
  fail_on:
  sfe4001_poweroff(efx);
  fail_ioexp:
  i2c_unregister_device(board->ioexp_client);
  fail_hwmon:
  i2c_unregister_device(board->hwmon_client);
  return (rc);
}
}
static u8 sfe4002_lm87_channel = 3U;
static u8 const sfe4002_lm87_regs[41U] =
  { 43U, 153U, 44U, 124U,
        45U, 94U, 46U, 76U,
        47U, 212U, 48U, 172U,
        49U, 212U, 50U, 172U,
        51U, 224U, 52U, 172U,
        53U, 79U, 54U, 63U,
        59U, 187U, 26U, 152U,
        60U, 169U, 27U, 138U,
        57U, 95U, 58U, 0U,
        55U, 90U, 56U, 0U,
        0U};
static struct i2c_board_info const sfe4002_hwmon_info =
     {{'l', 'm', '8', '7', '\000'}, (unsigned short)0, 46U, (void *)(& sfe4002_lm87_channel),
    0, 0, {0}, 0};
static void sfe4002_init_phy(struct efx_nic *efx )
{
  {
  falcon_qt202x_set_led(efx, 1, 3);
  falcon_qt202x_set_led(efx, 0, 11);
  falcon_qt202x_set_led(efx, 2, 4);
  return;
}
}
static void sfe4002_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  {
  falcon_qt202x_set_led(efx, 2, (unsigned int )mode == 1U ? 5 : 4);
  return;
}
}
static int sfe4002_check_hw(struct efx_nic *efx )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  unsigned int alarm_mask ;
  int tmp___0 ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  alarm_mask = board->major == 0 && board->minor == 0 ? 4294967263U : 4294967295U;
  tmp___0 = efx_check_lm87(efx, alarm_mask);
  return (tmp___0);
}
}
static int sfe4002_init(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_init_lm87(efx, & sfe4002_hwmon_info, (u8 const *)(& sfe4002_lm87_regs));
  return (tmp);
}
}
static u8 sfn4112f_lm87_channel = 3U;
static u8 const sfn4112f_lm87_regs[33U] =
  { 43U, 153U, 44U, 124U,
        45U, 94U, 46U, 76U,
        47U, 212U, 48U, 172U,
        51U, 224U, 52U, 172U,
        53U, 79U, 54U, 63U,
        60U, 169U, 27U, 138U,
        57U, 75U, 58U, 0U,
        55U, 90U, 56U, 0U,
        0U};
static struct i2c_board_info const sfn4112f_hwmon_info =
     {{'l', 'm', '8', '7', '\000'}, (unsigned short)0, 46U, (void *)(& sfn4112f_lm87_channel),
    0, 0, {0}, 0};
static void sfn4112f_init_phy(struct efx_nic *efx )
{
  {
  falcon_qt202x_set_led(efx, 0, 10);
  falcon_qt202x_set_led(efx, 1, 9);
  return;
}
}
static void sfn4112f_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  int reg ;
  {
  switch ((unsigned int )mode) {
  case 0U:
  reg = 4;
  goto ldv_41737;
  case 1U:
  reg = 5;
  goto ldv_41737;
  default:
  reg = 9;
  goto ldv_41737;
  }
  ldv_41737:
  falcon_qt202x_set_led(efx, 1, reg);
  return;
}
}
static int sfn4112f_check_hw(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_check_lm87(efx, 4294967223U);
  return (tmp);
}
}
static int sfn4112f_init(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_init_lm87(efx, & sfn4112f_hwmon_info, (u8 const *)(& sfn4112f_lm87_regs));
  return (tmp);
}
}
static u8 sfe4003_lm87_channel = 3U;
static u8 const sfe4003_lm87_regs[25U] =
  { 43U, 127U, 44U, 103U,
        45U, 94U, 46U, 76U,
        47U, 212U, 48U, 172U,
        51U, 224U, 52U, 172U,
        53U, 79U, 54U, 63U,
        57U, 85U, 58U, 0U,
        0U};
static struct i2c_board_info const sfe4003_hwmon_info =
     {{'l', 'm', '8', '7', '\000'}, (unsigned short)0, 46U, (void *)(& sfe4003_lm87_channel),
    0, 0, {0}, 0};
static void sfe4003_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  if (board->minor <= 2 && board->major == 0) {
    return;
  } else {
  }
  falcon_txc_set_gpio_val(efx, 11, (unsigned int )mode == 1U);
  return;
}
}
static void sfe4003_init_phy(struct efx_nic *efx )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  if (board->minor <= 2 && board->major == 0) {
    return;
  } else {
  }
  falcon_txc_set_gpio_dir(efx, 11, 1);
  falcon_txc_set_gpio_val(efx, 11, 0);
  return;
}
}
static int sfe4003_check_hw(struct efx_nic *efx )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  unsigned int alarm_mask ;
  int tmp___0 ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  alarm_mask = board->major == 0 && board->minor <= 2 ? 4294967263U : 4294967295U;
  tmp___0 = efx_check_lm87(efx, alarm_mask);
  return (tmp___0);
}
}
static int sfe4003_init(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_init_lm87(efx, & sfe4003_hwmon_info, (u8 const *)(& sfe4003_lm87_regs));
  return (tmp);
}
}
static struct falcon_board_type const board_types[4U] = { {1U, & sfe4001_init, & efx_port_dummy_op_void, & sfe4001_fini, & tenxpress_set_id_led,
      & sfe4001_check_hw},
        {2U, & sfe4002_init, & sfe4002_init_phy, & efx_fini_lm87, & sfe4002_set_id_led,
      & sfe4002_check_hw},
        {3U, & sfe4003_init, & sfe4003_init_phy, & efx_fini_lm87, & sfe4003_set_id_led,
      & sfe4003_check_hw},
        {82U, & sfn4112f_init, & sfn4112f_init_phy, & efx_fini_lm87, & sfn4112f_set_id_led,
      & sfn4112f_check_hw}};
int falcon_probe_board(struct efx_nic *efx , u16 revision_info )
{
  struct falcon_board *board ;
  struct falcon_board *tmp ;
  u8 type_id ;
  int i ;
  {
  tmp = falcon_board(efx);
  board = tmp;
  type_id = (u8 )((int )revision_info >> 8);
  board->major = ((int )revision_info >> 4) & 15;
  board->minor = (int )revision_info & 15;
  i = 0;
  goto ldv_41777;
  ldv_41776: ;
  if ((int )((unsigned char )board_types[i].id) == (int )type_id) {
    board->type = (struct falcon_board_type const *)(& board_types) + (unsigned long )i;
  } else {
  }
  i = i + 1;
  ldv_41777: ;
  if ((unsigned int )i <= 3U) {
    goto ldv_41776;
  } else {
  }
  if ((unsigned long )board->type != (unsigned long )((struct falcon_board_type const *)0)) {
    return (0);
  } else {
    if ((efx->msg_enable & 2U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "unknown board type %d\n",
                 (int )type_id);
    } else {
    }
    return (-19);
  }
}
}
void ldv_mutex_lock_335(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_336(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_337(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_338(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_339(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_340(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_341(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static __u16 __le16_to_cpup(__le16 const *p )
{
  {
  return ((__u16 )*p);
}
}
int ldv_mutex_trylock_352(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_350(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_353(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_355(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_349(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_351(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_354(struct mutex *ldv_func_arg1 ) ;
extern void schedule(void) ;
__inline static unsigned int efx_port_num(struct efx_nic *efx )
{
  {
  return ((unsigned int )(efx->net_dev)->dev_id);
}
}
void efx_mcdi_rpc_start(struct efx_nic *efx , unsigned int cmd , u8 const *inbuf ,
                        size_t inlen ) ;
int efx_mcdi_rpc_finish(struct efx_nic *efx , unsigned int cmd , size_t inlen , u8 *outbuf ,
                        size_t outlen , size_t *outlen_actual ) ;
void efx_mcdi_sensor_event(struct efx_nic *efx , efx_qword_t *ev ) ;
int efx_mcdi_nvram_types(struct efx_nic *efx , u32 *nvram_types_out ) ;
int efx_mcdi_nvram_info(struct efx_nic *efx , unsigned int type , size_t *size_out ,
                        size_t *erase_size_out , bool *protected_out ) ;
int efx_mcdi_nvram_update_start(struct efx_nic *efx , unsigned int type ) ;
int efx_mcdi_nvram_read(struct efx_nic *efx , unsigned int type , loff_t offset ,
                        u8 *buffer , size_t length ) ;
int efx_mcdi_nvram_write(struct efx_nic *efx , unsigned int type , loff_t offset ,
                         u8 const *buffer , size_t length ) ;
int efx_mcdi_nvram_erase(struct efx_nic *efx , unsigned int type , loff_t offset ,
                         size_t length ) ;
int efx_mcdi_nvram_update_finish(struct efx_nic *efx , unsigned int type ) ;
void efx_sriov_flr(struct efx_nic *efx , unsigned int vf_i ) ;
void efx_ptp_event(struct efx_nic *efx , efx_qword_t *ev ) ;
void efx_mcdi_phy_decode_link(struct efx_nic *efx , struct efx_link_state *link_state ,
                              u32 speed , u32 flags , u32 fcntl ) ;
void efx_mcdi_phy_check_fcntl(struct efx_nic *efx , u32 lpa ) ;
__inline static struct efx_mcdi_iface *efx_mcdi(struct efx_nic *efx )
{
  struct siena_nic_data *nic_data ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  return (& nic_data->mcdi);
}
}
void efx_mcdi_init(struct efx_nic *efx )
{
  struct efx_mcdi_iface *mcdi ;
  int tmp ;
  struct lock_class_key __key ;
  struct lock_class_key __key___0 ;
  {
  tmp = efx_nic_rev(efx);
  if (tmp <= 2) {
    return;
  } else {
  }
  mcdi = efx_mcdi(efx);
  __init_waitqueue_head(& mcdi->wq, "&mcdi->wq", & __key);
  spinlock_check(& mcdi->iface_lock);
  __raw_spin_lock_init(& mcdi->iface_lock.ldv_5961.rlock, "&(&mcdi->iface_lock)->rlock",
                       & __key___0);
  atomic_set(& mcdi->state, 0);
  mcdi->mode = 0;
  efx_mcdi_poll_reboot(efx);
  return;
}
}
static void efx_mcdi_copyin(struct efx_nic *efx , unsigned int cmd , u8 const *inbuf ,
                            size_t inlen )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  unsigned int pdu ;
  unsigned int tmp___0 ;
  unsigned int doorbell ;
  unsigned int tmp___1 ;
  unsigned int i ;
  efx_dword_t hdr ;
  u32 xflags ;
  u32 seqno ;
  int tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  long tmp___5 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  tmp___0 = efx_port_num(efx);
  pdu = tmp___0 != 0U ? 16711944U : 16711688U;
  tmp___1 = efx_port_num(efx);
  doorbell = tmp___1 != 0U ? 16711684U : 16711680U;
  tmp___2 = atomic_read((atomic_t const *)(& mcdi->state));
  tmp___3 = ldv__builtin_expect(tmp___2 == 0, 0L);
  if (tmp___3 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                         "i" (166), "i" (12UL));
    ldv_41772: ;
    goto ldv_41772;
  } else {
  }
  tmp___4 = ldv__builtin_expect((inlen & 3UL) != 0UL, 0L);
  if (tmp___4 != 0L) {
    goto _L;
  } else {
    tmp___5 = ldv__builtin_expect(inlen > 255UL, 0L);
    if (tmp___5 != 0L) {
      _L:
      __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                           "i" (167), "i" (12UL));
      ldv_41773: ;
      goto ldv_41773;
    } else {
    }
  }
  seqno = mcdi->seqno & 15U;
  xflags = 0U;
  if ((unsigned int )mcdi->mode == 1U) {
    xflags = xflags | 1U;
  } else {
  }
  hdr.u32[0] = (((((unsigned int )inlen << 8) | cmd) | (seqno << 16)) | (xflags << 24)) | 128U;
  efx_writed(efx, & hdr, pdu);
  i = 0U;
  goto ldv_41775;
  ldv_41774:
  _efx_writed(efx, *((__le32 *)inbuf + (unsigned long )i), (pdu + i) + 4U);
  i = i + 4U;
  ldv_41775: ;
  if ((size_t )i < inlen) {
    goto ldv_41774;
  } else {
  }
  __asm__ volatile ("sfence": : : "memory");
  _efx_writed(efx, 1165531836U, doorbell);
  return;
}
}
static void efx_mcdi_copyout(struct efx_nic *efx , u8 *outbuf , size_t outlen )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  unsigned int pdu ;
  unsigned int tmp___0 ;
  int i ;
  int tmp___1 ;
  long tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  tmp___0 = efx_port_num(efx);
  pdu = tmp___0 != 0U ? 16711944U : 16711688U;
  tmp___1 = atomic_read((atomic_t const *)(& mcdi->state));
  tmp___2 = ldv__builtin_expect(tmp___1 == 0, 0L);
  if (tmp___2 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                         "i" (200), "i" (12UL));
    ldv_41785: ;
    goto ldv_41785;
  } else {
  }
  tmp___3 = ldv__builtin_expect((outlen & 3UL) != 0UL, 0L);
  if (tmp___3 != 0L) {
    goto _L;
  } else {
    tmp___4 = ldv__builtin_expect(outlen > 255UL, 0L);
    if (tmp___4 != 0L) {
      _L:
      __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                           "i" (201), "i" (12UL));
      ldv_41786: ;
      goto ldv_41786;
    } else {
    }
  }
  i = 0;
  goto ldv_41788;
  ldv_41787:
  *((__le32 *)outbuf + (unsigned long )i) = _efx_readd(efx, (pdu + (unsigned int )i) + 4U);
  i = i + 4;
  ldv_41788: ;
  if ((size_t )i < outlen) {
    goto ldv_41787;
  } else {
  }
  return;
}
}
static int efx_mcdi_poll(struct efx_nic *efx )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  unsigned long time ;
  unsigned long finish ;
  unsigned int respseq ;
  unsigned int respcmd ;
  unsigned int error ;
  unsigned int pdu ;
  unsigned int tmp___0 ;
  unsigned int rc ;
  unsigned int spins ;
  efx_dword_t reg ;
  int tmp___1 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  tmp___0 = efx_port_num(efx);
  pdu = tmp___0 != 0U ? 16711944U : 16711688U;
  tmp___1 = efx_mcdi_poll_reboot(efx);
  rc = (unsigned int )(- tmp___1);
  if (rc != 0U) {
    goto out;
  } else {
  }
  spins = 10000U;
  finish = (unsigned long )jiffies + 2500UL;
  ldv_41811: ;
  if (spins != 0U) {
    spins = spins - 1U;
    __const_udelay(4295UL);
  } else {
    schedule_timeout_uninterruptible(1L);
  }
  time = jiffies;
  __asm__ volatile ("lfence": : : "memory");
  efx_readd(efx, & reg, pdu);
  if (reg.u32[0] != 4294967295U && (reg.u32[0] & 8388608U) != 0U) {
    goto ldv_41804;
  } else {
  }
  if ((long )finish - (long )time < 0L) {
    return (-110);
  } else {
  }
  goto ldv_41811;
  ldv_41804:
  mcdi->resplen = (size_t )(reg.u32[0] >> 8) & 255UL;
  respseq = (reg.u32[0] >> 16) & 15U;
  respcmd = reg.u32[0] & 127U;
  error = (reg.u32[0] >> 22) & 1U;
  if (error != 0U && mcdi->resplen == 0UL) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "MC rebooted\n");
    } else {
    }
    rc = 5U;
  } else
  if (((mcdi->seqno ^ respseq) & 15U) != 0U) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "MC response mismatch tx seq 0x%x rx seq 0x%x\n",
                 respseq, mcdi->seqno);
    } else {
    }
    rc = 5U;
  } else
  if (error != 0U) {
    efx_readd(efx, & reg, pdu + 4U);
    switch (reg.u32[0]) {
    case 2U:
    rc = 2U;
    goto ldv_41813;
    case 4U:
    rc = 4U;
    goto ldv_41813;
    case 13U:
    rc = 13U;
    goto ldv_41813;
    case 16U:
    rc = 16U;
    goto ldv_41813;
    case 22U:
    rc = 22U;
    goto ldv_41813;
    case 35U:
    rc = 35U;
    goto ldv_41813;
    case 38U:
    rc = 38U;
    goto ldv_41813;
    case 62U:
    rc = 62U;
    goto ldv_41813;
    default:
    rc = 5U;
    goto ldv_41813;
    }
    ldv_41813: ;
  } else {
    rc = 0U;
  }
  out:
  mcdi->resprc = rc;
  if (rc != 0U) {
    mcdi->resplen = 0UL;
  } else {
  }
  return (0);
}
}
int efx_mcdi_poll_reboot(struct efx_nic *efx )
{
  unsigned int addr ;
  unsigned int tmp ;
  efx_dword_t reg ;
  uint32_t value ;
  int tmp___0 ;
  {
  tmp = efx_port_num(efx);
  addr = tmp != 0U ? 16713724U : 16713720U;
  tmp___0 = efx_nic_rev(efx);
  if (tmp___0 <= 2) {
    return (0);
  } else {
  }
  efx_readd(efx, & reg, addr);
  value = reg.u32[0];
  if (value == 0U) {
    return (0);
  } else {
  }
  memset((void *)(& efx->mac_stats), 0, 488UL);
  reg.u32[0] = 0U;
  efx_writed(efx, & reg, addr);
  if (value == 3735936685U) {
    return (-4);
  } else {
    return (-5);
  }
}
}
static void efx_mcdi_acquire(struct efx_mcdi_iface *mcdi )
{
  int tmp ;
  wait_queue_t __wait ;
  struct task_struct *tmp___0 ;
  int tmp___1 ;
  {
  tmp = atomic_cmpxchg(& mcdi->state, 0, 1);
  if (tmp == 0) {
    goto ldv_41831;
  } else {
  }
  tmp___0 = get_current();
  __wait.flags = 0U;
  __wait.private = (void *)tmp___0;
  __wait.func = & autoremove_wake_function;
  __wait.task_list.next = & __wait.task_list;
  __wait.task_list.prev = & __wait.task_list;
  ldv_41834:
  prepare_to_wait(& mcdi->wq, & __wait, 2);
  tmp___1 = atomic_cmpxchg(& mcdi->state, 0, 1);
  if (tmp___1 == 0) {
    goto ldv_41833;
  } else {
  }
  schedule();
  goto ldv_41834;
  ldv_41833:
  finish_wait(& mcdi->wq, & __wait);
  ldv_41831: ;
  return;
}
}
static int efx_mcdi_await_completion(struct efx_nic *efx )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  long __ret ;
  wait_queue_t __wait ;
  struct task_struct *tmp___0 ;
  int tmp___1 ;
  int tmp___2 ;
  int tmp___3 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  __ret = 2500L;
  tmp___2 = atomic_read((atomic_t const *)(& mcdi->state));
  if (tmp___2 != 2) {
    tmp___0 = get_current();
    __wait.flags = 0U;
    __wait.private = (void *)tmp___0;
    __wait.func = & autoremove_wake_function;
    __wait.task_list.next = & __wait.task_list;
    __wait.task_list.prev = & __wait.task_list;
    ldv_41842:
    prepare_to_wait(& mcdi->wq, & __wait, 2);
    tmp___1 = atomic_read((atomic_t const *)(& mcdi->state));
    if (tmp___1 == 2) {
      goto ldv_41841;
    } else {
    }
    __ret = schedule_timeout(__ret);
    if (__ret == 0L) {
      goto ldv_41841;
    } else {
    }
    goto ldv_41842;
    ldv_41841:
    finish_wait(& mcdi->wq, & __wait);
  } else {
  }
  if (__ret == 0L) {
    return (-110);
  } else {
  }
  if ((unsigned int )mcdi->mode == 0U) {
    tmp___3 = efx_mcdi_poll(efx);
    return (tmp___3);
  } else {
  }
  return (0);
}
}
static bool efx_mcdi_complete(struct efx_mcdi_iface *mcdi )
{
  int tmp ;
  {
  tmp = atomic_cmpxchg(& mcdi->state, 1, 2);
  if (tmp == 1) {
    __wake_up(& mcdi->wq, 3U, 1, 0);
    return (1);
  } else {
  }
  return (0);
}
}
static void efx_mcdi_release(struct efx_mcdi_iface *mcdi )
{
  {
  atomic_set(& mcdi->state, 0);
  __wake_up(& mcdi->wq, 3U, 1, 0);
  return;
}
}
static void efx_mcdi_ev_cpl(struct efx_nic *efx , unsigned int seqno , unsigned int datalen ,
                            unsigned int errno )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  bool wake ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  wake = 0;
  spin_lock(& mcdi->iface_lock);
  if (((mcdi->seqno ^ seqno) & 15U) != 0U) {
    if (mcdi->credits != 0U) {
      mcdi->credits = mcdi->credits - 1U;
    } else
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "MC response mismatch tx seq 0x%x rx seq 0x%x\n",
                 seqno, mcdi->seqno);
    } else {
      mcdi->resprc = errno;
      mcdi->resplen = (size_t )datalen;
      wake = 1;
    }
  } else {
  }
  spin_unlock(& mcdi->iface_lock);
  if ((int )wake) {
    efx_mcdi_complete(mcdi);
  } else {
  }
  return;
}
}
int efx_mcdi_rpc(struct efx_nic *efx , unsigned int cmd , u8 const *inbuf , size_t inlen ,
                 u8 *outbuf , size_t outlen , size_t *outlen_actual )
{
  int tmp ;
  {
  efx_mcdi_rpc_start(efx, cmd, inbuf, inlen);
  tmp = efx_mcdi_rpc_finish(efx, cmd, inlen, outbuf, outlen, outlen_actual);
  return (tmp);
}
}
void efx_mcdi_rpc_start(struct efx_nic *efx , unsigned int cmd , u8 const *inbuf ,
                        size_t inlen )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  int tmp___0 ;
  long tmp___1 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  tmp___0 = efx_nic_rev(efx);
  tmp___1 = ldv__builtin_expect(tmp___0 <= 2, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                         "i" (431), "i" (12UL));
    ldv_41874: ;
    goto ldv_41874;
  } else {
  }
  efx_mcdi_acquire(mcdi);
  spin_lock_bh(& mcdi->iface_lock);
  mcdi->seqno = mcdi->seqno + 1U;
  spin_unlock_bh(& mcdi->iface_lock);
  efx_mcdi_copyin(efx, cmd, inbuf, inlen);
  return;
}
}
int efx_mcdi_rpc_finish(struct efx_nic *efx , unsigned int cmd , size_t inlen , u8 *outbuf ,
                        size_t outlen , size_t *outlen_actual )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  int rc ;
  int tmp___0 ;
  long tmp___1 ;
  size_t resplen ;
  size_t _min1 ;
  size_t _min2 ;
  struct _ddebug descriptor ;
  long tmp___2 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  tmp___0 = efx_nic_rev(efx);
  tmp___1 = ldv__builtin_expect(tmp___0 <= 2, 0L);
  if (tmp___1 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared"),
                         "i" (449), "i" (12UL));
    ldv_41885: ;
    goto ldv_41885;
  } else {
  }
  if ((unsigned int )mcdi->mode == 0U) {
    rc = efx_mcdi_poll(efx);
  } else {
    rc = efx_mcdi_await_completion(efx);
  }
  if (rc != 0) {
    spin_lock_bh(& mcdi->iface_lock);
    mcdi->seqno = mcdi->seqno + 1U;
    mcdi->credits = mcdi->credits + 1U;
    spin_unlock_bh(& mcdi->iface_lock);
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "MC command 0x%x inlen %d mode %d timed out\n",
                 cmd, (int )inlen, (unsigned int )mcdi->mode);
    } else {
    }
  } else {
    spin_lock_bh(& mcdi->iface_lock);
    rc = (int )(- mcdi->resprc);
    resplen = mcdi->resplen;
    spin_unlock_bh(& mcdi->iface_lock);
    if (rc == 0) {
      _min1 = outlen;
      _min2 = mcdi->resplen + 3UL;
      efx_mcdi_copyout(efx, outbuf, (_min1 < _min2 ? _min1 : _min2) & 0xfffffffffffffffcUL);
      if ((unsigned long )outlen_actual != (unsigned long )((size_t *)0)) {
        *outlen_actual = resplen;
      } else {
      }
    } else
    if (cmd == 61U && rc == -5) {
    } else
    if (rc == -5 || rc == -4) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "MC fatal error %d\n",
                   - rc);
      } else {
      }
      efx_schedule_reset(efx, 11);
    } else
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_mcdi_rpc_finish";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared";
      descriptor.format = "MC command 0x%x inlen %d failed rc=%d\n";
      descriptor.lineno = 495U;
      descriptor.flags = 0U;
      tmp___2 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp___2 != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "MC command 0x%x inlen %d failed rc=%d\n", cmd, (int )inlen,
                             - rc);
      } else {
      }
    } else {
    }
    if (rc == -5 || rc == -4) {
      msleep(10U);
      efx_mcdi_poll_reboot(efx);
    } else {
    }
  }
  efx_mcdi_release(mcdi);
  return (rc);
}
}
void efx_mcdi_mode_poll(struct efx_nic *efx )
{
  struct efx_mcdi_iface *mcdi ;
  int tmp ;
  {
  tmp = efx_nic_rev(efx);
  if (tmp <= 2) {
    return;
  } else {
  }
  mcdi = efx_mcdi(efx);
  if ((unsigned int )mcdi->mode == 0U) {
    return;
  } else {
  }
  mcdi->mode = 0;
  efx_mcdi_complete(mcdi);
  return;
}
}
void efx_mcdi_mode_event(struct efx_nic *efx )
{
  struct efx_mcdi_iface *mcdi ;
  int tmp ;
  {
  tmp = efx_nic_rev(efx);
  if (tmp <= 2) {
    return;
  } else {
  }
  mcdi = efx_mcdi(efx);
  if ((unsigned int )mcdi->mode == 1U) {
    return;
  } else {
  }
  efx_mcdi_acquire(mcdi);
  mcdi->mode = 1;
  efx_mcdi_release(mcdi);
  return;
}
}
static void efx_mcdi_ev_death(struct efx_nic *efx , int rc )
{
  struct efx_mcdi_iface *mcdi ;
  struct efx_mcdi_iface *tmp ;
  int count ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  tmp = efx_mcdi(efx);
  mcdi = tmp;
  spin_lock(& mcdi->iface_lock);
  tmp___1 = efx_mcdi_complete(mcdi);
  if ((int )tmp___1) {
    if ((unsigned int )mcdi->mode == 1U) {
      mcdi->resprc = (unsigned int )rc;
      mcdi->resplen = 0UL;
      mcdi->credits = mcdi->credits + 1U;
    } else {
      efx_schedule_reset(efx, 11);
      count = 0;
      goto ldv_41908;
      ldv_41907:
      tmp___0 = efx_mcdi_poll_reboot(efx);
      if (tmp___0 != 0) {
        goto ldv_41906;
      } else {
      }
      __const_udelay(429500UL);
      count = count + 1;
      ldv_41908: ;
      if (count <= 99) {
        goto ldv_41907;
      } else {
      }
      ldv_41906: ;
    }
  } else {
  }
  spin_unlock(& mcdi->iface_lock);
  return;
}
}
static unsigned int efx_mcdi_event_link_speed[4U] = { 0U, 100U, 1000U, 10000U};
static void efx_mcdi_process_link_change(struct efx_nic *efx , efx_qword_t *ev )
{
  u32 flags ;
  u32 fcntl ;
  u32 speed ;
  u32 lpa ;
  {
  speed = (u32 )(ev->u64[0] >> 16) & 15U;
  speed = efx_mcdi_event_link_speed[speed];
  flags = (u32 )(ev->u64[0] >> 24) & 255U;
  fcntl = (u32 )(ev->u64[0] >> 20) & 15U;
  lpa = (u32 )ev->u64[0] & 65535U;
  efx_mcdi_phy_decode_link(efx, & efx->link_state, speed, flags, fcntl);
  efx_mcdi_phy_check_fcntl(efx, lpa);
  efx_link_status_changed(efx);
  return;
}
}
void efx_mcdi_process_event(struct efx_channel *channel , efx_qword_t *event )
{
  struct efx_nic *efx ;
  int code ;
  u32 data ;
  {
  efx = channel->efx;
  code = (int )(event->u64[0] >> 44) & 255;
  data = (u32 )event->u64[0];
  switch (code) {
  case 1: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "MC watchdog or assertion failure at 0x%x\n",
               data);
  } else {
  }
  efx_mcdi_ev_death(efx, 4);
  goto ldv_41926;
  case 2: ;
  if ((efx->msg_enable & 16384U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "MCDI PM event.\n");
  } else {
  }
  goto ldv_41926;
  case 3:
  efx_mcdi_ev_cpl(efx, (unsigned int )event->u64[0] & 255U, (unsigned int )(event->u64[0] >> 8) & 255U,
                  (unsigned int )(event->u64[0] >> 16) & 255U);
  goto ldv_41926;
  case 4:
  efx_mcdi_process_link_change(efx, event);
  goto ldv_41926;
  case 5:
  efx_mcdi_sensor_event(efx, event);
  goto ldv_41926;
  case 6: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "MC Scheduler error address=0x%x\n",
                data);
  } else {
  }
  goto ldv_41926;
  case 7: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "MC Reboot\n");
  } else {
  }
  efx_mcdi_ev_death(efx, 5);
  goto ldv_41926;
  case 8: ;
  goto ldv_41926;
  case 10:
  efx_sriov_flr(efx, (unsigned int )event->u64[0] & 255U);
  goto ldv_41926;
  case 13: ;
  case 14: ;
  case 15:
  efx_ptp_event(efx, event);
  goto ldv_41926;
  default: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "Unknown MCDI event 0x%x\n",
               code);
  } else {
  }
  }
  ldv_41926: ;
  return;
}
}
void efx_mcdi_print_fwver(struct efx_nic *efx , char *buf , size_t len )
{
  u8 outbuf[32U] ;
  size_t outlength ;
  __le16 const *ver_words ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 8U, 0, 0UL, (u8 *)(& outbuf), 32UL, & outlength);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlength <= 31UL) {
    rc = -5;
    goto fail;
  } else {
  }
  ver_words = (__le16 const *)(& outbuf) + 24U;
  snprintf(buf, len, "%u.%u.%u.%u", (int )*ver_words, (int )*(ver_words + 1UL), (int )*(ver_words + 2UL),
           (int )*(ver_words + 3UL));
  return;
  fail: ;
  if ((efx->msg_enable & 2U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_print_fwver",
               rc);
  } else {
  }
  *buf = 0;
  return;
}
}
int efx_mcdi_drv_attach(struct efx_nic *efx , bool driver_operating , bool *was_attached )
{
  u8 inbuf[8U] ;
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = (int )driver_operating ? 1U : 0U;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = 1U;
  rc = efx_mcdi_rpc(efx, 28U, (u8 const *)(& inbuf), 8UL, (u8 *)(& outbuf), 4UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 3UL) {
    rc = -5;
    goto fail;
  } else {
  }
  if ((unsigned long )was_attached != (unsigned long )((bool *)0)) {
    *was_attached = ((efx_dword_t *)(& outbuf))->u32[0] != 0U;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 2U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_drv_attach",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_get_board_cfg(struct efx_nic *efx , u8 *mac_address , u16 *fw_subtype_list ,
                           u32 *capabilities )
{
  uint8_t outbuf[96U] ;
  size_t outlen ;
  size_t offset ;
  size_t i ;
  int port_num ;
  unsigned int tmp ;
  int rc ;
  size_t __len ;
  void *__ret ;
  __u16 tmp___0 ;
  {
  tmp = efx_port_num(efx);
  port_num = (int )tmp;
  rc = efx_mcdi_rpc(efx, 24U, 0, 0UL, (u8 *)(& outbuf), 96UL, & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 95UL) {
    rc = -5;
    goto fail;
  } else {
  }
  offset = port_num != 0 ? 50UL : 44UL;
  if ((unsigned long )mac_address != (unsigned long )((u8 *)0)) {
    __len = 6UL;
    if (__len > 63UL) {
      __ret = memcpy((void *)mac_address, (void const *)(& outbuf) + offset, __len);
    } else {
      __ret = memcpy((void *)mac_address, (void const *)(& outbuf) + offset,
                               __len);
    }
  } else {
  }
  if ((unsigned long )fw_subtype_list != (unsigned long )((u16 *)0)) {
    offset = 72UL;
    i = 0UL;
    goto ldv_41978;
    ldv_41977: ;
    if (offset + 2UL <= outlen) {
      tmp___0 = __le16_to_cpup((__le16 const *)(& outbuf) + offset);
      *(fw_subtype_list + i) = tmp___0;
    } else {
      *(fw_subtype_list + i) = 0U;
    }
    offset = offset + 2UL;
    i = i + 1UL;
    ldv_41978: ;
    if (i <= 31UL) {
      goto ldv_41977;
    } else {
    }
  } else {
  }
  if ((unsigned long )capabilities != (unsigned long )((u32 *)0)) {
    if (port_num != 0) {
      *capabilities = ((efx_dword_t *)(& outbuf) + 40U)->u32[0];
    } else {
      *capabilities = ((efx_dword_t *)(& outbuf) + 36U)->u32[0];
    }
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d len=%d\n",
               "efx_mcdi_get_board_cfg", rc, (int )outlen);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_log_ctrl(struct efx_nic *efx , bool evq , bool uart , u32 dest_evq )
{
  u8 inbuf[8U] ;
  u32 dest ;
  int rc ;
  {
  dest = 0U;
  if ((int )uart) {
    dest = dest | 1U;
  } else {
  }
  if ((int )evq) {
    dest = dest | 2U;
  } else {
  }
  ((efx_dword_t *)(& inbuf))->u32[0] = dest;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = dest_evq;
  rc = efx_mcdi_rpc(efx, 7U, (u8 const *)(& inbuf), 8UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_log_ctrl",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_types(struct efx_nic *efx , u32 *nvram_types_out )
{
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 54U, 0, 0UL, (u8 *)(& outbuf), 4UL, & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 3UL) {
    rc = -5;
    goto fail;
  } else {
  }
  *nvram_types_out = ((efx_dword_t *)(& outbuf))->u32[0];
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_types",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_info(struct efx_nic *efx , unsigned int type , size_t *size_out ,
                        size_t *erase_size_out , bool *protected_out )
{
  u8 inbuf[4U] ;
  u8 outbuf[24U] ;
  size_t outlen ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  rc = efx_mcdi_rpc(efx, 55U, (u8 const *)(& inbuf), 4UL, (u8 *)(& outbuf), 24UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 23UL) {
    rc = -5;
    goto fail;
  } else {
  }
  *size_out = (size_t )((efx_dword_t *)(& outbuf) + 4U)->u32[0];
  *erase_size_out = (size_t )((efx_dword_t *)(& outbuf) + 8U)->u32[0];
  *protected_out = ((int )((efx_dword_t *)(& outbuf) + 12U)->u32[0] & 1) != 0;
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_info",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_update_start(struct efx_nic *efx , unsigned int type )
{
  u8 inbuf[4U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  rc = efx_mcdi_rpc(efx, 56U, (u8 const *)(& inbuf), 4UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_update_start",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_read(struct efx_nic *efx , unsigned int type , loff_t offset ,
                        u8 *buffer , size_t length )
{
  u8 inbuf[12U] ;
  u8 outbuf[128U] ;
  size_t outlen ;
  int rc ;
  size_t __len ;
  void *__ret ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = (unsigned int )offset;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = (unsigned int )length;
  rc = efx_mcdi_rpc(efx, 57U, (u8 const *)(& inbuf), 12UL, (u8 *)(& outbuf), 128UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  __len = length;
  __ret = memcpy((void *)buffer, (void const *)(& outbuf), __len);
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_read",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_write(struct efx_nic *efx , unsigned int type , loff_t offset ,
                         u8 const *buffer , size_t length )
{
  u8 inbuf[140U] ;
  int rc ;
  size_t __len ;
  void *__ret ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = (unsigned int )offset;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = (unsigned int )length;
  __len = length;
  __ret = memcpy((void *)(& inbuf) + 12U, (void const *)buffer, __len);
  rc = efx_mcdi_rpc(efx, 58U, (u8 const *)(& inbuf), (length + 15UL) & 0xfffffffffffffffcUL,
                    0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_write",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_erase(struct efx_nic *efx , unsigned int type , loff_t offset ,
                         size_t length )
{
  u8 inbuf[12U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = (unsigned int )offset;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = (unsigned int )length;
  rc = efx_mcdi_rpc(efx, 59U, (u8 const *)(& inbuf), 12UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_erase",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_nvram_update_finish(struct efx_nic *efx , unsigned int type )
{
  u8 inbuf[8U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  rc = efx_mcdi_rpc(efx, 60U, (u8 const *)(& inbuf), 8UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_update_finish",
               rc);
  } else {
  }
  return (rc);
}
}
static int efx_mcdi_nvram_test(struct efx_nic *efx , unsigned int type )
{
  u8 inbuf[4U] ;
  u8 outbuf[4U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = type;
  rc = efx_mcdi_rpc(efx, 76U, (u8 const *)(& inbuf), 4UL, (u8 *)(& outbuf), 4UL,
                    0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  switch (((efx_dword_t *)(& outbuf))->u32[0]) {
  case 0U: ;
  case 2U: ;
  return (0);
  default: ;
  return (-5);
  }
}
}
int efx_mcdi_nvram_test_all(struct efx_nic *efx )
{
  u32 nvram_types ;
  unsigned int type ;
  int rc ;
  {
  rc = efx_mcdi_nvram_types(efx, & nvram_types);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  type = 0U;
  goto ldv_42089;
  ldv_42088: ;
  if ((int )nvram_types & 1) {
    rc = efx_mcdi_nvram_test(efx, type);
    if (rc != 0) {
      goto fail2;
    } else {
    }
  } else {
  }
  type = type + 1U;
  nvram_types = nvram_types >> 1;
  ldv_42089: ;
  if (nvram_types != 0U) {
    goto ldv_42088;
  } else {
  }
  return (0);
  fail2: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed type=%u\n",
               "efx_mcdi_nvram_test_all", type);
  } else {
  }
  fail1: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_nvram_test_all",
               rc);
  } else {
  }
  return (rc);
}
}
static int efx_mcdi_read_assertion(struct efx_nic *efx )
{
  u8 inbuf[4U] ;
  u8 outbuf[140U] ;
  unsigned int flags ;
  unsigned int index ;
  unsigned int ofst ;
  char const *reason ;
  size_t outlen ;
  int retry ;
  int rc ;
  int tmp ;
  {
  retry = 2;
  ldv_42104:
  ((efx_dword_t *)(& inbuf))->u32[0] = 1U;
  rc = efx_mcdi_rpc(efx, 6U, (u8 const *)(& inbuf), 4UL, (u8 *)(& outbuf), 140UL,
                    & outlen);
  if (rc == -4 || rc == -5) {
    tmp = retry;
    retry = retry - 1;
    if (tmp > 0) {
      goto ldv_42104;
    } else {
      goto ldv_42105;
    }
  } else {
  }
  ldv_42105: ;
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (outlen <= 139UL) {
    return (-5);
  } else {
  }
  flags = ((efx_dword_t *)(& outbuf))->u32[0];
  if (flags == 1U) {
    return (0);
  } else {
  }
  reason = flags != 2U ? (flags != 3U ? (flags == 4U ? "watchdog reset" : "unknown assertion") : "thread-level assertion") : "system-level assertion";
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "MCPU %s at PC = 0x%.8x in thread 0x%.8x\n",
               reason, ((efx_dword_t *)(& outbuf) + 4U)->u32[0], ((efx_dword_t *)(& outbuf) + 132U)->u32[0]);
  } else {
  }
  ofst = 8U;
  index = 1U;
  goto ldv_42107;
  ldv_42106: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "R%.2d (?): 0x%.8x\n", index,
               ((efx_dword_t *)(& outbuf) + (unsigned long )ofst)->u32[0]);
  } else {
  }
  ofst = ofst + 4U;
  index = index + 1U;
  ldv_42107: ;
  if (index <= 31U) {
    goto ldv_42106;
  } else {
  }
  return (0);
}
}
static void efx_mcdi_exit_assertion(struct efx_nic *efx )
{
  u8 inbuf[4U] ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = 1U;
  efx_mcdi_rpc(efx, 61U, (u8 const *)(& inbuf), 4UL, 0, 0UL, 0);
  return;
}
}
int efx_mcdi_handle_assertion(struct efx_nic *efx )
{
  int rc ;
  {
  rc = efx_mcdi_read_assertion(efx);
  if (rc != 0) {
    return (rc);
  } else {
  }
  efx_mcdi_exit_assertion(efx);
  return (0);
}
}
void efx_mcdi_set_id_led(struct efx_nic *efx , enum efx_led_mode mode )
{
  u8 inbuf[4U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = (unsigned int )mode;
  rc = efx_mcdi_rpc(efx, 43U, (u8 const *)(& inbuf), 4UL, 0, 0UL, 0);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n",
                 "efx_mcdi_set_id_led", rc);
    } else {
    }
  } else {
  }
  return;
}
}
int efx_mcdi_reset_port(struct efx_nic *efx )
{
  int rc ;
  int tmp ;
  {
  tmp = efx_mcdi_rpc(efx, 32U, 0, 0UL, 0, 0UL, 0);
  rc = tmp;
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n",
                 "efx_mcdi_reset_port", rc);
    } else {
    }
  } else {
  }
  return (rc);
}
}
int efx_mcdi_reset_mc(struct efx_nic *efx )
{
  u8 inbuf[4U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = 0U;
  rc = efx_mcdi_rpc(efx, 61U, (u8 const *)(& inbuf), 4UL, 0, 0UL, 0);
  if (rc == -5) {
    return (0);
  } else {
  }
  if (rc == 0) {
    rc = -5;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_reset_mc",
               rc);
  } else {
  }
  return (rc);
}
}
static int efx_mcdi_wol_filter_set(struct efx_nic *efx , u32 type , u8 const *mac ,
                                   int *id_out )
{
  u8 inbuf[192U] ;
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  size_t __len ;
  void *__ret ;
  {
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = type;
  ((efx_dword_t *)(& inbuf))->u32[0] = 0U;
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& inbuf) + 8U, (void const *)mac, __len);
  } else {
    __ret = memcpy((void *)(& inbuf) + 8U, (void const *)mac, __len);
  }
  rc = efx_mcdi_rpc(efx, 50U, (u8 const *)(& inbuf), 192UL, (u8 *)(& outbuf), 4UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 3UL) {
    rc = -5;
    goto fail;
  } else {
  }
  *id_out = (int )((efx_dword_t *)(& outbuf))->u32[0];
  return (0);
  fail:
  *id_out = -1;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_wol_filter_set",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_wol_filter_set_magic(struct efx_nic *efx , u8 const *mac , int *id_out )
{
  int tmp ;
  {
  tmp = efx_mcdi_wol_filter_set(efx, 0U, mac, id_out);
  return (tmp);
}
}
int efx_mcdi_wol_filter_get_magic(struct efx_nic *efx , int *id_out )
{
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 69U, 0, 0UL, (u8 *)(& outbuf), 4UL, & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 3UL) {
    rc = -5;
    goto fail;
  } else {
  }
  *id_out = (int )((efx_dword_t *)(& outbuf))->u32[0];
  return (0);
  fail:
  *id_out = -1;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_wol_filter_get_magic",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_wol_filter_remove(struct efx_nic *efx , int id )
{
  u8 inbuf[4U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = (unsigned int )id;
  rc = efx_mcdi_rpc(efx, 51U, (u8 const *)(& inbuf), 4UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_wol_filter_remove",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_flush_rxqs(struct efx_nic *efx )
{
  struct efx_channel *channel ;
  struct efx_rx_queue *rx_queue ;
  __le32 *qid ;
  int rc ;
  int count ;
  void *tmp ;
  int tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  int __ret_warn_on ;
  long tmp___4 ;
  {
  tmp = kmalloc(128UL, 208U);
  qid = (__le32 *)tmp;
  if ((unsigned long )qid == (unsigned long )((__le32 *)0)) {
    return (-12);
  } else {
  }
  count = 0;
  channel = efx->channel[0];
  goto ldv_42184;
  ldv_42183:
  tmp___2 = efx_channel_has_rx_queue(channel);
  if (tmp___2) {
    tmp___3 = 0;
  } else {
    tmp___3 = 1;
  }
  if (tmp___3) {
  } else {
    rx_queue = & channel->rx_queue;
    goto ldv_42181;
    ldv_42180: ;
    if ((int )rx_queue->flush_pending) {
      rx_queue->flush_pending = 0;
      atomic_dec(& efx->rxq_flush_pending);
      tmp___0 = count;
      count = count + 1;
      tmp___1 = efx_rx_queue_index(rx_queue);
      *(qid + (unsigned long )tmp___0) = (unsigned int )tmp___1;
    } else {
    }
    rx_queue = 0;
    ldv_42181: ;
    if ((unsigned long )rx_queue != (unsigned long )((struct efx_rx_queue *)0)) {
      goto ldv_42180;
    } else {
    }
  }
  channel = (unsigned int )(channel->channel + 1) < efx->n_channels ? efx->channel[channel->channel + 1] : 0;
  ldv_42184: ;
  if ((unsigned long )channel != (unsigned long )((struct efx_channel *)0)) {
    goto ldv_42183;
  } else {
  }
  rc = efx_mcdi_rpc(efx, 39U, (u8 const *)qid, (unsigned long )count * 4UL, 0, 0UL,
                    0);
  __ret_warn_on = rc < 0;
  tmp___4 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___4 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi.c.prepared",
                       1313);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  kfree((void const *)qid);
  return (rc);
}
}
int efx_mcdi_wol_filter_reset(struct efx_nic *efx )
{
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 52U, 0, 0UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_wol_filter_reset",
               rc);
  } else {
  }
  return (rc);
}
}
void ldv_mutex_lock_349(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_350(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_351(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_352(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_353(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_354(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_355(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int ldv_mutex_is_locked_8(struct mutex *lock ) ;
int ldv_mutex_trylock_366(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_364(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_367(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_369(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_363(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_365(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_368(struct mutex *ldv_func_arg1 ) ;
static int efx_mcdi_get_phy_cfg(struct efx_nic *efx , struct efx_mcdi_phy_data *cfg )
{
  u8 outbuf[72U] ;
  size_t outlen ;
  int rc ;
  size_t __len ;
  void *__ret ;
  size_t __len___0 ;
  void *__ret___0 ;
  {
  rc = efx_mcdi_rpc(efx, 36U, 0, 0UL, (u8 *)(& outbuf), 72UL, & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 71UL) {
    rc = -5;
    goto fail;
  } else {
  }
  cfg->flags = ((efx_dword_t *)(& outbuf))->u32[0];
  cfg->type = ((efx_dword_t *)(& outbuf) + 4U)->u32[0];
  cfg->supported_cap = ((efx_dword_t *)(& outbuf) + 8U)->u32[0];
  cfg->channel = ((efx_dword_t *)(& outbuf) + 12U)->u32[0];
  cfg->port = ((efx_dword_t *)(& outbuf) + 16U)->u32[0];
  cfg->stats_mask = ((efx_dword_t *)(& outbuf) + 20U)->u32[0];
  __len = 20UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& cfg->name), (void const *)(& outbuf) + 24U, __len);
  } else {
    __ret = memcpy((void *)(& cfg->name), (void const *)(& outbuf) + 24U,
                             __len);
  }
  cfg->media = ((efx_dword_t *)(& outbuf) + 44U)->u32[0];
  cfg->mmd_mask = ((efx_dword_t *)(& outbuf) + 48U)->u32[0];
  __len___0 = 20UL;
  if (__len___0 > 63UL) {
    __ret___0 = memcpy((void *)(& cfg->revision), (void const *)(& outbuf) + 52U,
                         __len___0);
  } else {
    __ret___0 = memcpy((void *)(& cfg->revision), (void const *)(& outbuf) + 52U,
                                 __len___0);
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_get_phy_cfg",
               rc);
  } else {
  }
  return (rc);
}
}
static int efx_mcdi_set_link(struct efx_nic *efx , u32 capabilities , u32 flags ,
                             u32 loopback_mode , u32 loopback_speed )
{
  u8 inbuf[16U] ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = capabilities;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = flags;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = loopback_mode;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = loopback_speed;
  rc = efx_mcdi_rpc(efx, 42U, (u8 const *)(& inbuf), 16UL, 0, 0UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_set_link",
               rc);
  } else {
  }
  return (rc);
}
}
static int efx_mcdi_loopback_modes(struct efx_nic *efx , u64 *loopback_modes )
{
  u8 outbuf[32U] ;
  size_t outlen ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 40U, 0, 0UL, (u8 *)(& outbuf), 32UL, & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if (outlen <= 31UL) {
    rc = -5;
    goto fail;
  } else {
  }
  *loopback_modes = ((efx_qword_t *)(& outbuf) + 24U)->u64[0];
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_loopback_modes",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_mdio_read(struct efx_nic *efx , unsigned int bus , unsigned int prtad ,
                       unsigned int devad , u16 addr , u16 *value_out , u32 *status_out )
{
  u8 inbuf[16U] ;
  u8 outbuf[8U] ;
  size_t outlen ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = bus;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = prtad;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = devad;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = (unsigned int )addr;
  rc = efx_mcdi_rpc(efx, 16U, (u8 const *)(& inbuf), 16UL, (u8 *)(& outbuf), 8UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  *value_out = (unsigned short )((efx_dword_t *)(& outbuf))->u32[0];
  *status_out = ((efx_dword_t *)(& outbuf) + 4U)->u32[0];
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_mdio_read",
               rc);
  } else {
  }
  return (rc);
}
}
int efx_mcdi_mdio_write(struct efx_nic *efx , unsigned int bus , unsigned int prtad ,
                        unsigned int devad , u16 addr , u16 value , u32 *status_out )
{
  u8 inbuf[20U] ;
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = bus;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = prtad;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = devad;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = (unsigned int )addr;
  ((efx_dword_t *)(& inbuf) + 16U)->u32[0] = (unsigned int )value;
  rc = efx_mcdi_rpc(efx, 17U, (u8 const *)(& inbuf), 20UL, (u8 *)(& outbuf), 4UL,
                    & outlen);
  if (rc != 0) {
    goto fail;
  } else {
  }
  *status_out = ((efx_dword_t *)(& outbuf))->u32[0];
  return (0);
  fail: ;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n", "efx_mcdi_mdio_write",
               rc);
  } else {
  }
  return (rc);
}
}
static u32 mcdi_to_ethtool_cap(u32 media , u32 cap )
{
  u32 result ;
  {
  result = 0U;
  switch (media) {
  case 3U:
  result = result | 65536U;
  if ((cap & 64U) != 0U) {
    result = result | 131072U;
  } else {
  }
  if ((cap & 128U) != 0U) {
    result = result | 262144U;
  } else {
  }
  goto ldv_41728;
  case 4U: ;
  case 5U:
  result = result | 1024U;
  goto ldv_41728;
  case 6U:
  result = result | 128U;
  if ((cap & 2U) != 0U) {
    result = result | 1U;
  } else {
  }
  if ((cap & 4U) != 0U) {
    result = result | 2U;
  } else {
  }
  if ((cap & 8U) != 0U) {
    result = result | 4U;
  } else {
  }
  if ((cap & 16U) != 0U) {
    result = result | 8U;
  } else {
  }
  if ((cap & 32U) != 0U) {
    result = result | 16U;
  } else {
  }
  if ((cap & 64U) != 0U) {
    result = result | 32U;
  } else {
  }
  if ((cap & 128U) != 0U) {
    result = result | 4096U;
  } else {
  }
  goto ldv_41728;
  }
  ldv_41728: ;
  if ((cap & 256U) != 0U) {
    result = result | 8192U;
  } else {
  }
  if ((cap & 512U) != 0U) {
    result = result | 16384U;
  } else {
  }
  if ((cap & 1024U) != 0U) {
    result = result | 64U;
  } else {
  }
  return (result);
}
}
static u32 ethtool_to_mcdi_cap(u32 cap )
{
  u32 result ;
  {
  result = 0U;
  if ((int )cap & 1) {
    result = result | 2U;
  } else {
  }
  if ((cap & 2U) != 0U) {
    result = result | 4U;
  } else {
  }
  if ((cap & 4U) != 0U) {
    result = result | 8U;
  } else {
  }
  if ((cap & 8U) != 0U) {
    result = result | 16U;
  } else {
  }
  if ((cap & 16U) != 0U) {
    result = result | 32U;
  } else {
  }
  if ((cap & 131104U) != 0U) {
    result = result | 64U;
  } else {
  }
  if ((cap & 266240U) != 0U) {
    result = result | 128U;
  } else {
  }
  if ((cap & 8192U) != 0U) {
    result = result | 256U;
  } else {
  }
  if ((cap & 16384U) != 0U) {
    result = result | 512U;
  } else {
  }
  if ((cap & 64U) != 0U) {
    result = result | 1024U;
  } else {
  }
  return (result);
}
}
static u32 efx_get_mcdi_phy_flags(struct efx_nic *efx )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  enum efx_phy_mode mode ;
  enum efx_phy_mode supported ;
  u32 flags ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  supported = 0;
  if ((phy_cfg->flags & 32U) != 0U) {
    supported = (enum efx_phy_mode )((unsigned int )supported | 1U);
  } else {
  }
  if ((phy_cfg->flags & 8U) != 0U) {
    supported = (enum efx_phy_mode )((unsigned int )supported | 2U);
  } else {
  }
  if ((phy_cfg->flags & 16U) != 0U) {
    supported = (enum efx_phy_mode )((unsigned int )supported | 4U);
  } else {
  }
  mode = (enum efx_phy_mode )((unsigned int )efx->phy_mode & (unsigned int )supported);
  flags = 0U;
  if ((int )mode & 1) {
    flags = flags | 4U;
  } else {
  }
  if (((unsigned int )mode & 2U) != 0U) {
    flags = flags | 1U;
  } else {
  }
  if (((unsigned int )mode & 4U) != 0U) {
    flags = flags | 2U;
  } else {
  }
  return (flags);
}
}
static u32 mcdi_to_ethtool_media(u32 media )
{
  {
  switch (media) {
  case 1U: ;
  case 2U: ;
  case 3U: ;
  return (255U);
  case 4U: ;
  case 5U: ;
  return (3U);
  case 6U: ;
  return (0U);
  default: ;
  return (255U);
  }
}
}
static int efx_mcdi_phy_probe(struct efx_nic *efx )
{
  struct efx_mcdi_phy_data *phy_data ;
  u8 outbuf[28U] ;
  u32 caps ;
  int rc ;
  void *tmp ;
  {
  tmp = kzalloc(76UL, 208U);
  phy_data = (struct efx_mcdi_phy_data *)tmp;
  if ((unsigned long )phy_data == (unsigned long )((struct efx_mcdi_phy_data *)0)) {
    return (-12);
  } else {
  }
  rc = efx_mcdi_get_phy_cfg(efx, phy_data);
  if (rc != 0) {
    goto fail;
  } else {
  }
  rc = efx_mcdi_rpc(efx, 41U, 0, 0UL, (u8 *)(& outbuf), 28UL, 0);
  if (rc != 0) {
    goto fail;
  } else {
  }
  efx->phy_data = (void *)phy_data;
  efx->phy_type = phy_data->type;
  efx->mdio_bus = phy_data->channel;
  efx->mdio.prtad = (int )phy_data->port;
  efx->mdio.mmds = phy_data->mmd_mask & 4294967294U;
  efx->mdio.mode_support = 0U;
  if ((int )phy_data->mmd_mask & 1) {
    efx->mdio.mode_support = efx->mdio.mode_support | 1U;
  } else {
  }
  if ((phy_data->mmd_mask & 4294967294U) != 0U) {
    efx->mdio.mode_support = efx->mdio.mode_support | 6U;
  } else {
  }
  caps = ((efx_dword_t *)(& outbuf))->u32[0];
  if ((caps & 1024U) != 0U) {
    efx->link_advertising = mcdi_to_ethtool_cap(phy_data->media, caps);
  } else {
    phy_data->forced_cap = caps;
  }
  rc = efx_mcdi_loopback_modes(efx, & efx->loopback_modes);
  if (rc != 0) {
    goto fail;
  } else {
  }
  efx->loopback_modes = efx->loopback_modes & 0xfffffffffffffffeULL;
  efx_mcdi_phy_decode_link(efx, & efx->link_state, ((efx_dword_t *)(& outbuf) + 8U)->u32[0],
                           ((efx_dword_t *)(& outbuf) + 16U)->u32[0], ((efx_dword_t *)(& outbuf) + 20U)->u32[0]);
  efx->wanted_fc = 3U;
  if ((phy_data->supported_cap & 1024U) != 0U) {
    efx->wanted_fc = (u8 )((unsigned int )efx->wanted_fc | 4U);
  } else {
  }
  efx_link_set_wanted_fc(efx, (int )efx->wanted_fc);
  return (0);
  fail:
  kfree((void const *)phy_data);
  return (rc);
}
}
int efx_mcdi_phy_reconfigure(struct efx_nic *efx )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  u32 caps ;
  u32 tmp ;
  u32 tmp___0 ;
  u32 tmp___1 ;
  int tmp___2 ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  if (efx->link_advertising != 0U) {
    tmp = ethtool_to_mcdi_cap(efx->link_advertising);
    tmp___0 = tmp;
  } else {
    tmp___0 = phy_cfg->forced_cap;
  }
  caps = tmp___0;
  tmp___1 = efx_get_mcdi_phy_flags(efx);
  tmp___2 = efx_mcdi_set_link(efx, caps, tmp___1, (u32 )efx->loopback_mode, 0U);
  return (tmp___2);
}
}
void efx_mcdi_phy_decode_link(struct efx_nic *efx , struct efx_link_state *link_state ,
                              u32 speed , u32 flags , u32 fcntl )
{
  int __ret_warn_on ;
  long tmp ;
  int __ret_warn_on___0 ;
  long tmp___0 ;
  {
  switch (fcntl) {
  case 3U:
  __ret_warn_on = 1;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi_phy.c.prepared",
                       511);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  link_state->fc = 7U;
  goto ldv_41776;
  case 2U:
  link_state->fc = 3U;
  goto ldv_41776;
  case 1U:
  link_state->fc = 2U;
  goto ldv_41776;
  default:
  __ret_warn_on___0 = 1;
  tmp___0 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi_phy.c.prepared",
                       521);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  case 0U:
  link_state->fc = 0U;
  goto ldv_41776;
  }
  ldv_41776:
  link_state->up = ((int )flags & 1) != 0;
  link_state->fd = (flags & 2U) != 0U;
  link_state->speed = speed;
  return;
}
}
void efx_mcdi_phy_check_fcntl(struct efx_nic *efx , u32 lpa )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  u32 rmtadv ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  if ((phy_cfg->supported_cap & 1024U) == 0U) {
    return;
  } else {
  }
  if (((int )efx->wanted_fc & 4) != 0) {
    return;
  } else {
  }
  rmtadv = 0U;
  if ((lpa & 256U) != 0U) {
    rmtadv = rmtadv | 8192U;
  } else {
  }
  if ((lpa & 512U) != 0U) {
    rmtadv = rmtadv | 16384U;
  } else {
  }
  if ((int )efx->wanted_fc & 1 && rmtadv == 16384U) {
    if ((efx->msg_enable & 4U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "warning: link partner doesn\'t support pause frames");
    } else {
    }
  } else {
  }
  return;
}
}
static bool efx_mcdi_phy_poll(struct efx_nic *efx )
{
  struct efx_link_state old_state ;
  u8 outbuf[28U] ;
  int rc ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  {
  old_state = efx->link_state;
  tmp = ldv_mutex_is_locked_8(& efx->mac_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mcdi_phy.c.prepared",
                       566);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  rc = efx_mcdi_rpc(efx, 41U, 0, 0UL, (u8 *)(& outbuf), 28UL, 0);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n",
                 "efx_mcdi_phy_poll", rc);
    } else {
    }
    efx->link_state.up = 0;
  } else {
    efx_mcdi_phy_decode_link(efx, & efx->link_state, ((efx_dword_t *)(& outbuf) + 8U)->u32[0],
                             ((efx_dword_t *)(& outbuf) + 16U)->u32[0], ((efx_dword_t *)(& outbuf) + 20U)->u32[0]);
  }
  tmp___1 = efx_link_state_equal((struct efx_link_state const *)(& efx->link_state),
                                 (struct efx_link_state const *)(& old_state));
  if ((int )tmp___1 != 0) {
    tmp___2 = 0;
  } else {
    tmp___2 = 1;
  }
  return ((bool )tmp___2);
}
}
static void efx_mcdi_phy_remove(struct efx_nic *efx )
{
  struct efx_mcdi_phy_data *phy_data ;
  {
  phy_data = (struct efx_mcdi_phy_data *)efx->phy_data;
  efx->phy_data = 0;
  kfree((void const *)phy_data);
  return;
}
}
static void efx_mcdi_phy_get_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  u8 outbuf[28U] ;
  int rc ;
  u32 tmp ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  ecmd->supported = mcdi_to_ethtool_cap(phy_cfg->media, phy_cfg->supported_cap);
  ecmd->advertising = efx->link_advertising;
  ethtool_cmd_speed_set(ecmd, efx->link_state.speed);
  ecmd->duplex = (__u8 )efx->link_state.fd;
  tmp = mcdi_to_ethtool_media(phy_cfg->media);
  ecmd->port = (__u8 )tmp;
  ecmd->phy_address = (__u8 )phy_cfg->port;
  ecmd->transceiver = 0U;
  ecmd->autoneg = (efx->link_advertising & 64U) != 0U;
  ecmd->mdio_support = (unsigned int )((__u8 )efx->mdio.mode_support) & 3U;
  rc = efx_mcdi_rpc(efx, 41U, 0, 0UL, (u8 *)(& outbuf), 28UL, 0);
  if (rc != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "%s: failed rc=%d\n",
                 "efx_mcdi_phy_get_settings", rc);
    } else {
    }
    return;
  } else {
  }
  ecmd->lp_advertising = mcdi_to_ethtool_cap(phy_cfg->media, ((efx_dword_t *)(& outbuf) + 4U)->u32[0]);
  return;
}
}
static int efx_mcdi_phy_set_settings(struct efx_nic *efx , struct ethtool_cmd *ecmd )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  u32 caps ;
  int rc ;
  u32 tmp ;
  __u32 tmp___0 ;
  __u32 tmp___1 ;
  u32 tmp___2 ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  if ((unsigned int )ecmd->autoneg != 0U) {
    tmp = ethtool_to_mcdi_cap(ecmd->advertising);
    caps = tmp | 1024U;
  } else
  if ((unsigned int )ecmd->duplex != 0U) {
    tmp___0 = ethtool_cmd_speed((struct ethtool_cmd const *)ecmd);
    switch (tmp___0) {
    case 10U:
    caps = 4U;
    goto ldv_41818;
    case 100U:
    caps = 16U;
    goto ldv_41818;
    case 1000U:
    caps = 64U;
    goto ldv_41818;
    case 10000U:
    caps = 128U;
    goto ldv_41818;
    default: ;
    return (-22);
    }
    ldv_41818: ;
  } else {
    tmp___1 = ethtool_cmd_speed((struct ethtool_cmd const *)ecmd);
    switch (tmp___1) {
    case 10U:
    caps = 2U;
    goto ldv_41824;
    case 100U:
    caps = 8U;
    goto ldv_41824;
    case 1000U:
    caps = 32U;
    goto ldv_41824;
    default: ;
    return (-22);
    }
    ldv_41824: ;
  }
  tmp___2 = efx_get_mcdi_phy_flags(efx);
  rc = efx_mcdi_set_link(efx, caps, tmp___2, (u32 )efx->loopback_mode, 0U);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((unsigned int )ecmd->autoneg != 0U) {
    efx_link_set_advertising(efx, ecmd->advertising | 64U);
    phy_cfg->forced_cap = 0U;
  } else {
    efx_link_set_advertising(efx, 0U);
    phy_cfg->forced_cap = caps;
  }
  return (0);
}
}
static int efx_mcdi_phy_test_alive(struct efx_nic *efx )
{
  u8 outbuf[4U] ;
  size_t outlen ;
  int rc ;
  {
  rc = efx_mcdi_rpc(efx, 67U, 0, 0UL, (u8 *)(& outbuf), 4UL, & outlen);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (outlen <= 3UL) {
    return (-5);
  } else {
  }
  if (((efx_dword_t *)(& outbuf))->u32[0] != 1U) {
    return (-22);
  } else {
  }
  return (0);
}
}
static char const * const mcdi_sft9001_cable_diag_names[8U] =
  { "cable.pairA.length", "cable.pairB.length", "cable.pairC.length", "cable.pairD.length",
        "cable.pairA.status", "cable.pairB.status", "cable.pairC.status", "cable.pairD.status"};
static int efx_mcdi_bist(struct efx_nic *efx , unsigned int bist_mode , int *results )
{
  unsigned int retry ;
  unsigned int i ;
  unsigned int count ;
  size_t outlen ;
  u32 status ;
  u8 *buf ;
  u8 *ptr ;
  int rc ;
  void *tmp ;
  unsigned int tmp___0 ;
  {
  count = 0U;
  tmp = kzalloc(256UL, 208U);
  buf = (u8 *)tmp;
  if ((unsigned long )buf == (unsigned long )((u8 *)0)) {
    return (-12);
  } else {
  }
  ((efx_dword_t *)buf)->u32[0] = bist_mode;
  rc = efx_mcdi_rpc(efx, 37U, (u8 const *)buf, 4UL, 0, 0UL, 0);
  if (rc != 0) {
    goto out;
  } else {
  }
  retry = 0U;
  goto ldv_41851;
  ldv_41850:
  rc = efx_mcdi_rpc(efx, 38U, 0, 0UL, buf, 256UL, & outlen);
  if (rc != 0) {
    goto out;
  } else {
  }
  status = ((efx_dword_t *)buf)->u32[0];
  if (status != 1U) {
    goto finished;
  } else {
  }
  msleep(100U);
  retry = retry + 1U;
  ldv_41851: ;
  if (retry <= 99U) {
    goto ldv_41850;
  } else {
  }
  rc = -110;
  goto out;
  finished:
  tmp___0 = count;
  count = count + 1U;
  *(results + (unsigned long )tmp___0) = status == 2U ? 1 : -1;
  if (efx->phy_type == 10U && (bist_mode == 1U || bist_mode == 2U)) {
    ptr = buf + 4UL;
    if (status == 2U && outlen > 35UL) {
      i = 0U;
      goto ldv_41854;
      ldv_41853:
      *(results + (unsigned long )(count + i)) = (int )((efx_dword_t *)ptr + (unsigned long )i)->u32[0];
      i = i + 1U;
      ldv_41854: ;
      if (i <= 7U) {
        goto ldv_41853;
      } else {
      }
    } else {
    }
    count = count + 8U;
  } else {
  }
  rc = (int )count;
  out:
  kfree((void const *)buf);
  return (rc);
}
}
static int efx_mcdi_phy_run_tests(struct efx_nic *efx , int *results , unsigned int flags )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  u32 mode ;
  int rc ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  if ((phy_cfg->flags & 64U) != 0U) {
    rc = efx_mcdi_bist(efx, 5U, results);
    if (rc < 0) {
      return (rc);
    } else {
    }
    results = results + (unsigned long )rc;
  } else {
  }
  mode = 0U;
  if ((phy_cfg->flags & 2U) != 0U) {
    if ((int )flags & 1 && (phy_cfg->flags & 4U) != 0U) {
      mode = 2U;
    } else {
      mode = 1U;
    }
  } else
  if ((phy_cfg->flags & 4U) != 0U) {
    mode = 2U;
  } else {
  }
  if (mode != 0U) {
    rc = efx_mcdi_bist(efx, mode, results);
    if (rc < 0) {
      return (rc);
    } else {
    }
    results = results + (unsigned long )rc;
  } else {
  }
  return (0);
}
}
static char const *efx_mcdi_phy_test_name(struct efx_nic *efx , unsigned int index )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  if ((phy_cfg->flags & 64U) != 0U) {
    if (index == 0U) {
      return ("bist");
    } else {
    }
    index = index - 1U;
  } else {
  }
  if ((phy_cfg->flags & 6U) != 0U) {
    if (index == 0U) {
      return ("cable");
    } else {
    }
    index = index - 1U;
    if (efx->phy_type == 10U) {
      if (index <= 7U) {
        return ((char const *)mcdi_sft9001_cable_diag_names[index]);
      } else {
      }
      index = index - 8U;
    } else {
    }
  } else {
  }
  return (0);
}
}
static int efx_mcdi_phy_get_module_eeprom(struct efx_nic *efx , struct ethtool_eeprom *ee ,
                                          u8 *data )
{
  u8 outbuf[252U] ;
  u8 inbuf[4U] ;
  size_t outlen ;
  int rc ;
  unsigned int payload_len ;
  unsigned int space_remaining ;
  unsigned int page ;
  unsigned int page_off ;
  unsigned int to_copy ;
  u8 *user_data ;
  size_t __len ;
  void *__ret ;
  {
  space_remaining = ee->len;
  user_data = data;
  page_off = ee->offset & 127U;
  page = ee->offset / 128U;
  goto ldv_41892;
  ldv_41891:
  ((efx_dword_t *)(& inbuf))->u32[0] = page;
  rc = efx_mcdi_rpc(efx, 75U, (u8 const *)(& inbuf), 4UL, (u8 *)(& outbuf), 252UL,
                    & outlen);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (outlen <= 131UL) {
    return (-5);
  } else {
  }
  payload_len = ((efx_dword_t *)(& outbuf))->u32[0];
  if (payload_len != 128U) {
    return (-5);
  } else {
  }
  payload_len = payload_len - page_off;
  to_copy = space_remaining < payload_len ? space_remaining : payload_len;
  __len = (size_t )to_copy;
  __ret = memcpy((void *)user_data, (void const *)(& outbuf) + ((unsigned long )page_off + 4UL),
                           __len);
  space_remaining = space_remaining - to_copy;
  user_data = user_data + (unsigned long )to_copy;
  page_off = 0U;
  page = page + 1U;
  ldv_41892: ;
  if (space_remaining != 0U && page <= 1U) {
    goto ldv_41891;
  } else {
  }
  return (0);
}
}
static int efx_mcdi_phy_get_module_info(struct efx_nic *efx , struct ethtool_modinfo *modinfo )
{
  struct efx_mcdi_phy_data *phy_cfg ;
  {
  phy_cfg = (struct efx_mcdi_phy_data *)efx->phy_data;
  switch (phy_cfg->media) {
  case 5U:
  modinfo->type = 1U;
  modinfo->eeprom_len = 256U;
  return (0);
  default: ;
  return (-95);
  }
}
}
struct efx_phy_operations const efx_mcdi_phy_ops =
     {& efx_mcdi_phy_probe, & efx_port_dummy_op_int, & efx_port_dummy_op_void, & efx_mcdi_phy_remove,
    & efx_mcdi_phy_reconfigure, & efx_mcdi_phy_poll, & efx_mcdi_phy_get_settings,
    & efx_mcdi_phy_set_settings, 0, & efx_mcdi_phy_test_alive, & efx_mcdi_phy_test_name,
    & efx_mcdi_phy_run_tests, & efx_mcdi_phy_get_module_eeprom, & efx_mcdi_phy_get_module_info};
void ldv_main17_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  int res_efx_mcdi_phy_probe_9 ;
  struct ethtool_cmd *var_group2 ;
  int *var_efx_mcdi_phy_run_tests_19_p1 ;
  unsigned int var_efx_mcdi_phy_run_tests_19_p2 ;
  unsigned int var_efx_mcdi_phy_test_name_20_p1 ;
  struct ethtool_eeprom *var_group3 ;
  u8 *var_efx_mcdi_phy_get_module_eeprom_21_p2 ;
  struct ethtool_modinfo *var_group4 ;
  int ldv_s_efx_mcdi_phy_ops_efx_phy_operations ;
  int tmp ;
  int tmp___0 ;
  {
  ldv_s_efx_mcdi_phy_ops_efx_phy_operations = 0;
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_41943;
  ldv_41942:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0: ;
  if (ldv_s_efx_mcdi_phy_ops_efx_phy_operations == 0) {
    res_efx_mcdi_phy_probe_9 = efx_mcdi_phy_probe(var_group1);
    ldv_check_return_value(res_efx_mcdi_phy_probe_9);
    ldv_check_return_value_probe(res_efx_mcdi_phy_probe_9);
    if (res_efx_mcdi_phy_probe_9 != 0) {
      goto ldv_module_exit;
    } else {
    }
    ldv_s_efx_mcdi_phy_ops_efx_phy_operations = ldv_s_efx_mcdi_phy_ops_efx_phy_operations + 1;
  } else {
  }
  goto ldv_41930;
  case 1: ;
  if (ldv_s_efx_mcdi_phy_ops_efx_phy_operations == 1) {
    ldv_handler_precall();
    efx_mcdi_phy_remove(var_group1);
    ldv_s_efx_mcdi_phy_ops_efx_phy_operations = 0;
  } else {
  }
  goto ldv_41930;
  case 2:
  ldv_handler_precall();
  efx_mcdi_phy_reconfigure(var_group1);
  goto ldv_41930;
  case 3:
  ldv_handler_precall();
  efx_mcdi_phy_poll(var_group1);
  goto ldv_41930;
  case 4:
  ldv_handler_precall();
  efx_mcdi_phy_get_settings(var_group1, var_group2);
  goto ldv_41930;
  case 5:
  ldv_handler_precall();
  efx_mcdi_phy_set_settings(var_group1, var_group2);
  goto ldv_41930;
  case 6:
  ldv_handler_precall();
  efx_mcdi_phy_test_alive(var_group1);
  goto ldv_41930;
  case 7:
  ldv_handler_precall();
  efx_mcdi_phy_run_tests(var_group1, var_efx_mcdi_phy_run_tests_19_p1, var_efx_mcdi_phy_run_tests_19_p2);
  goto ldv_41930;
  case 8:
  ldv_handler_precall();
  efx_mcdi_phy_test_name(var_group1, var_efx_mcdi_phy_test_name_20_p1);
  goto ldv_41930;
  case 9:
  ldv_handler_precall();
  efx_mcdi_phy_get_module_eeprom(var_group1, var_group3, var_efx_mcdi_phy_get_module_eeprom_21_p2);
  goto ldv_41930;
  case 10:
  ldv_handler_precall();
  efx_mcdi_phy_get_module_info(var_group1, var_group4);
  goto ldv_41930;
  default: ;
  goto ldv_41930;
  }
  ldv_41930: ;
  ldv_41943:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0 || ldv_s_efx_mcdi_phy_ops_efx_phy_operations != 0) {
    goto ldv_41942;
  } else {
  }
  ldv_module_exit: ;
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_363(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_364(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_365(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_366(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_367(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_368(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_369(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static unsigned int __arch_hweight32(unsigned int w )
{
  unsigned int res ;
  {
  res = 0U;
  __asm__ ("661:\n\tcall __sw_hweight32\n662:\n.pushsection .altinstructions,\"a\"\n .long 661b - .\n .long 6631f - .\n .word (4*32+23)\n .byte 662b-661b\n .byte 6641f-6631f\n.popsection\n.pushsection .discard,\"aw\",@progbits\n .byte 0xff + (6641f-6631f) - (662b-661b)\n.popsection\n.pushsection .altinstr_replacement, \"ax\"\n6631:\n\t.byte 0xf3,0x40,0x0f,0xb8,0xc7\n6641:\n\t.popsection": "=a" (res): "D" (w));
  return (res);
}
}
__inline static long PTR_ERR(void const *ptr )
{
  {
  return ((long )ptr);
}
}
__inline static long IS_ERR(void const *ptr )
{
  long tmp ;
  {
  tmp = ldv__builtin_expect((unsigned long )ptr > 0xfffffffffffff000UL, 0L);
  return (tmp);
}
}
int ldv_mutex_trylock_382(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_380(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_383(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_385(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_387(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_379(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_381(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_384(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_386(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_update_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_update_lock(struct mutex *lock ) ;
extern struct device *hwmon_device_register(struct device * ) ;
extern void hwmon_device_unregister(struct device * ) ;
__inline static struct efx_mcdi_mon *efx_mcdi_mon(struct efx_nic *efx )
{
  struct siena_nic_data *nic_data ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  return (& nic_data->hwmon);
}
}
static struct __anonstruct_efx_mcdi_sensor_type_225 const efx_mcdi_sensor_type[31U] =
  { {"Controller temp.", 1, -1},
        {"PHY temp.", 1, -1},
        {"Controller cooling", 2, -1},
        {"PHY temp.", 1, 0},
        {"PHY cooling", 2, 0},
        {"PHY temp.", 1, 1},
        {"PHY cooling", 2, 1},
        {"1.0V supply", 3, -1},
        {"1.2V supply", 3, -1},
        {"1.8V supply", 3, -1},
        {"2.5V supply", 3, -1},
        {"3.3V supply", 3, -1},
        {"12.0V supply", 3, -1},
        {"1.2V analogue supply", 3, -1},
        {"ref. voltage", 3, -1}};
static char const * const sensor_status_names[4U] = { "OK", "Warning", "Fatal", "Device failure"};
void efx_mcdi_sensor_event(struct efx_nic *efx , efx_qword_t *ev )
{
  unsigned int type ;
  unsigned int state ;
  unsigned int value ;
  char const *name ;
  char const *state_txt ;
  {
  name = 0;
  type = (unsigned int )ev->u64[0] & 255U;
  state = (unsigned int )(ev->u64[0] >> 8) & 255U;
  value = (unsigned int )(ev->u64[0] >> 16) & 65535U;
  if (type <= 30U) {
    name = efx_mcdi_sensor_type[type].label;
  } else {
  }
  if ((unsigned long )name == (unsigned long )((char const *)0)) {
    name = "No sensor name available";
  } else {
  }
  state_txt = sensor_status_names[state];
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "Sensor %d (%s) reports condition \'%s\' for raw value %d\n",
               type, name, state_txt, value);
  } else {
  }
  return;
}
}
static int efx_mcdi_mon_update(struct efx_nic *efx )
{
  struct efx_mcdi_mon *hwmon ;
  struct efx_mcdi_mon *tmp ;
  u8 inbuf[8U] ;
  int rc ;
  {
  tmp = efx_mcdi_mon(efx);
  hwmon = tmp;
  ((efx_dword_t *)(& inbuf))->u32[0] = (unsigned int )hwmon->dma_buf.dma_addr;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = (unsigned int )(hwmon->dma_buf.dma_addr >> 32);
  rc = efx_mcdi_rpc(efx, 66U, (u8 const *)(& inbuf), 8UL, 0, 0UL, 0);
  if (rc == 0) {
    hwmon->last_update = jiffies;
  } else {
  }
  return (rc);
}
}
static ssize_t efx_mcdi_mon_show_name(struct device *dev , struct device_attribute *attr ,
                                      char *buf )
{
  int tmp ;
  {
  tmp = sprintf(buf, "%s\n", (char *)"sfc");
  return ((ssize_t )tmp);
}
}
static int efx_mcdi_mon_get_entry(struct device *dev , unsigned int index , efx_dword_t *entry )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_mcdi_mon *hwmon ;
  struct efx_mcdi_mon *tmp___0 ;
  int rc ;
  {
  tmp = dev_get_drvdata((struct device const *)dev);
  efx = (struct efx_nic *)tmp;
  tmp___0 = efx_mcdi_mon(efx);
  hwmon = tmp___0;
  ldv_mutex_lock_386(& hwmon->update_lock);
  if ((long )jiffies - (long )(hwmon->last_update + 250UL) < 0L) {
    rc = 0;
  } else {
    rc = efx_mcdi_mon_update(efx);
  }
  *entry = *((efx_dword_t *)hwmon->dma_buf.addr + (unsigned long )index);
  ldv_mutex_unlock_387(& hwmon->update_lock);
  return (rc);
}
}
static ssize_t efx_mcdi_mon_show_value(struct device *dev , struct device_attribute *attr ,
                                       char *buf )
{
  struct efx_mcdi_mon_attribute *mon_attr ;
  struct device_attribute const *__mptr ;
  efx_dword_t entry ;
  unsigned int value ;
  int rc ;
  int tmp ;
  {
  __mptr = (struct device_attribute const *)attr;
  mon_attr = (struct efx_mcdi_mon_attribute *)__mptr;
  rc = efx_mcdi_mon_get_entry(dev, mon_attr->index, & entry);
  if (rc != 0) {
    return ((ssize_t )rc);
  } else {
  }
  value = entry.u32[0] & 65535U;
  if ((unsigned int )efx_mcdi_sensor_type[mon_attr->type].hwmon_type == 1U) {
    value = value * 1000U;
  } else {
  }
  tmp = sprintf(buf, "%u\n", value);
  return ((ssize_t )tmp);
}
}
static ssize_t efx_mcdi_mon_show_limit(struct device *dev , struct device_attribute *attr ,
                                       char *buf )
{
  struct efx_mcdi_mon_attribute *mon_attr ;
  struct device_attribute const *__mptr ;
  unsigned int value ;
  int tmp ;
  {
  __mptr = (struct device_attribute const *)attr;
  mon_attr = (struct efx_mcdi_mon_attribute *)__mptr;
  value = mon_attr->limit_value;
  if ((unsigned int )efx_mcdi_sensor_type[mon_attr->type].hwmon_type == 1U) {
    value = value * 1000U;
  } else {
  }
  tmp = sprintf(buf, "%u\n", value);
  return ((ssize_t )tmp);
}
}
static ssize_t efx_mcdi_mon_show_alarm(struct device *dev , struct device_attribute *attr ,
                                       char *buf )
{
  struct efx_mcdi_mon_attribute *mon_attr ;
  struct device_attribute const *__mptr ;
  efx_dword_t entry ;
  int state ;
  int rc ;
  int tmp ;
  {
  __mptr = (struct device_attribute const *)attr;
  mon_attr = (struct efx_mcdi_mon_attribute *)__mptr;
  rc = efx_mcdi_mon_get_entry(dev, mon_attr->index, & entry);
  if (rc != 0) {
    return ((ssize_t )rc);
  } else {
  }
  state = (int )(entry.u32[0] >> 16) & 255;
  tmp = sprintf(buf, "%d\n", state != 0);
  return ((ssize_t )tmp);
}
}
static ssize_t efx_mcdi_mon_show_label(struct device *dev , struct device_attribute *attr ,
                                       char *buf )
{
  struct efx_mcdi_mon_attribute *mon_attr ;
  struct device_attribute const *__mptr ;
  int tmp ;
  {
  __mptr = (struct device_attribute const *)attr;
  mon_attr = (struct efx_mcdi_mon_attribute *)__mptr;
  tmp = sprintf(buf, "%s\n", efx_mcdi_sensor_type[mon_attr->type].label);
  return ((ssize_t )tmp);
}
}
static int efx_mcdi_mon_add_attr(struct efx_nic *efx , char const *name , ssize_t (*reader)(struct device * ,
                                                                                              struct device_attribute * ,
                                                                                              char * ) ,
                                 unsigned int index , unsigned int type , unsigned int limit_value )
{
  struct efx_mcdi_mon *hwmon ;
  struct efx_mcdi_mon *tmp ;
  struct efx_mcdi_mon_attribute *attr ;
  int rc ;
  struct lock_class_key __key ;
  {
  tmp = efx_mcdi_mon(efx);
  hwmon = tmp;
  attr = hwmon->attrs + (unsigned long )hwmon->n_attrs;
  strlcpy((char *)(& attr->name), name, 12UL);
  attr->index = index;
  attr->type = type;
  attr->limit_value = limit_value;
  attr->dev_attr.attr.key = & __key;
  attr->dev_attr.attr.name = (char const *)(& attr->name);
  attr->dev_attr.attr.mode = 292U;
  attr->dev_attr.show = reader;
  rc = device_create_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& attr->dev_attr));
  if (rc == 0) {
    hwmon->n_attrs = hwmon->n_attrs + 1U;
  } else {
  }
  return (rc);
}
}
int efx_mcdi_mon_probe(struct efx_nic *efx )
{
  struct efx_mcdi_mon *hwmon ;
  struct efx_mcdi_mon *tmp ;
  unsigned int n_attrs ;
  unsigned int n_temp ;
  unsigned int n_cool ;
  unsigned int n_in ;
  u8 outbuf[252U] ;
  size_t outlen ;
  char name[12U] ;
  u32 mask ;
  int rc ;
  int i ;
  int type ;
  unsigned int tmp___0 ;
  struct lock_class_key __key ;
  unsigned int tmp___1 ;
  void *tmp___2 ;
  long tmp___3 ;
  long tmp___4 ;
  char const *hwmon_prefix ;
  unsigned int hwmon_index ;
  u16 min1 ;
  u16 max1 ;
  u16 min2 ;
  u16 max2 ;
  unsigned int tmp___5 ;
  unsigned int tmp___6 ;
  {
  tmp = efx_mcdi_mon(efx);
  hwmon = tmp;
  n_temp = 0U;
  n_cool = 0U;
  n_in = 0U;
  rc = efx_mcdi_rpc(efx, 65U, 0, 0UL, (u8 *)(& outbuf), 252UL, & outlen);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (outlen <= 11UL) {
    return (-5);
  } else {
  }
  mask = ((efx_dword_t *)(& outbuf))->u32[0];
  if (mask == 0U) {
    return (0);
  } else {
  }
  tmp___0 = __arch_hweight32(mask);
  if ((size_t )(tmp___0 * 8U + 4U) > outlen) {
    return (-5);
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & hwmon->dma_buf, 124U);
  if (rc != 0) {
    return (rc);
  } else {
  }
  __mutex_init(& hwmon->update_lock, "&hwmon->update_lock", & __key);
  efx_mcdi_mon_update(efx);
  tmp___1 = __arch_hweight32(mask);
  n_attrs = tmp___1 * 6U + 1U;
  tmp___2 = kcalloc((size_t )n_attrs, 72UL, 208U);
  hwmon->attrs = (struct efx_mcdi_mon_attribute *)tmp___2;
  if ((unsigned long )hwmon->attrs == (unsigned long )((struct efx_mcdi_mon_attribute *)0)) {
    rc = -12;
    goto fail;
  } else {
  }
  hwmon->device = hwmon_device_register(& (efx->pci_dev)->dev);
  tmp___4 = IS_ERR((void const *)hwmon->device);
  if (tmp___4 != 0L) {
    tmp___3 = PTR_ERR((void const *)hwmon->device);
    rc = (int )tmp___3;
    goto fail;
  } else {
  }
  rc = efx_mcdi_mon_add_attr(efx, "name", & efx_mcdi_mon_show_name, 0U, 0U, 0U);
  if (rc != 0) {
    goto fail;
  } else {
  }
  i = 0;
  type = -1;
  ldv_41721:
  type = type + 1;
  goto ldv_41714;
  ldv_41713:
  type = type + 1;
  if (type == 32) {
    return (0);
  } else {
  }
  ldv_41714: ;
  if (((u32 )(1 << type) & mask) == 0U) {
    goto ldv_41713;
  } else {
  }
  if ((unsigned int )efx_mcdi_sensor_type[type].hwmon_type != 0U && (int )efx_mcdi_sensor_type[type].port >= 0) {
    tmp___5 = efx_port_num(efx);
    if ((unsigned int )efx_mcdi_sensor_type[type].port != tmp___5) {
      goto ldv_41716;
    } else {
    }
  } else {
  }
  switch ((unsigned int )efx_mcdi_sensor_type[type].hwmon_type) {
  case 1U:
  hwmon_prefix = "temp";
  n_temp = n_temp + 1U;
  hwmon_index = n_temp;
  goto ldv_41718;
  case 2U:
  hwmon_prefix = "fan";
  n_cool = n_cool + 1U;
  hwmon_index = n_cool;
  goto ldv_41718;
  default:
  hwmon_prefix = "in";
  tmp___6 = n_in;
  n_in = n_in + 1U;
  hwmon_index = tmp___6;
  goto ldv_41718;
  }
  ldv_41718:
  min1 = (u16 )((efx_dword_t *)(& outbuf) + ((unsigned long )(i * 8) + 4UL))->u32[0];
  max1 = (u16 )(((efx_dword_t *)(& outbuf) + ((unsigned long )(i * 8) + 4UL))->u32[0] >> 16);
  min2 = (u16 )((efx_dword_t *)(& outbuf) + ((unsigned long )(i * 8) + 8UL))->u32[0];
  max2 = (u16 )(((efx_dword_t *)(& outbuf) + ((unsigned long )(i * 8) + 8UL))->u32[0] >> 16);
  if ((int )min1 != (int )max1) {
    snprintf((char *)(& name), 12UL, "%s%u_input", hwmon_prefix, hwmon_index);
    rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_value,
                               (unsigned int )i, (unsigned int )type, 0U);
    if (rc != 0) {
      goto fail;
    } else {
    }
    snprintf((char *)(& name), 12UL, "%s%u_min", hwmon_prefix, hwmon_index);
    rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_limit,
                               (unsigned int )i, (unsigned int )type, (unsigned int )min1);
    if (rc != 0) {
      goto fail;
    } else {
    }
    snprintf((char *)(& name), 12UL, "%s%u_max", hwmon_prefix, hwmon_index);
    rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_limit,
                               (unsigned int )i, (unsigned int )type, (unsigned int )max1);
    if (rc != 0) {
      goto fail;
    } else {
    }
    if ((int )min2 != (int )max2) {
      snprintf((char *)(& name), 12UL, "%s%u_crit", hwmon_prefix, hwmon_index);
      rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_limit,
                                 (unsigned int )i, (unsigned int )type, (unsigned int )max2);
      if (rc != 0) {
        goto fail;
      } else {
      }
    } else {
    }
  } else {
  }
  snprintf((char *)(& name), 12UL, "%s%u_alarm", hwmon_prefix, hwmon_index);
  rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_alarm,
                             (unsigned int )i, (unsigned int )type, 0U);
  if (rc != 0) {
    goto fail;
  } else {
  }
  if ((unsigned long )efx_mcdi_sensor_type[type].label != (unsigned long )((char const * )0)) {
    snprintf((char *)(& name), 12UL, "%s%u_label", hwmon_prefix, hwmon_index);
    rc = efx_mcdi_mon_add_attr(efx, (char const *)(& name), & efx_mcdi_mon_show_label,
                               (unsigned int )i, (unsigned int )type, 0U);
    if (rc != 0) {
      goto fail;
    } else {
    }
  } else {
  }
  ldv_41716:
  i = i + 1;
  goto ldv_41721;
  fail:
  efx_mcdi_mon_remove(efx);
  return (rc);
}
}
void efx_mcdi_mon_remove(struct efx_nic *efx )
{
  struct siena_nic_data *nic_data ;
  struct efx_mcdi_mon *hwmon ;
  unsigned int i ;
  {
  nic_data = (struct siena_nic_data *)efx->nic_data;
  hwmon = & nic_data->hwmon;
  i = 0U;
  goto ldv_41729;
  ldv_41728:
  device_remove_file(& (efx->pci_dev)->dev, (struct device_attribute const *)(& (hwmon->attrs + (unsigned long )i)->dev_attr));
  i = i + 1U;
  ldv_41729: ;
  if (hwmon->n_attrs > i) {
    goto ldv_41728;
  } else {
  }
  kfree((void const *)hwmon->attrs);
  if ((unsigned long )hwmon->device != (unsigned long )((struct device *)0)) {
    hwmon_device_unregister(hwmon->device);
  } else {
  }
  efx_nic_free_buffer(efx, & hwmon->dma_buf);
  return;
}
}
void ldv_mutex_lock_379(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_380(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_381(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_382(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_383(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_384(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_385(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_386(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_update_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_387(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_update_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
extern void might_fault(void) ;
extern void __list_add(struct list_head * , struct list_head * , struct list_head * ) ;
__inline static void list_add(struct list_head *new , struct list_head *head )
{
  {
  __list_add(new, head, head->next);
  return;
}
}
__inline static void list_add_tail(struct list_head *new , struct list_head *head )
{
  {
  __list_add(new, head->prev, head);
  return;
}
}
extern void __list_del_entry(struct list_head * ) ;
extern void list_del(struct list_head * ) ;
__inline static void list_move(struct list_head *list , struct list_head *head )
{
  {
  __list_del_entry(list);
  list_add(list, head);
  return;
}
}
__inline static int list_empty(struct list_head const *head )
{
  {
  return ((unsigned long )((struct list_head const *)head->next) == (unsigned long )head);
}
}
extern void warn_slowpath_fmt(char const * , int const , char const * , ...) ;
__inline static u32 __iter_div_u64_rem(u64 dividend , u32 divisor , u64 *remainder )
{
  u32 ret ;
  {
  ret = 0U;
  goto ldv_4950;
  ldv_4949:
  __asm__ ("": "+rm" (dividend));
  dividend = dividend - (u64 )divisor;
  ret = ret + 1U;
  ldv_4950: ;
  if ((u64 )divisor <= dividend) {
    goto ldv_4949;
  } else {
  }
  *remainder = dividend;
  return (ret);
}
}
int ldv_mutex_trylock_400(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_398(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_401(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_403(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_397(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_399(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_402(struct mutex *ldv_func_arg1 ) ;
__inline static int timespec_compare(struct timespec const *lhs , struct timespec const *rhs )
{
  {
  if ((long )lhs->tv_sec < (long )rhs->tv_sec) {
    return (-1);
  } else {
  }
  if ((long )lhs->tv_sec > (long )rhs->tv_sec) {
    return (1);
  } else {
  }
  return ((int )((unsigned int )lhs->tv_nsec - (unsigned int )rhs->tv_nsec));
}
}
extern void set_normalized_timespec(struct timespec * , time_t , s64 ) ;
__inline static struct timespec timespec_sub(struct timespec lhs , struct timespec rhs )
{
  struct timespec ts_delta ;
  {
  set_normalized_timespec(& ts_delta, lhs.tv_sec - rhs.tv_sec, (s64 )(lhs.tv_nsec - rhs.tv_nsec));
  return (ts_delta);
}
}
extern void getnstimeofday(struct timespec * ) ;
__inline static s64 timespec_to_ns(struct timespec const *ts )
{
  {
  return ((long long )ts->tv_sec * 1000000000LL + (long long )ts->tv_nsec);
}
}
extern struct timespec ns_to_timespec(s64 const ) ;
__inline static void timespec_add_ns(struct timespec *a , u64 ns )
{
  u32 tmp ;
  {
  tmp = __iter_div_u64_rem((unsigned long long )a->tv_nsec + ns, 1000000000U, & ns);
  a->tv_sec = a->tv_sec + (__kernel_time_t )tmp;
  a->tv_nsec = (long )ns;
  return;
}
}
__inline static ktime_t ktime_set(long const secs , unsigned long const nsecs )
{
  ktime_t __constr_expr_0 ;
  long tmp ;
  ktime_t __constr_expr_1 ;
  {
  tmp = ldv__builtin_expect((long long )secs > 9223372035LL, 0L);
  if (tmp != 0L) {
    __constr_expr_0.tv64 = 9223372036854775807LL;
    return (__constr_expr_0);
  } else {
  }
  __constr_expr_1.tv64 = (long long )secs * 1000000000LL + (long long )nsecs;
  return (__constr_expr_1);
}
}
extern unsigned long _copy_to_user(void * , void const * , unsigned int ) ;
extern unsigned long _copy_from_user(void * , void const * , unsigned int ) ;
__inline static unsigned long copy_from_user(void *to , void const *from , unsigned long n )
{
  int sz ;
  unsigned long tmp ;
  int __ret_warn_on ;
  long tmp___0 ;
  long tmp___1 ;
  long tmp___2 ;
  {
  tmp = __builtin_object_size((void const *)to, 0);
  sz = (int )tmp;
  might_fault();
  tmp___1 = ldv__builtin_expect(sz == -1, 1L);
  if (tmp___1 != 0L) {
    n = _copy_from_user(to, from, (unsigned int )n);
  } else {
    tmp___2 = ldv__builtin_expect((unsigned long )sz >= n, 1L);
    if (tmp___2 != 0L) {
      n = _copy_from_user(to, from, (unsigned int )n);
    } else {
      __ret_warn_on = 1;
      tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
      if (tmp___0 != 0L) {
        warn_slowpath_fmt("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/inst/current/envs/linux-3.8-rc1/linux-3.8-rc1/arch/x86/include/asm/uaccess_64.h",
                          66, "Buffer overflow detected!\n");
      } else {
      }
      ldv__builtin_expect(__ret_warn_on != 0, 0L);
    }
  }
  return (n);
}
}
__inline static int copy_to_user(void *dst , void const *src , unsigned int size )
{
  unsigned long tmp ;
  {
  might_fault();
  tmp = _copy_to_user(dst, src, size);
  return ((int )tmp);
}
}
__inline static struct skb_shared_hwtstamps *skb_hwtstamps(struct sk_buff *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  return (& ((struct skb_shared_info *)tmp)->hwtstamps);
}
}
__inline static int skb_queue_empty(struct sk_buff_head const *list )
{
  {
  return ((unsigned long )((struct sk_buff *)list->next) == (unsigned long )((struct sk_buff *)list));
}
}
__inline static struct sk_buff *skb_peek(struct sk_buff_head const *list_ )
{
  struct sk_buff *skb ;
  {
  skb = list_->next;
  if ((unsigned long )skb == (unsigned long )((struct sk_buff *)list_)) {
    skb = 0;
  } else {
  }
  return (skb);
}
}
__inline static void __skb_queue_head_init(struct sk_buff_head *list )
{
  struct sk_buff *tmp ;
  {
  tmp = (struct sk_buff *)list;
  list->next = tmp;
  list->prev = tmp;
  list->qlen = 0U;
  return;
}
}
__inline static void skb_queue_head_init(struct sk_buff_head *list )
{
  struct lock_class_key __key ;
  {
  spinlock_check(& list->lock);
  __raw_spin_lock_init(& list->lock.ldv_5961.rlock, "&(&list->lock)->rlock", & __key);
  __skb_queue_head_init(list);
  return;
}
}
__inline static void __skb_insert(struct sk_buff *newsk , struct sk_buff *prev , struct sk_buff *next ,
                                  struct sk_buff_head *list )
{
  struct sk_buff *tmp ;
  {
  newsk->next = next;
  newsk->prev = prev;
  tmp = newsk;
  prev->next = tmp;
  next->prev = tmp;
  list->qlen = list->qlen + 1U;
  return;
}
}
__inline static void __skb_queue_before(struct sk_buff_head *list , struct sk_buff *next ,
                                        struct sk_buff *newsk )
{
  {
  __skb_insert(newsk, next->prev, next, list);
  return;
}
}
extern void skb_queue_head(struct sk_buff_head * , struct sk_buff * ) ;
extern void skb_queue_tail(struct sk_buff_head * , struct sk_buff * ) ;
__inline static void __skb_queue_tail(struct sk_buff_head *list , struct sk_buff *newsk )
{
  {
  __skb_queue_before(list, (struct sk_buff *)list, newsk);
  return;
}
}
__inline static void __skb_unlink(struct sk_buff *skb , struct sk_buff_head *list )
{
  struct sk_buff *next ;
  struct sk_buff *prev ;
  struct sk_buff *tmp ;
  {
  list->qlen = list->qlen - 1U;
  next = skb->next;
  prev = skb->prev;
  tmp = 0;
  skb->prev = tmp;
  skb->next = tmp;
  next->prev = prev;
  prev->next = next;
  return;
}
}
extern struct sk_buff *skb_dequeue(struct sk_buff_head * ) ;
__inline static struct sk_buff *__skb_dequeue(struct sk_buff_head *list )
{
  struct sk_buff *skb ;
  struct sk_buff *tmp ;
  {
  tmp = skb_peek((struct sk_buff_head const *)list);
  skb = tmp;
  if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
    __skb_unlink(skb, list);
  } else {
  }
  return (skb);
}
}
__inline static bool skb_is_nonlinear(struct sk_buff const *skb )
{
  {
  return ((unsigned int )skb->data_len != 0U);
}
}
extern unsigned char *__pskb_pull_tail(struct sk_buff * , int ) ;
extern void skb_queue_purge(struct sk_buff_head * ) ;
__inline static int __skb_linearize(struct sk_buff *skb )
{
  unsigned char *tmp ;
  {
  tmp = __pskb_pull_tail(skb, (int )skb->data_len);
  return ((unsigned long )tmp != (unsigned long )((unsigned char *)0) ? 0 : -12);
}
}
__inline static int skb_linearize(struct sk_buff *skb )
{
  int tmp___0 ;
  int tmp___1 ;
  bool tmp___2 ;
  {
  tmp___2 = skb_is_nonlinear((struct sk_buff const *)skb);
  if ((int )tmp___2) {
    tmp___0 = __skb_linearize(skb);
    tmp___1 = tmp___0;
  } else {
    tmp___1 = 0;
  }
  return (tmp___1);
}
}
__inline static void skb_copy_from_linear_data(struct sk_buff const *skb , void *to ,
                                               unsigned int const len )
{
  size_t __len ;
  void *__ret ;
  {
  __len = (size_t )len;
  __ret = memcpy(to, (void const *)skb->data, __len);
  return;
}
}
extern void skb_tstamp_tx(struct sk_buff * , struct skb_shared_hwtstamps * ) ;
extern int skb_checksum_help(struct sk_buff * ) ;
__inline static struct udphdr *udp_hdr(struct sk_buff const *skb )
{
  unsigned char *tmp ;
  {
  tmp = skb_transport_header(skb);
  return ((struct udphdr *)tmp);
}
}
__inline static void pps_get_ts(struct pps_event_time *ts )
{
  {
  getnstimeofday(& ts->ts_real);
  return;
}
}
__inline static void pps_sub_ts(struct pps_event_time *ts , struct timespec delta )
{
  {
  ts->ts_real = timespec_sub(ts->ts_real, delta);
  return;
}
}
extern struct ptp_clock *ptp_clock_register(struct ptp_clock_info * , struct device * ) ;
extern int ptp_clock_unregister(struct ptp_clock * ) ;
extern void ptp_clock_event(struct ptp_clock * , struct ptp_clock_event * ) ;
extern int ptp_clock_index(struct ptp_clock * ) ;
__inline static void efx_xmit_hwtstamp_pending(struct sk_buff *skb )
{
  unsigned char *tmp ;
  unsigned char *tmp___0 ;
  {
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  tmp___0 = skb_end_pointer((struct sk_buff const *)skb);
  ((struct skb_shared_info *)tmp)->tx_flags = (__u8 )((unsigned int )((struct skb_shared_info *)tmp___0)->tx_flags | 4U);
  return;
}
}
static int efx_phc_adjfreq(struct ptp_clock_info *ptp , s32 delta ) ;
static int efx_phc_adjtime(struct ptp_clock_info *ptp , s64 delta ) ;
static int efx_phc_gettime(struct ptp_clock_info *ptp , struct timespec *ts ) ;
static int efx_phc_settime(struct ptp_clock_info *ptp , struct timespec const *e_ts ) ;
static int efx_phc_enable(struct ptp_clock_info *ptp , struct ptp_clock_request *request ,
                          int enable ) ;
static int efx_ptp_enable(struct efx_nic *efx )
{
  u8 inbuf[16U] ;
  int tmp ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = 1U;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = (unsigned int )((efx->ptp_data)->channel)->channel;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = (efx->ptp_data)->mode;
  tmp = efx_mcdi_rpc(efx, 11U, (u8 const *)(& inbuf), 16UL, 0, 0UL, 0);
  return (tmp);
}
}
static int efx_ptp_disable(struct efx_nic *efx )
{
  u8 inbuf[8U] ;
  int tmp ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = 2U;
  tmp = efx_mcdi_rpc(efx, 11U, (u8 const *)(& inbuf), 8UL, 0, 0UL, 0);
  return (tmp);
}
}
static void efx_ptp_deliver_rx_queue(struct sk_buff_head *q )
{
  struct sk_buff *skb ;
  {
  goto ldv_45992;
  ldv_45991:
  local_bh_disable();
  netif_receive_skb(skb);
  local_bh_enable();
  ldv_45992:
  skb = skb_dequeue(q);
  if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
    goto ldv_45991;
  } else {
  }
  return;
}
}
static void efx_ptp_handle_no_channel(struct efx_nic *efx )
{
  {
  if ((int )efx->msg_enable & 1) {
    netdev_err((struct net_device const *)efx->net_dev, "ERROR: PTP requires MSI-X and 1 additional interruptvector. PTP disabled\n");
  } else {
  }
  return;
}
}
static void efx_ptp_send_times(struct efx_nic *efx , struct pps_event_time *last_time )
{
  struct pps_event_time now ;
  struct timespec limit ;
  struct efx_ptp_data *ptp ;
  struct timespec start ;
  int *mc_running ;
  struct timespec update_time ;
  unsigned int host_time ;
  int tmp ;
  int tmp___0 ;
  {
  ptp = efx->ptp_data;
  mc_running = (int *)ptp->start.addr;
  pps_get_ts(& now);
  start = now.ts_real;
  limit = now.ts_real;
  timespec_add_ns(& limit, 250000ULL);
  goto ldv_46011;
  ldv_46010:
  update_time = now.ts_real;
  timespec_add_ns(& update_time, 200ULL);
  ldv_46008:
  pps_get_ts(& now);
  tmp = timespec_compare((struct timespec const *)(& now.ts_real), (struct timespec const *)(& update_time));
  if (tmp < 0 && (int )*((int volatile *)mc_running) != 0) {
    goto ldv_46008;
  } else {
  }
  host_time = (unsigned int )((int )(now.ts_real.tv_sec << 30) | (int )now.ts_real.tv_nsec);
  _efx_writed(efx, host_time, 16713712U);
  ldv_46011:
  tmp___0 = timespec_compare((struct timespec const *)(& now.ts_real), (struct timespec const *)(& limit));
  if (tmp___0 < 0 && (int )*((int volatile *)mc_running) != 0) {
    goto ldv_46010;
  } else {
  }
  *last_time = now;
  return;
}
}
static void efx_ptp_read_timeset(u8 *data , struct efx_ptp_timeset *timeset )
{
  unsigned int start_ns ;
  unsigned int end_ns ;
  {
  timeset->host_start = ((efx_dword_t *)data)->u32[0];
  timeset->seconds = ((efx_dword_t *)data + 4U)->u32[0];
  timeset->nanoseconds = ((efx_dword_t *)data + 8U)->u32[0];
  timeset->host_end = ((efx_dword_t *)data + 12U)->u32[0];
  timeset->waitns = ((efx_dword_t *)data + 16U)->u32[0];
  start_ns = timeset->host_start & 1073741823U;
  end_ns = timeset->host_end & 1073741823U;
  if (end_ns < start_ns) {
    end_ns = end_ns + 1000000000U;
  } else {
  }
  timeset->window = end_ns - start_ns;
  return;
}
}
static int efx_ptp_process_times(struct efx_nic *efx , u8 *synch_buf , size_t response_length ,
                                 struct pps_event_time const *last_time )
{
  unsigned int number_readings ;
  unsigned int i ;
  unsigned int min ;
  unsigned int min_set ;
  unsigned int total ;
  unsigned int ngood ;
  unsigned int last_good ;
  struct efx_ptp_data *ptp ;
  bool min_valid ;
  u32 last_sec ;
  u32 start_sec ;
  struct timespec delta ;
  unsigned int win ;
  {
  number_readings = (unsigned int )(response_length / 20UL);
  min_set = 0U;
  ngood = 0U;
  last_good = 0U;
  ptp = efx->ptp_data;
  min_valid = 0;
  if (number_readings == 0U) {
    return (-11);
  } else {
  }
  i = 0U;
  goto ldv_46038;
  ldv_46037:
  efx_ptp_read_timeset(synch_buf, (struct efx_ptp_timeset *)(& ptp->timeset) + (unsigned long )i);
  synch_buf = synch_buf + 20UL;
  if (ptp->timeset[i].window > 200U) {
    if ((int )min_valid) {
      if (ptp->timeset[i].window < min_set) {
        min_set = ptp->timeset[i].window;
      } else {
        min_valid = 1;
        min_set = ptp->timeset[i].window;
      }
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_46038: ;
  if (i < number_readings) {
    goto ldv_46037;
  } else {
  }
  if ((int )min_valid) {
    if ((int )ptp->base_sync_valid && ptp->base_sync_ns < min_set) {
      min = ptp->base_sync_ns;
    } else {
      min = min_set;
    }
  } else {
    min = 200U;
  }
  total = 0U;
  i = 0U;
  goto ldv_46042;
  ldv_46041: ;
  if (ptp->timeset[i].window > ptp->timeset[i].waitns) {
    win = ptp->timeset[i].window - ptp->timeset[i].waitns;
    if (win > 119U && win <= 999U) {
      total = ptp->timeset[i].window + total;
      ngood = ngood + 1U;
      last_good = i;
    } else {
    }
  } else {
  }
  i = i + 1U;
  ldv_46042: ;
  if (i < number_readings) {
    goto ldv_46041;
  } else {
  }
  if (ngood == 0U) {
    if ((int )efx->msg_enable & 1) {
      netdev_warn((struct net_device const *)efx->net_dev, "PTP no suitable synchronisations %dns %dns\n",
                  ptp->base_sync_ns, min_set);
    } else {
    }
    return (-11);
  } else {
  }
  ptp->last_sync_ns = ((total + ngood) - 1U) / ngood;
  if (! ptp->base_sync_valid || ptp->last_sync_ns < ptp->base_sync_ns) {
    ptp->base_sync_valid = 1;
    ptp->base_sync_ns = ptp->last_sync_ns;
  } else {
  }
  delta.tv_nsec = ((long )ptp->timeset[last_good].nanoseconds + (long )last_time->ts_real.tv_nsec) - ((long )ptp->timeset[last_good].host_start & 1073741823L);
  start_sec = ptp->timeset[last_good].host_start >> 30;
  last_sec = (u32 )last_time->ts_real.tv_sec & 3U;
  if (start_sec != last_sec) {
    if (((start_sec + 1U) & 3U) != last_sec) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_warn((struct net_device const *)efx->net_dev, "PTP bad synchronisation seconds\n");
      } else {
      }
      return (-11);
    } else {
      delta.tv_sec = 1L;
    }
  } else {
    delta.tv_sec = 0L;
  }
  ptp->host_time_pps = *last_time;
  pps_sub_ts(& ptp->host_time_pps, delta);
  return (0);
}
}
static int efx_ptp_synchronize(struct efx_nic *efx , unsigned int num_readings )
{
  struct efx_ptp_data *ptp ;
  u8 synch_buf[240U] ;
  size_t response_length ;
  int rc ;
  unsigned long timeout ;
  struct pps_event_time last_time ;
  unsigned int loops ;
  int *start ;
  unsigned long tmp ;
  {
  ptp = efx->ptp_data;
  last_time.ts_real.tv_sec = 0L;
  last_time.ts_real.tv_nsec = 0L;
  loops = 0U;
  start = (int *)ptp->start.addr;
  ((efx_dword_t *)(& synch_buf))->u32[0] = 7U;
  ((efx_dword_t *)(& synch_buf) + 8U)->u32[0] = num_readings;
  ((efx_dword_t *)(& synch_buf) + 12U)->u32[0] = (unsigned int )ptp->start.dma_addr;
  ((efx_dword_t *)(& synch_buf) + 16U)->u32[0] = (unsigned int )(ptp->start.dma_addr >> 32);
  *((int volatile *)start) = 0;
  efx_mcdi_rpc_start(efx, 11U, (u8 const *)(& synch_buf), 20UL);
  tmp = msecs_to_jiffies(2U);
  timeout = tmp + (unsigned long )jiffies;
  goto ldv_46063;
  ldv_46062:
  __const_udelay(85900UL);
  loops = loops + 1U;
  ldv_46063: ;
  if ((int )*((int volatile *)start) == 0 && (long )jiffies - (long )timeout < 0L) {
    goto ldv_46062;
  } else {
  }
  if ((int )*((int volatile *)start) != 0) {
    efx_ptp_send_times(efx, & last_time);
  } else {
  }
  rc = efx_mcdi_rpc_finish(efx, 11U, 20UL, (u8 *)(& synch_buf), 240UL, & response_length);
  if (rc == 0) {
    rc = efx_ptp_process_times(efx, (u8 *)(& synch_buf), response_length, (struct pps_event_time const *)(& last_time));
  } else {
  }
  return (rc);
}
}
static int efx_ptp_xmit_skb(struct efx_nic *efx , struct sk_buff *skb )
{
  u8 *txbuf ;
  struct skb_shared_hwtstamps timestamps ;
  int rc ;
  size_t len ;
  u8 txtime[8U] ;
  unsigned char *tmp ;
  {
  txbuf = (u8 *)(& (efx->ptp_data)->txbuf);
  rc = -5;
  len = (size_t )(skb->len + 15U) & 4294967292UL;
  ((efx_dword_t *)txbuf)->u32[0] = 3U;
  ((efx_dword_t *)txbuf + 8U)->u32[0] = skb->len;
  tmp = skb_end_pointer((struct sk_buff const *)skb);
  if ((unsigned int )((struct skb_shared_info *)tmp)->nr_frags != 0U) {
    rc = skb_linearize(skb);
    if (rc != 0) {
      goto fail;
    } else {
    }
  } else {
  }
  if ((unsigned int )*((unsigned char *)skb + 124UL) == 12U) {
    rc = skb_checksum_help(skb);
    if (rc != 0) {
      goto fail;
    } else {
    }
  } else {
  }
  skb_copy_from_linear_data((struct sk_buff const *)skb, (void *)txbuf + 12U, (unsigned int const )len);
  rc = efx_mcdi_rpc(efx, 11U, (u8 const *)txbuf, len, (u8 *)(& txtime), 8UL, & len);
  if (rc != 0) {
    goto fail;
  } else {
  }
  memset((void *)(& timestamps), 0, 16UL);
  timestamps.hwtstamp = ktime_set((long const )((efx_dword_t *)(& txtime))->u32[0],
                                  (unsigned long const )((efx_dword_t *)(& txtime) + 4U)->u32[0]);
  skb_tstamp_tx(skb, & timestamps);
  rc = 0;
  fail:
  consume_skb(skb);
  return (rc);
}
}
static void efx_ptp_drop_time_expired_events(struct efx_nic *efx )
{
  struct efx_ptp_data *ptp ;
  struct list_head *cursor ;
  struct list_head *next ;
  struct efx_ptp_event_rx *evt ;
  struct list_head const *__mptr ;
  int tmp ;
  {
  ptp = efx->ptp_data;
  spin_lock_bh(& ptp->evt_lock);
  tmp = list_empty((struct list_head const *)(& ptp->evt_list));
  if (tmp == 0) {
    cursor = ptp->evt_list.next;
    next = cursor->next;
    goto ldv_46091;
    ldv_46090:
    __mptr = (struct list_head const *)cursor;
    evt = (struct efx_ptp_event_rx *)__mptr;
    if ((long )evt->expiry - (long )jiffies < 0L) {
      list_move(& evt->link, & ptp->evt_free_list);
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_warn((struct net_device const *)efx->net_dev, "PTP rx event dropped\n");
      } else {
      }
    } else {
    }
    cursor = next;
    next = cursor->next;
    ldv_46091: ;
    if ((unsigned long )(& ptp->evt_list) != (unsigned long )cursor) {
      goto ldv_46090;
    } else {
    }
  } else {
  }
  spin_unlock_bh(& ptp->evt_lock);
  return;
}
}
static enum ptp_packet_state efx_ptp_match_rx(struct efx_nic *efx , struct sk_buff *skb )
{
  struct efx_ptp_data *ptp ;
  bool evts_waiting ;
  struct list_head *cursor ;
  struct list_head *next ;
  struct efx_ptp_match *match ;
  enum ptp_packet_state rc ;
  int tmp ;
  struct efx_ptp_event_rx *evt ;
  struct list_head const *__mptr ;
  struct skb_shared_hwtstamps *timestamps ;
  {
  ptp = efx->ptp_data;
  rc = 0;
  spin_lock_bh(& ptp->evt_lock);
  tmp = list_empty((struct list_head const *)(& ptp->evt_list));
  evts_waiting = tmp == 0;
  spin_unlock_bh(& ptp->evt_lock);
  if (! evts_waiting) {
    return (0);
  } else {
  }
  match = (struct efx_ptp_match *)(& skb->cb);
  spin_lock_bh(& ptp->evt_lock);
  cursor = ptp->evt_list.next;
  next = cursor->next;
  goto ldv_46109;
  ldv_46108:
  __mptr = (struct list_head const *)cursor;
  evt = (struct efx_ptp_event_rx *)__mptr;
  if (evt->seq0 == match->words[0] && evt->seq1 == match->words[1]) {
    timestamps = skb_hwtstamps(skb);
    timestamps->hwtstamp = evt->hwtimestamp;
    match->state = 1;
    rc = 1;
    list_move(& evt->link, & ptp->evt_free_list);
    goto ldv_46107;
  } else {
  }
  cursor = next;
  next = cursor->next;
  ldv_46109: ;
  if ((unsigned long )(& ptp->evt_list) != (unsigned long )cursor) {
    goto ldv_46108;
  } else {
  }
  ldv_46107:
  spin_unlock_bh(& ptp->evt_lock);
  return (rc);
}
}
static bool efx_ptp_process_events(struct efx_nic *efx , struct sk_buff_head *q )
{
  struct efx_ptp_data *ptp ;
  bool rc ;
  struct sk_buff *skb ;
  struct efx_ptp_match *match ;
  enum ptp_packet_state tmp ;
  {
  ptp = efx->ptp_data;
  rc = 0;
  goto ldv_46126;
  ldv_46125:
  match = (struct efx_ptp_match *)(& skb->cb);
  if ((unsigned int )match->state == 3U) {
    __skb_queue_tail(q, skb);
  } else {
    tmp = efx_ptp_match_rx(efx, skb);
    if ((unsigned int )tmp == 1U) {
      rc = 1;
      __skb_queue_tail(q, skb);
    } else
    if ((long )match->expiry - (long )jiffies < 0L) {
      match->state = 2;
      if ((efx->msg_enable & 64U) != 0U) {
        netdev_warn((struct net_device const *)efx->net_dev, "PTP packet - no timestamp seen\n");
      } else {
      }
      __skb_queue_tail(q, skb);
    } else {
      skb_queue_head(& ptp->rxq, skb);
      goto ldv_46124;
    }
  }
  ldv_46126:
  skb = skb_dequeue(& ptp->rxq);
  if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
    goto ldv_46125;
  } else {
  }
  ldv_46124: ;
  return (rc);
}
}
__inline static void efx_ptp_process_rx(struct efx_nic *efx , struct sk_buff *skb )
{
  {
  local_bh_disable();
  netif_receive_skb(skb);
  local_bh_enable();
  return;
}
}
static int efx_ptp_start(struct efx_nic *efx )
{
  struct efx_ptp_data *ptp ;
  struct efx_filter_spec rxfilter ;
  int rc ;
  struct efx_rx_queue *tmp ;
  int tmp___0 ;
  struct efx_rx_queue *tmp___1 ;
  int tmp___2 ;
  {
  ptp = efx->ptp_data;
  ptp->reset_required = 0;
  tmp = efx_channel_get_rx_queue(ptp->channel);
  tmp___0 = efx_rx_queue_index(tmp);
  efx_filter_init_rx(& rxfilter, 2, 0, (unsigned int )tmp___0);
  rc = efx_filter_set_ipv4_local(& rxfilter, 17, 2164326624U, 16129);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_filter_insert_filter(efx, & rxfilter, 1);
  if (rc < 0) {
    return (rc);
  } else {
  }
  ptp->rxfilter_event = (u32 )rc;
  tmp___1 = efx_channel_get_rx_queue(ptp->channel);
  tmp___2 = efx_rx_queue_index(tmp___1);
  efx_filter_init_rx(& rxfilter, 2, 0, (unsigned int )tmp___2);
  rc = efx_filter_set_ipv4_local(& rxfilter, 17, 2164326624U, 16385);
  if (rc != 0) {
    goto fail;
  } else {
  }
  rc = efx_filter_insert_filter(efx, & rxfilter, 1);
  if (rc < 0) {
    goto fail;
  } else {
  }
  ptp->rxfilter_general = (u32 )rc;
  rc = efx_ptp_enable(efx);
  if (rc != 0) {
    goto fail2;
  } else {
  }
  ptp->evt_frag_idx = 0;
  ptp->current_adjfreq = 0LL;
  ptp->rxfilter_installed = 1;
  return (0);
  fail2:
  efx_filter_remove_id_safe(efx, 2, ptp->rxfilter_general);
  fail:
  efx_filter_remove_id_safe(efx, 2, ptp->rxfilter_event);
  return (rc);
}
}
static int efx_ptp_stop(struct efx_nic *efx )
{
  struct efx_ptp_data *ptp ;
  int rc ;
  int tmp ;
  struct list_head *cursor ;
  struct list_head *next ;
  {
  ptp = efx->ptp_data;
  tmp = efx_ptp_disable(efx);
  rc = tmp;
  if ((int )ptp->rxfilter_installed) {
    efx_filter_remove_id_safe(efx, 2, ptp->rxfilter_general);
    efx_filter_remove_id_safe(efx, 2, ptp->rxfilter_event);
    ptp->rxfilter_installed = 0;
  } else {
  }
  efx_ptp_deliver_rx_queue(& (efx->ptp_data)->rxq);
  skb_queue_purge(& (efx->ptp_data)->txq);
  spin_lock_bh(& (efx->ptp_data)->evt_lock);
  cursor = (efx->ptp_data)->evt_list.next;
  next = cursor->next;
  goto ldv_46147;
  ldv_46146:
  list_move(cursor, & (efx->ptp_data)->evt_free_list);
  cursor = next;
  next = cursor->next;
  ldv_46147: ;
  if ((unsigned long )(& (efx->ptp_data)->evt_list) != (unsigned long )cursor) {
    goto ldv_46146;
  } else {
  }
  spin_unlock_bh(& (efx->ptp_data)->evt_lock);
  return (rc);
}
}
static void efx_ptp_pps_worker(struct work_struct *work )
{
  struct efx_ptp_data *ptp ;
  struct work_struct const *__mptr ;
  struct efx_nic *efx ;
  struct ptp_clock_event ptp_evt ;
  int tmp ;
  {
  __mptr = (struct work_struct const *)work;
  ptp = (struct efx_ptp_data *)__mptr + 0xfffffffffffffc50UL;
  efx = (ptp->channel)->efx;
  tmp = efx_ptp_synchronize(efx, 4U);
  if (tmp != 0) {
    return;
  } else {
  }
  ptp_evt.type = 3;
  ptp_evt.ldv_41673.pps_times = ptp->host_time_pps;
  ptp_clock_event(ptp->phc_clock, & ptp_evt);
  return;
}
}
static void efx_ptp_worker(struct work_struct *work )
{
  struct efx_ptp_data *ptp_data ;
  struct work_struct const *__mptr ;
  struct efx_nic *efx ;
  struct sk_buff *skb ;
  struct sk_buff_head tempq ;
  bool tmp ;
  int tmp___0 ;
  {
  __mptr = (struct work_struct const *)work;
  ptp_data = (struct efx_ptp_data *)__mptr + 0xfffffffffffffd88UL;
  efx = (ptp_data->channel)->efx;
  if ((int )ptp_data->reset_required) {
    efx_ptp_stop(efx);
    efx_ptp_start(efx);
    return;
  } else {
  }
  efx_ptp_drop_time_expired_events(efx);
  __skb_queue_head_init(& tempq);
  tmp = efx_ptp_process_events(efx, & tempq);
  if ((int )tmp) {
    goto _L;
  } else {
    tmp___0 = skb_queue_empty((struct sk_buff_head const *)(& ptp_data->txq));
    if (tmp___0 == 0) {
      _L:
      goto ldv_46167;
      ldv_46166:
      efx_ptp_xmit_skb(efx, skb);
      ldv_46167:
      skb = skb_dequeue(& ptp_data->txq);
      if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
        goto ldv_46166;
      } else {
      }
    } else {
    }
  }
  goto ldv_46170;
  ldv_46169:
  efx_ptp_process_rx(efx, skb);
  ldv_46170:
  skb = __skb_dequeue(& tempq);
  if ((unsigned long )skb != (unsigned long )((struct sk_buff *)0)) {
    goto ldv_46169;
  } else {
  }
  return;
}
}
static int efx_ptp_probe_channel(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  struct efx_ptp_data *ptp ;
  int rc ;
  unsigned int pos ;
  void *tmp ;
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp___0 ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___1 ;
  struct lock_class_key __key___2 ;
  atomic_long_t __constr_expr_1 ;
  struct lock_class_key __key___3 ;
  char const *__lock_name___0 ;
  struct workqueue_struct *tmp___1 ;
  {
  efx = channel->efx;
  rc = 0;
  channel->irq_moderation = 0U;
  channel->rx_queue.core_index = 0;
  tmp = kzalloc(1576UL, 208U);
  ptp = (struct efx_ptp_data *)tmp;
  efx->ptp_data = ptp;
  if ((unsigned long )efx->ptp_data == (unsigned long )((struct efx_ptp_data *)0)) {
    return (-12);
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & ptp->start, 4U);
  if (rc != 0) {
    goto fail1;
  } else {
  }
  ptp->channel = channel;
  skb_queue_head_init(& ptp->rxq);
  skb_queue_head_init(& ptp->txq);
  __lock_name = "sfc_ptp";
  tmp___0 = __alloc_workqueue_key("sfc_ptp", 10U, 1, & __key, __lock_name);
  ptp->workwq = tmp___0;
  if ((unsigned long )ptp->workwq == (unsigned long )((struct workqueue_struct *)0)) {
    rc = -12;
    goto fail2;
  } else {
  }
  __init_work(& ptp->work, 0);
  __constr_expr_0.counter = 4195328L;
  ptp->work.data = __constr_expr_0;
  lockdep_init_map(& ptp->work.lockdep_map, "(&ptp->work)", & __key___0, 0);
  INIT_LIST_HEAD(& ptp->work.entry);
  ptp->work.func = & efx_ptp_worker;
  ptp->config.flags = 0;
  ptp->config.tx_type = 0;
  ptp->config.rx_filter = 0;
  INIT_LIST_HEAD(& ptp->evt_list);
  INIT_LIST_HEAD(& ptp->evt_free_list);
  spinlock_check(& ptp->evt_lock);
  __raw_spin_lock_init(& ptp->evt_lock.ldv_5961.rlock, "&(&ptp->evt_lock)->rlock",
                       & __key___1);
  pos = 0U;
  goto ldv_46188;
  ldv_46187:
  list_add(& ptp->rx_evts[pos].link, & ptp->evt_free_list);
  pos = pos + 1U;
  ldv_46188: ;
  if (pos <= 7U) {
    goto ldv_46187;
  } else {
  }
  ptp->phc_clock_info.owner = & __this_module;
  snprintf((char *)(& ptp->phc_clock_info.name), 16UL, "%pm", (unsigned char *)(& (efx->net_dev)->perm_addr));
  ptp->phc_clock_info.max_adj = 1000000;
  ptp->phc_clock_info.n_alarm = 0;
  ptp->phc_clock_info.n_ext_ts = 0;
  ptp->phc_clock_info.n_per_out = 0;
  ptp->phc_clock_info.pps = 1;
  ptp->phc_clock_info.adjfreq = & efx_phc_adjfreq;
  ptp->phc_clock_info.adjtime = & efx_phc_adjtime;
  ptp->phc_clock_info.gettime = & efx_phc_gettime;
  ptp->phc_clock_info.settime = & efx_phc_settime;
  ptp->phc_clock_info.enable = & efx_phc_enable;
  ptp->phc_clock = ptp_clock_register(& ptp->phc_clock_info, & (efx->pci_dev)->dev);
  if ((unsigned long )ptp->phc_clock == (unsigned long )((struct ptp_clock *)0)) {
    goto fail3;
  } else {
  }
  __init_work(& ptp->pps_work, 0);
  __constr_expr_1.counter = 4195328L;
  ptp->pps_work.data = __constr_expr_1;
  lockdep_init_map(& ptp->pps_work.lockdep_map, "(&ptp->pps_work)", & __key___2, 0);
  INIT_LIST_HEAD(& ptp->pps_work.entry);
  ptp->pps_work.func = & efx_ptp_pps_worker;
  __lock_name___0 = "sfc_pps";
  tmp___1 = __alloc_workqueue_key("sfc_pps", 10U, 1, & __key___3, __lock_name___0);
  ptp->pps_workwq = tmp___1;
  if ((unsigned long )ptp->pps_workwq == (unsigned long )((struct workqueue_struct *)0)) {
    rc = -12;
    goto fail4;
  } else {
  }
  ptp->nic_ts_enabled = 0;
  return (0);
  fail4:
  ptp_clock_unregister((efx->ptp_data)->phc_clock);
  fail3:
  destroy_workqueue((efx->ptp_data)->workwq);
  fail2:
  efx_nic_free_buffer(efx, & ptp->start);
  fail1:
  kfree((void const *)efx->ptp_data);
  efx->ptp_data = 0;
  return (rc);
}
}
static void efx_ptp_remove_channel(struct efx_channel *channel )
{
  struct efx_nic *efx ;
  {
  efx = channel->efx;
  if ((unsigned long )efx->ptp_data == (unsigned long )((struct efx_ptp_data *)0)) {
    return;
  } else {
  }
  efx_ptp_disable(channel->efx);
  cancel_work_sync(& (efx->ptp_data)->work);
  cancel_work_sync(& (efx->ptp_data)->pps_work);
  skb_queue_purge(& (efx->ptp_data)->rxq);
  skb_queue_purge(& (efx->ptp_data)->txq);
  ptp_clock_unregister((efx->ptp_data)->phc_clock);
  destroy_workqueue((efx->ptp_data)->workwq);
  destroy_workqueue((efx->ptp_data)->pps_workwq);
  efx_nic_free_buffer(efx, & (efx->ptp_data)->start);
  kfree((void const *)efx->ptp_data);
  return;
}
}
static void efx_ptp_get_channel_name(struct efx_channel *channel , char *buf , size_t len )
{
  {
  snprintf(buf, len, "%s-ptp", (char *)(& (channel->efx)->name));
  return;
}
}
bool efx_ptp_is_ptp_tx(struct efx_nic *efx , struct sk_buff *skb )
{
  long tmp ;
  struct iphdr *tmp___0 ;
  struct udphdr *tmp___1 ;
  int tmp___2 ;
  {
  if ((((unsigned long )efx->ptp_data != (unsigned long )((struct efx_ptp_data *)0) && (int )(efx->ptp_data)->enabled) && skb->len > 62U) && skb->len <= 240U) {
    tmp = ldv__builtin_expect((unsigned int )skb->protocol == 8U, 1L);
    if (tmp != 0L) {
      tmp___0 = ip_hdr((struct sk_buff const *)skb);
      if ((unsigned int )tmp___0->protocol == 17U) {
        tmp___1 = udp_hdr((struct sk_buff const *)skb);
        if ((unsigned int )tmp___1->dest == 16129U) {
          tmp___2 = 1;
        } else {
          tmp___2 = 0;
        }
      } else {
        tmp___2 = 0;
      }
    } else {
      tmp___2 = 0;
    }
  } else {
    tmp___2 = 0;
  }
  return ((bool )tmp___2);
}
}
static void efx_ptp_rx(struct efx_channel *channel , struct sk_buff *skb )
{
  struct efx_nic *efx ;
  struct efx_ptp_data *ptp ;
  struct efx_ptp_match *match ;
  u8 *data ;
  unsigned int version ;
  unsigned long tmp ;
  __u16 tmp___0 ;
  long tmp___1 ;
  struct skb_shared_hwtstamps *timestamps ;
  __u16 tmp___2 ;
  {
  efx = channel->efx;
  ptp = efx->ptp_data;
  match = (struct efx_ptp_match *)(& skb->cb);
  tmp = msecs_to_jiffies(10U);
  match->expiry = tmp + (unsigned long )jiffies;
  if (ptp->mode == 0U) {
    if (skb->len <= 63U) {
      netif_receive_skb(skb);
      return;
    } else {
    }
    tmp___0 = __fswab16((int )*((__be16 *)skb->data + 28U));
    version = (unsigned int )tmp___0;
    if (version != 1U) {
      netif_receive_skb(skb);
      return;
    } else {
    }
  } else {
    if (skb->len <= 62U) {
      netif_receive_skb(skb);
      return;
    } else {
    }
    version = (unsigned int )*(skb->data + 29UL);
    tmp___1 = ldv__builtin_expect(ptp->mode != 2U, 0L);
    if (tmp___1 != 0L) {
      __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/ptp.c.prepared"),
                           "i" (1125), "i" (12UL));
      ldv_46219: ;
      goto ldv_46219;
    } else {
    }
    if ((version & 15U) != 2U) {
      netif_receive_skb(skb);
      return;
    } else {
    }
  }
  tmp___2 = __fswab16((int )*((__be16 *)skb->data + 22U));
  if ((unsigned int )tmp___2 == 319U) {
    match->state = 0;
    timestamps = skb_hwtstamps(skb);
    memset((void *)timestamps, 0, 16UL);
    data = skb->data + 50U;
    match->words[0] = (u32 )((((int )*data | ((int )*(data + 1UL) << 8)) | ((int )*(data + 2UL) << 16)) | ((int )*(data + 3UL) << 24));
    match->words[1] = (u32 )(((int )*(data + 4UL) | ((int )*(data + 5UL) << 8)) | ((int )*(skb->data + 59UL) << 16));
  } else {
    match->state = 3;
  }
  skb_queue_tail(& ptp->rxq, skb);
  queue_work(ptp->workwq, & ptp->work);
  return;
}
}
int efx_ptp_tx(struct efx_nic *efx , struct sk_buff *skb )
{
  struct efx_ptp_data *ptp ;
  struct udphdr *tmp ;
  {
  ptp = efx->ptp_data;
  skb_queue_tail(& ptp->txq, skb);
  tmp = udp_hdr((struct sk_buff const *)skb);
  if ((unsigned int )tmp->dest == 16129U && skb->len <= 240U) {
    efx_xmit_hwtstamp_pending(skb);
  } else {
  }
  queue_work(ptp->workwq, & ptp->work);
  return (0);
}
}
static int efx_ptp_change_mode(struct efx_nic *efx , bool enable_wanted , unsigned int new_mode )
{
  int rc ;
  {
  if ((int )(efx->ptp_data)->enabled != (int )enable_wanted || ((int )enable_wanted && (efx->ptp_data)->mode != new_mode)) {
    if ((int )enable_wanted) {
      if ((int )(efx->ptp_data)->enabled && (efx->ptp_data)->mode != new_mode) {
        (efx->ptp_data)->enabled = 0;
        rc = efx_ptp_stop(efx);
        if (rc != 0) {
          return (rc);
        } else {
        }
      } else {
      }
      (efx->ptp_data)->mode = new_mode;
      rc = efx_ptp_start(efx);
      if (rc == 0) {
        rc = efx_ptp_synchronize(efx, 8U);
        if (rc != 0) {
          efx_ptp_stop(efx);
        } else {
        }
      } else {
      }
    } else {
      rc = efx_ptp_stop(efx);
    }
    if (rc != 0) {
      return (rc);
    } else {
    }
    (efx->ptp_data)->enabled = enable_wanted;
  } else {
  }
  return (0);
}
}
static int efx_ptp_ts_init(struct efx_nic *efx , struct hwtstamp_config *init )
{
  bool enable_wanted ;
  unsigned int new_mode ;
  int rc ;
  {
  enable_wanted = 0;
  if (init->flags != 0) {
    return (-22);
  } else {
  }
  if (init->tx_type != 0 && init->tx_type != 1) {
    return (-34);
  } else {
  }
  new_mode = (efx->ptp_data)->mode;
  switch (init->rx_filter) {
  case 0: ;
  goto ldv_46240;
  case 3: ;
  case 4: ;
  case 5:
  init->rx_filter = 3;
  new_mode = 0U;
  enable_wanted = 1;
  goto ldv_46240;
  case 6: ;
  case 7: ;
  case 8:
  init->rx_filter = 6;
  new_mode = 2U;
  enable_wanted = 1;
  goto ldv_46240;
  case 12: ;
  case 13: ;
  case 14: ;
  case 9: ;
  case 10: ;
  case 11: ;
  return (-34);
  default: ;
  return (-34);
  }
  ldv_46240: ;
  if (init->tx_type != 0) {
    enable_wanted = 1;
  } else {
  }
  rc = efx_ptp_change_mode(efx, (int )enable_wanted, new_mode);
  if (rc != 0) {
    return (rc);
  } else {
  }
  (efx->ptp_data)->config = *init;
  return (0);
}
}
int efx_ptp_get_ts_info(struct net_device *net_dev , struct ethtool_ts_info *ts_info )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_ptp_data *ptp ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  ptp = efx->ptp_data;
  if ((unsigned long )ptp == (unsigned long )((struct efx_ptp_data *)0)) {
    return (-95);
  } else {
  }
  ts_info->so_timestamping = 69U;
  ts_info->phc_index = ptp_clock_index(ptp->phc_clock);
  ts_info->tx_types = 3U;
  ts_info->rx_filters = 505U;
  return (0);
}
}
int efx_ptp_ioctl(struct efx_nic *efx , struct ifreq *ifr , int cmd )
{
  struct hwtstamp_config config ;
  int rc ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  if ((unsigned long )efx->ptp_data == (unsigned long )((struct efx_ptp_data *)0)) {
    return (-95);
  } else {
  }
  tmp = copy_from_user((void *)(& config), (void const *)ifr->ifr_ifru.ifru_data,
                       12UL);
  if (tmp != 0UL) {
    return (-14);
  } else {
  }
  rc = efx_ptp_ts_init(efx, & config);
  if (rc != 0) {
    return (rc);
  } else {
  }
  tmp___0 = copy_to_user(ifr->ifr_ifru.ifru_data, (void const *)(& config), 12U);
  return (tmp___0 != 0 ? -14 : 0);
}
}
static void ptp_event_failure(struct efx_nic *efx , int expected_frag_len )
{
  struct efx_ptp_data *ptp ;
  {
  ptp = efx->ptp_data;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "PTP unexpected event length: got %d expected %d\n",
               ptp->evt_frag_idx, expected_frag_len);
  } else {
  }
  ptp->reset_required = 1;
  queue_work(ptp->workwq, & ptp->work);
  return;
}
}
static void ptp_event_rx(struct efx_nic *efx , struct efx_ptp_data *ptp )
{
  struct efx_ptp_event_rx *evt ;
  struct list_head const *__mptr ;
  unsigned long tmp ;
  int tmp___0 ;
  {
  evt = 0;
  if (ptp->evt_frag_idx != 3) {
    ptp_event_failure(efx, 3);
    return;
  } else {
  }
  spin_lock_bh(& ptp->evt_lock);
  tmp___0 = list_empty((struct list_head const *)(& ptp->evt_free_list));
  if (tmp___0 == 0) {
    __mptr = (struct list_head const *)ptp->evt_free_list.next;
    evt = (struct efx_ptp_event_rx *)__mptr;
    list_del(& evt->link);
    evt->seq0 = (u32 )ptp->evt_frags[2].u64[0];
    evt->seq1 = (((u32 )(ptp->evt_frags[2].u64[0] >> 36) & 255U) | (((u32 )(ptp->evt_frags[1].u64[0] >> 36) << 8U) & 65535U)) | (((u32 )(ptp->evt_frags[0].u64[0] >> 36) & 255U) << 16U);
    evt->hwtimestamp = ktime_set((long const )ptp->evt_frags[0].u64[0] & 4294967295L,
                                 (unsigned long const )ptp->evt_frags[1].u64[0] & 4294967295UL);
    tmp = msecs_to_jiffies(10U);
    evt->expiry = tmp + (unsigned long )jiffies;
    list_add_tail(& evt->link, & ptp->evt_list);
    queue_work(ptp->workwq, & ptp->work);
  } else
  if ((efx->msg_enable & 64U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "No free PTP event");
  } else {
  }
  spin_unlock_bh(& ptp->evt_lock);
  return;
}
}
static void ptp_event_fault(struct efx_nic *efx , struct efx_ptp_data *ptp )
{
  int code ;
  {
  code = (int )ptp->evt_frags[0].u64[0];
  if (ptp->evt_frag_idx != 1) {
    ptp_event_failure(efx, 1);
    return;
  } else {
  }
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_err((struct net_device const *)efx->net_dev, "PTP error %d\n", code);
  } else {
  }
  return;
}
}
static void ptp_event_pps(struct efx_nic *efx , struct efx_ptp_data *ptp )
{
  {
  if ((int )ptp->nic_ts_enabled) {
    queue_work(ptp->pps_workwq, & ptp->pps_work);
  } else {
  }
  return;
}
}
void efx_ptp_event(struct efx_nic *efx , efx_qword_t *ev )
{
  struct efx_ptp_data *ptp ;
  int code ;
  int tmp ;
  {
  ptp = efx->ptp_data;
  code = (int )(ev->u64[0] >> 44) & 255;
  if (! ptp->enabled) {
    return;
  } else {
  }
  if (ptp->evt_frag_idx == 0) {
    ptp->evt_code = code;
  } else
  if (ptp->evt_code != code) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "PTP out of sequence event %d\n",
                 code);
    } else {
    }
    ptp->evt_frag_idx = 0;
  } else {
  }
  tmp = ptp->evt_frag_idx;
  ptp->evt_frag_idx = ptp->evt_frag_idx + 1;
  ptp->evt_frags[tmp] = *ev;
  if (((ev->u64[0] >> 32) & 1ULL) == 0ULL) {
    switch (code) {
    case 13:
    ptp_event_rx(efx, ptp);
    goto ldv_46295;
    case 14:
    ptp_event_fault(efx, ptp);
    goto ldv_46295;
    case 15:
    ptp_event_pps(efx, ptp);
    goto ldv_46295;
    default: ;
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "PTP unknown event %d\n",
                 code);
    } else {
    }
    goto ldv_46295;
    }
    ldv_46295:
    ptp->evt_frag_idx = 0;
  } else
  if (ptp->evt_frag_idx == 3) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "PTP too many event fragments\n");
    } else {
    }
    ptp->evt_frag_idx = 0;
  } else {
  }
  return;
}
}
static int efx_phc_adjfreq(struct ptp_clock_info *ptp , s32 delta )
{
  struct efx_ptp_data *ptp_data ;
  struct ptp_clock_info const *__mptr ;
  struct efx_nic *efx ;
  u8 inadj[24U] ;
  s64 adjustment_ns ;
  int rc ;
  {
  __mptr = (struct ptp_clock_info const *)ptp;
  ptp_data = (struct efx_ptp_data *)__mptr + 0xfffffffffffffca8UL;
  efx = (ptp_data->channel)->efx;
  if (delta > 1000000) {
    delta = 1000000;
  } else
  if (delta < -1000000) {
    delta = -1000000;
  } else {
  }
  adjustment_ns = (long long )delta * 4611686018LL >> 22;
  ((efx_dword_t *)(& inadj))->u32[0] = 6U;
  ((efx_dword_t *)(& inadj) + 8U)->u32[0] = (unsigned int )adjustment_ns;
  ((efx_dword_t *)(& inadj) + 12U)->u32[0] = (unsigned int )(adjustment_ns >> 32);
  ((efx_dword_t *)(& inadj) + 16U)->u32[0] = 0U;
  ((efx_dword_t *)(& inadj) + 20U)->u32[0] = 0U;
  rc = efx_mcdi_rpc(efx, 11U, (u8 const *)(& inadj), 24UL, 0, 0UL, 0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  ptp_data->current_adjfreq = (s64 )delta;
  return (0);
}
}
static int efx_phc_adjtime(struct ptp_clock_info *ptp , s64 delta )
{
  struct efx_ptp_data *ptp_data ;
  struct ptp_clock_info const *__mptr ;
  struct efx_nic *efx ;
  struct timespec delta_ts ;
  struct timespec tmp ;
  u8 inbuf[24U] ;
  int tmp___0 ;
  {
  __mptr = (struct ptp_clock_info const *)ptp;
  ptp_data = (struct efx_ptp_data *)__mptr + 0xfffffffffffffca8UL;
  efx = (ptp_data->channel)->efx;
  tmp = ns_to_timespec(delta);
  delta_ts = tmp;
  ((efx_dword_t *)(& inbuf))->u32[0] = 6U;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = 0U;
  ((efx_dword_t *)(& inbuf) + 12U)->u32[0] = 0U;
  ((efx_dword_t *)(& inbuf) + 16U)->u32[0] = (unsigned int )delta_ts.tv_sec;
  ((efx_dword_t *)(& inbuf) + 20U)->u32[0] = (unsigned int )delta_ts.tv_nsec;
  tmp___0 = efx_mcdi_rpc(efx, 11U, (u8 const *)(& inbuf), 24UL, 0, 0UL, 0);
  return (tmp___0);
}
}
static int efx_phc_gettime(struct ptp_clock_info *ptp , struct timespec *ts )
{
  struct efx_ptp_data *ptp_data ;
  struct ptp_clock_info const *__mptr ;
  struct efx_nic *efx ;
  u8 inbuf[8U] ;
  u8 outbuf[8U] ;
  int rc ;
  {
  __mptr = (struct ptp_clock_info const *)ptp;
  ptp_data = (struct efx_ptp_data *)__mptr + 0xfffffffffffffca8UL;
  efx = (ptp_data->channel)->efx;
  ((efx_dword_t *)(& inbuf))->u32[0] = 4U;
  rc = efx_mcdi_rpc(efx, 11U, (u8 const *)(& inbuf), 8UL, (u8 *)(& outbuf), 8UL,
                    0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  ts->tv_sec = (__kernel_time_t )((efx_dword_t *)(& outbuf))->u32[0];
  ts->tv_nsec = (long )((efx_dword_t *)(& outbuf) + 4U)->u32[0];
  return (0);
}
}
static int efx_phc_settime(struct ptp_clock_info *ptp , struct timespec const *e_ts )
{
  int rc ;
  struct timespec time_now ;
  struct timespec delta ;
  s64 tmp ;
  {
  rc = efx_phc_gettime(ptp, & time_now);
  if (rc != 0) {
    return (rc);
  } else {
  }
  delta = timespec_sub(*e_ts, time_now);
  tmp = timespec_to_ns((struct timespec const *)(& delta));
  efx_phc_adjtime(ptp, tmp);
  if (rc != 0) {
    return (rc);
  } else {
  }
  return (0);
}
}
static int efx_phc_enable(struct ptp_clock_info *ptp , struct ptp_clock_request *request ,
                          int enable )
{
  struct efx_ptp_data *ptp_data ;
  struct ptp_clock_info const *__mptr ;
  {
  __mptr = (struct ptp_clock_info const *)ptp;
  ptp_data = (struct efx_ptp_data *)__mptr + 0xfffffffffffffca8UL;
  if ((unsigned int )request->type != 2U) {
    return (-95);
  } else {
  }
  ptp_data->nic_ts_enabled = enable != 0;
  return (0);
}
}
static struct efx_channel_type const efx_ptp_channel_type = {& efx_ptp_handle_no_channel, & efx_ptp_probe_channel, & efx_ptp_remove_channel,
    & efx_ptp_get_channel_name, 0, & efx_ptp_rx, 0};
void efx_ptp_probe(struct efx_nic *efx )
{
  int tmp ;
  {
  tmp = efx_ptp_disable(efx);
  if (tmp == 0) {
    efx->extra_channel_type[1] = & efx_ptp_channel_type;
  } else {
  }
  return;
}
}
void ldv_main19_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  struct efx_channel *var_group2 ;
  char *var_efx_ptp_get_channel_name_19_p1 ;
  size_t var_efx_ptp_get_channel_name_19_p2 ;
  struct sk_buff *var_group3 ;
  int tmp ;
  int tmp___0 ;
  {
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_46379;
  ldv_46378:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0:
  ldv_handler_precall();
  efx_ptp_handle_no_channel(var_group1);
  goto ldv_46372;
  case 1:
  ldv_handler_precall();
  efx_ptp_probe_channel(var_group2);
  goto ldv_46372;
  case 2:
  ldv_handler_precall();
  efx_ptp_remove_channel(var_group2);
  goto ldv_46372;
  case 3:
  ldv_handler_precall();
  efx_ptp_get_channel_name(var_group2, var_efx_ptp_get_channel_name_19_p1, var_efx_ptp_get_channel_name_19_p2);
  goto ldv_46372;
  case 4:
  ldv_handler_precall();
  efx_ptp_rx(var_group2, var_group3);
  goto ldv_46372;
  default: ;
  goto ldv_46372;
  }
  ldv_46372: ;
  ldv_46379:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    goto ldv_46378;
  } else {
  }
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_397(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_398(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_399(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_400(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_401(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_402(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_403(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_414(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_412(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_415(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_417(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_419(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_421(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_423(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_425(struct mutex *ldv_func_arg1 ) ;
extern int mutex_lock_interruptible(struct mutex * ) ;
int ldv_mutex_lock_interruptible_418(struct mutex *ldv_func_arg1 ) ;
int ldv_mutex_lock_interruptible_420(struct mutex *ldv_func_arg1 ) ;
int ldv_mutex_lock_interruptible_422(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_411(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_413(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_416(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_424(struct mutex *ldv_func_arg1 ) ;
int ldv_mutex_lock_interruptible_spi_lock(struct mutex *lock ) ;
extern int mtd_device_parse_register(struct mtd_info * , char const ** , struct mtd_part_parser_data * ,
                                     struct mtd_partition const * , int ) ;
extern int mtd_device_unregister(struct mtd_info * ) ;
extern void mtd_erase_callback(struct erase_info * ) ;
__inline static void ssleep(unsigned int seconds )
{
  {
  msleep(seconds * 1000U);
  return;
}
}
__inline static int efx_dev_registered(struct efx_nic *efx )
{
  {
  return ((unsigned int )(efx->net_dev)->reg_state == 1U);
}
}
static int falcon_mtd_probe(struct efx_nic *efx ) ;
static int siena_mtd_probe(struct efx_nic *efx ) ;
static int efx_spi_slow_wait(struct efx_mtd_partition *part , bool uninterruptible )
{
  struct efx_mtd *efx_mtd ;
  struct efx_spi_device const *spi ;
  struct efx_nic *efx ;
  u8 status ;
  int rc ;
  int i ;
  struct task_struct *tmp ;
  struct task_struct *tmp___0 ;
  int tmp___1 ;
  {
  efx_mtd = (struct efx_mtd *)part->mtd.priv;
  spi = efx_mtd->spi;
  efx = efx_mtd->efx;
  i = 0;
  goto ldv_42862;
  ldv_42861:
  tmp = get_current();
  tmp->state = (int )uninterruptible ? 2L : 1L;
  schedule_timeout(25L);
  rc = falcon_spi_cmd(efx, spi, 5U, -1, 0, (void *)(& status), 1UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (((int )status & 1) == 0) {
    return (0);
  } else {
  }
  tmp___0 = get_current();
  tmp___1 = signal_pending(tmp___0);
  if (tmp___1 != 0) {
    return (-4);
  } else {
  }
  i = i + 1;
  ldv_42862: ;
  if (i <= 39) {
    goto ldv_42861;
  } else {
  }
  printk("\v%s: timed out waiting for %s\n", (char *)(& part->name), efx_mtd->name);
  return (-110);
}
}
static int efx_spi_unlock(struct efx_nic *efx , struct efx_spi_device const *spi )
{
  u8 unlock_mask ;
  u8 status ;
  int rc ;
  {
  unlock_mask = 28U;
  rc = falcon_spi_cmd(efx, spi, 5U, -1, 0, (void *)(& status), 1UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((unsigned int )((int )status & (int )unlock_mask) == 0U) {
    return (0);
  } else {
  }
  rc = falcon_spi_cmd(efx, spi, 6U, -1, 0, 0, 0UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_cmd(efx, spi, 80U, -1, 0, 0, 0UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  status = (u8 )(~ ((int )((signed char )unlock_mask)) & (int )((signed char )status));
  rc = falcon_spi_cmd(efx, spi, 1U, -1, (void const *)(& status), 0, 1UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_wait_write(efx, spi);
  if (rc != 0) {
    return (rc);
  } else {
  }
  return (0);
}
}
static int efx_spi_erase(struct efx_mtd_partition *part , loff_t start , size_t len )
{
  struct efx_mtd *efx_mtd ;
  struct efx_spi_device const *spi ;
  struct efx_nic *efx ;
  unsigned int pos ;
  unsigned int block_len ;
  u8 empty[16U] ;
  u8 buffer[16U] ;
  int rc ;
  size_t _min1 ;
  unsigned long _min2 ;
  int tmp ;
  struct task_struct *tmp___0 ;
  int tmp___1 ;
  {
  efx_mtd = (struct efx_mtd *)part->mtd.priv;
  spi = efx_mtd->spi;
  efx = efx_mtd->efx;
  if ((size_t )spi->erase_size != len) {
    return (-22);
  } else {
  }
  if ((unsigned int )((unsigned char )spi->erase_command) == 0U) {
    return (-95);
  } else {
  }
  rc = efx_spi_unlock(efx, spi);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_cmd(efx, spi, 6U, -1, 0, 0, 0UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_cmd(efx, spi, (unsigned int )spi->erase_command, (int )start, 0,
                      0, 0UL);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_spi_slow_wait(part, 0);
  memset((void *)(& empty), 255, 16UL);
  pos = 0U;
  goto ldv_42889;
  ldv_42888:
  _min1 = len - (size_t )pos;
  _min2 = 16UL;
  block_len = (unsigned int )(_min1 < _min2 ? _min1 : _min2);
  rc = falcon_spi_read(efx, spi, (loff_t )pos + start, (size_t )block_len, 0, (u8 *)(& buffer));
  if (rc != 0) {
    return (rc);
  } else {
  }
  tmp = memcmp((void const *)(& empty), (void const *)(& buffer), (size_t )block_len);
  if (tmp != 0) {
    return (-5);
  } else {
  }
  __might_sleep("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
                264, 0);
  _cond_resched();
  tmp___0 = get_current();
  tmp___1 = signal_pending(tmp___0);
  if (tmp___1 != 0) {
    return (-4);
  } else {
  }
  pos = pos + block_len;
  ldv_42889: ;
  if ((size_t )pos < len) {
    goto ldv_42888;
  } else {
  }
  return (rc);
}
}
static int efx_mtd_erase(struct mtd_info *mtd , struct erase_info *erase )
{
  struct efx_mtd *efx_mtd ;
  int rc ;
  {
  efx_mtd = (struct efx_mtd *)mtd->priv;
  rc = (*((efx_mtd->ops)->erase))(mtd, (loff_t )erase->addr, (size_t )erase->len);
  if (rc == 0) {
    erase->state = 8U;
  } else {
    erase->state = 16U;
    erase->fail_addr = 0xffffffffffffffffULL;
  }
  mtd_erase_callback(erase);
  return (rc);
}
}
static void efx_mtd_sync(struct mtd_info *mtd )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  rc = (*((efx_mtd->ops)->sync))(mtd);
  if (rc != 0) {
    printk("\v%s: %s sync failed (%d)\n", (char *)(& part->name), efx_mtd->name, rc);
  } else {
  }
  return;
}
}
static void efx_mtd_remove_partition(struct efx_mtd_partition *part )
{
  int rc ;
  int __ret_warn_on ;
  long tmp ;
  {
  ldv_42910:
  rc = mtd_device_unregister(& part->mtd);
  if (rc != -16) {
    goto ldv_42909;
  } else {
  }
  ssleep(1U);
  goto ldv_42910;
  ldv_42909:
  __ret_warn_on = rc != 0;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
                       312);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  return;
}
}
static void efx_mtd_remove_device(struct efx_mtd *efx_mtd )
{
  struct efx_mtd_partition *part ;
  {
  part = (struct efx_mtd_partition *)(& efx_mtd->part);
  goto ldv_42918;
  ldv_42917:
  efx_mtd_remove_partition(part);
  part = part + 1;
  ldv_42918: ;
  if ((unsigned long )((struct efx_mtd_partition *)(& efx_mtd->part) + efx_mtd->n_parts) != (unsigned long )part) {
    goto ldv_42917;
  } else {
  }
  list_del(& efx_mtd->node);
  kfree((void const *)efx_mtd);
  return;
}
}
static void efx_mtd_rename_device(struct efx_mtd *efx_mtd )
{
  struct efx_mtd_partition *part ;
  int tmp ;
  {
  part = (struct efx_mtd_partition *)(& efx_mtd->part);
  goto ldv_42925;
  ldv_42924:
  tmp = efx_nic_rev(efx_mtd->efx);
  if (tmp > 2) {
    snprintf((char *)(& part->name), 36UL, "%s %s:%02x", (char *)(& (efx_mtd->efx)->name),
             part->type_name, (int )part->ldv_42817.mcdi.fw_subtype);
  } else {
    snprintf((char *)(& part->name), 36UL, "%s %s", (char *)(& (efx_mtd->efx)->name),
             part->type_name);
  }
  part = part + 1;
  ldv_42925: ;
  if ((unsigned long )((struct efx_mtd_partition *)(& efx_mtd->part) + efx_mtd->n_parts) != (unsigned long )part) {
    goto ldv_42924;
  } else {
  }
  return;
}
}
static int efx_mtd_probe_device(struct efx_nic *efx , struct efx_mtd *efx_mtd )
{
  struct efx_mtd_partition *part ;
  int tmp ;
  {
  efx_mtd->efx = efx;
  efx_mtd_rename_device(efx_mtd);
  part = (struct efx_mtd_partition *)(& efx_mtd->part);
  goto ldv_42934;
  ldv_42933:
  part->mtd.writesize = 1U;
  part->mtd.owner = & __this_module;
  part->mtd.priv = (void *)efx_mtd;
  part->mtd.name = (char const *)(& part->name);
  part->mtd._erase = & efx_mtd_erase;
  part->mtd._read = (int (*)(struct mtd_info * , loff_t , size_t , size_t * , u_char * ))(efx_mtd->ops)->read;
  part->mtd._write = (int (*)(struct mtd_info * , loff_t , size_t , size_t * , u_char const * ))(efx_mtd->ops)->write;
  part->mtd._sync = & efx_mtd_sync;
  tmp = mtd_device_parse_register(& part->mtd, 0, 0, 0, 0);
  if (tmp != 0) {
    goto fail;
  } else {
  }
  part = part + 1;
  ldv_42934: ;
  if ((unsigned long )((struct efx_mtd_partition *)(& efx_mtd->part) + efx_mtd->n_parts) != (unsigned long )part) {
    goto ldv_42933;
  } else {
  }
  list_add(& efx_mtd->node, & efx->mtd_list);
  return (0);
  fail: ;
  goto ldv_42937;
  ldv_42936:
  part = part - 1;
  efx_mtd_remove_partition(part);
  ldv_42937: ;
  if ((unsigned long )((struct efx_mtd_partition *)(& efx_mtd->part)) != (unsigned long )part) {
    goto ldv_42936;
  } else {
  }
  return (-12);
}
}
void efx_mtd_remove(struct efx_nic *efx )
{
  struct efx_mtd *efx_mtd ;
  struct efx_mtd *next ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  struct list_head const *__mptr___1 ;
  {
  tmp = efx_dev_registered(efx);
  __ret_warn_on = tmp != 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
                       379);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __mptr = (struct list_head const *)efx->mtd_list.next;
  efx_mtd = (struct efx_mtd *)__mptr;
  __mptr___0 = (struct list_head const *)efx_mtd->node.next;
  next = (struct efx_mtd *)__mptr___0;
  goto ldv_42953;
  ldv_42952:
  efx_mtd_remove_device(efx_mtd);
  efx_mtd = next;
  __mptr___1 = (struct list_head const *)next->node.next;
  next = (struct efx_mtd *)__mptr___1;
  ldv_42953: ;
  if ((unsigned long )(& efx_mtd->node) != (unsigned long )(& efx->mtd_list)) {
    goto ldv_42952;
  } else {
  }
  return;
}
}
void efx_mtd_rename(struct efx_nic *efx )
{
  struct efx_mtd *efx_mtd ;
  int tmp ;
  long tmp___0 ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  {
  tmp = rtnl_is_locked();
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
           389);
    dump_stack();
  } else {
  }
  __mptr = (struct list_head const *)efx->mtd_list.next;
  efx_mtd = (struct efx_mtd *)__mptr;
  goto ldv_42964;
  ldv_42963:
  efx_mtd_rename_device(efx_mtd);
  __mptr___0 = (struct list_head const *)efx_mtd->node.next;
  efx_mtd = (struct efx_mtd *)__mptr___0;
  ldv_42964: ;
  if ((unsigned long )(& efx_mtd->node) != (unsigned long )(& efx->mtd_list)) {
    goto ldv_42963;
  } else {
  }
  return;
}
}
int efx_mtd_probe(struct efx_nic *efx )
{
  int tmp ;
  int tmp___0 ;
  int tmp___1 ;
  {
  tmp___1 = efx_nic_rev(efx);
  if (tmp___1 > 2) {
    tmp = siena_mtd_probe(efx);
    return (tmp);
  } else {
    tmp___0 = falcon_mtd_probe(efx);
    return (tmp___0);
  }
}
}
static int falcon_mtd_read(struct mtd_info *mtd , loff_t start , size_t len , size_t *retlen ,
                           u8 *buffer )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_spi_device const *spi ;
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  spi = efx_mtd->spi;
  efx = efx_mtd->efx;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  rc = ldv_mutex_lock_interruptible_418(& nic_data->spi_lock);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_read(efx, spi, (loff_t )((unsigned long long )part->ldv_42817.offset + (unsigned long long )start),
                       len, retlen, buffer);
  ldv_mutex_unlock_419(& nic_data->spi_lock);
  return (rc);
}
}
static int falcon_mtd_erase(struct mtd_info *mtd , loff_t start , size_t len )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  rc = ldv_mutex_lock_interruptible_420(& nic_data->spi_lock);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = efx_spi_erase(part, (loff_t )((unsigned long long )part->ldv_42817.offset + (unsigned long long )start),
                     len);
  ldv_mutex_unlock_421(& nic_data->spi_lock);
  return (rc);
}
}
static int falcon_mtd_write(struct mtd_info *mtd , loff_t start , size_t len , size_t *retlen ,
                            u8 const *buffer )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_spi_device const *spi ;
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  spi = efx_mtd->spi;
  efx = efx_mtd->efx;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  rc = ldv_mutex_lock_interruptible_422(& nic_data->spi_lock);
  if (rc != 0) {
    return (rc);
  } else {
  }
  rc = falcon_spi_write(efx, spi, (loff_t )((unsigned long long )part->ldv_42817.offset + (unsigned long long )start),
                        len, retlen, buffer);
  ldv_mutex_unlock_423(& nic_data->spi_lock);
  return (rc);
}
}
static int falcon_mtd_sync(struct mtd_info *mtd )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  struct falcon_nic_data *nic_data ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  ldv_mutex_lock_424(& nic_data->spi_lock);
  rc = efx_spi_slow_wait(part, 1);
  ldv_mutex_unlock_425(& nic_data->spi_lock);
  return (rc);
}
}
static struct efx_mtd_ops const falcon_mtd_ops = {& falcon_mtd_read, & falcon_mtd_erase, & falcon_mtd_write, & falcon_mtd_sync};
static int falcon_mtd_probe(struct efx_nic *efx )
{
  struct falcon_nic_data *nic_data ;
  struct efx_spi_device *spi ;
  struct efx_mtd *efx_mtd ;
  int rc ;
  int tmp ;
  long tmp___0 ;
  void *tmp___1 ;
  bool tmp___2 ;
  void *tmp___3 ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  bool tmp___4 ;
  {
  nic_data = (struct falcon_nic_data *)efx->nic_data;
  rc = -19;
  tmp = rtnl_is_locked();
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
           487);
    dump_stack();
  } else {
  }
  spi = & nic_data->spi_flash;
  tmp___2 = efx_spi_present((struct efx_spi_device const *)spi);
  if ((int )tmp___2 && spi->size > 32768U) {
    tmp___1 = kzalloc(1632UL, 208U);
    efx_mtd = (struct efx_mtd *)tmp___1;
    if ((unsigned long )efx_mtd == (unsigned long )((struct efx_mtd *)0)) {
      return (-12);
    } else {
    }
    efx_mtd->spi = (struct efx_spi_device const *)spi;
    efx_mtd->name = "flash";
    efx_mtd->ops = & falcon_mtd_ops;
    efx_mtd->n_parts = 1UL;
    efx_mtd->part[0].mtd.type = 3U;
    efx_mtd->part[0].mtd.flags = 3072U;
    efx_mtd->part[0].mtd.size = (uint64_t )(spi->size - 32768U);
    efx_mtd->part[0].mtd.erasesize = spi->erase_size;
    efx_mtd->part[0].ldv_42817.offset = 32768UL;
    efx_mtd->part[0].type_name = "sfc_flash_bootrom";
    rc = efx_mtd_probe_device(efx, efx_mtd);
    if (rc != 0) {
      kfree((void const *)efx_mtd);
      return (rc);
    } else {
    }
  } else {
  }
  spi = & nic_data->spi_eeprom;
  tmp___4 = efx_spi_present((struct efx_spi_device const *)spi);
  if ((int )tmp___4 && spi->size > 2048U) {
    tmp___3 = kzalloc(1632UL, 208U);
    efx_mtd = (struct efx_mtd *)tmp___3;
    if ((unsigned long )efx_mtd == (unsigned long )((struct efx_mtd *)0)) {
      return (-12);
    } else {
    }
    efx_mtd->spi = (struct efx_spi_device const *)spi;
    efx_mtd->name = "EEPROM";
    efx_mtd->ops = & falcon_mtd_ops;
    efx_mtd->n_parts = 1UL;
    efx_mtd->part[0].mtd.type = 1U;
    efx_mtd->part[0].mtd.flags = 7168U;
    _min1 = spi->size;
    _min2 = 6144U;
    efx_mtd->part[0].mtd.size = (uint64_t )((_min1 < _min2 ? _min1 : _min2) - 2048U);
    efx_mtd->part[0].mtd.erasesize = spi->erase_size;
    efx_mtd->part[0].ldv_42817.offset = 2048UL;
    efx_mtd->part[0].type_name = "sfc_bootconfig";
    rc = efx_mtd_probe_device(efx, efx_mtd);
    if (rc != 0) {
      kfree((void const *)efx_mtd);
      return (rc);
    } else {
    }
  } else {
  }
  return (rc);
}
}
static int siena_mtd_read(struct mtd_info *mtd , loff_t start , size_t len , size_t *retlen ,
                          u8 *buffer )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  loff_t offset ;
  loff_t end ;
  loff_t __min1 ;
  loff_t __min2 ;
  size_t chunk ;
  int rc ;
  size_t __min1___0 ;
  size_t __min2___0 ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  offset = start;
  __min1 = (loff_t )((unsigned long long )start + (unsigned long long )len);
  __min2 = (loff_t )mtd->size;
  end = __min1 < __min2 ? __min1 : __min2;
  rc = 0;
  goto ldv_43056;
  ldv_43055:
  __min1___0 = (size_t )(end - offset);
  __min2___0 = 128UL;
  chunk = __min1___0 < __min2___0 ? __min1___0 : __min2___0;
  rc = efx_mcdi_nvram_read(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type, offset,
                           buffer, chunk);
  if (rc != 0) {
    goto out;
  } else {
  }
  offset = (loff_t )((unsigned long long )offset + (unsigned long long )chunk);
  buffer = buffer + chunk;
  ldv_43056: ;
  if (offset < end) {
    goto ldv_43055;
  } else {
  }
  out:
  *retlen = (size_t )(offset - start);
  return (rc);
}
}
static int siena_mtd_erase(struct mtd_info *mtd , loff_t start , size_t len )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  loff_t offset ;
  loff_t end ;
  loff_t __min1 ;
  loff_t __min2 ;
  size_t chunk ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  offset = ~ ((long long )(mtd->erasesize - 1U)) & start;
  __min1 = (loff_t )((unsigned long long )start + (unsigned long long )len);
  __min2 = (loff_t )mtd->size;
  end = __min1 < __min2 ? __min1 : __min2;
  chunk = (size_t )part->mtd.erasesize;
  rc = 0;
  if (! part->ldv_42817.mcdi.updating) {
    rc = efx_mcdi_nvram_update_start(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type);
    if (rc != 0) {
      goto out;
    } else {
    }
    part->ldv_42817.mcdi.updating = 1;
  } else {
  }
  goto ldv_43077;
  ldv_43076:
  rc = efx_mcdi_nvram_erase(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type, offset,
                            chunk);
  if (rc != 0) {
    goto out;
  } else {
  }
  offset = (loff_t )((unsigned long long )offset + (unsigned long long )chunk);
  ldv_43077: ;
  if (offset < end) {
    goto ldv_43076;
  } else {
  }
  out: ;
  return (rc);
}
}
static int siena_mtd_write(struct mtd_info *mtd , loff_t start , size_t len , size_t *retlen ,
                           u8 const *buffer )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  loff_t offset ;
  loff_t end ;
  loff_t __min1 ;
  loff_t __min2 ;
  size_t chunk ;
  int rc ;
  size_t __min1___0 ;
  size_t __min2___0 ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  offset = start;
  __min1 = (loff_t )((unsigned long long )start + (unsigned long long )len);
  __min2 = (loff_t )mtd->size;
  end = __min1 < __min2 ? __min1 : __min2;
  rc = 0;
  if (! part->ldv_42817.mcdi.updating) {
    rc = efx_mcdi_nvram_update_start(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type);
    if (rc != 0) {
      goto out;
    } else {
    }
    part->ldv_42817.mcdi.updating = 1;
  } else {
  }
  goto ldv_43103;
  ldv_43102:
  __min1___0 = (size_t )(end - offset);
  __min2___0 = 128UL;
  chunk = __min1___0 < __min2___0 ? __min1___0 : __min2___0;
  rc = efx_mcdi_nvram_write(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type, offset,
                            buffer, chunk);
  if (rc != 0) {
    goto out;
  } else {
  }
  offset = (loff_t )((unsigned long long )offset + (unsigned long long )chunk);
  buffer = buffer + chunk;
  ldv_43103: ;
  if (offset < end) {
    goto ldv_43102;
  } else {
  }
  out:
  *retlen = (size_t )(offset - start);
  return (rc);
}
}
static int siena_mtd_sync(struct mtd_info *mtd )
{
  struct efx_mtd_partition *part ;
  struct mtd_info const *__mptr ;
  struct efx_mtd *efx_mtd ;
  struct efx_nic *efx ;
  int rc ;
  {
  __mptr = (struct mtd_info const *)mtd;
  part = (struct efx_mtd_partition *)__mptr;
  efx_mtd = (struct efx_mtd *)mtd->priv;
  efx = efx_mtd->efx;
  rc = 0;
  if ((int )part->ldv_42817.mcdi.updating) {
    part->ldv_42817.mcdi.updating = 0;
    rc = efx_mcdi_nvram_update_finish(efx, (unsigned int )part->ldv_42817.mcdi.nvram_type);
  } else {
  }
  return (rc);
}
}
static struct efx_mtd_ops const siena_mtd_ops = {& siena_mtd_read, & siena_mtd_erase, & siena_mtd_write, & siena_mtd_sync};
static struct siena_nvram_type_info const siena_nvram_types[14U] =
  { {0, "sfc_dummy_phy"},
        {0, "sfc_mcfw"},
        {0, "sfc_mcfw_backup"},
        {0, "sfc_static_cfg"},
        {1, "sfc_static_cfg"},
        {0, "sfc_dynamic_cfg"},
        {1, "sfc_dynamic_cfg"},
        {0, "sfc_exp_rom"},
        {0, "sfc_exp_rom_cfg"},
        {1, "sfc_exp_rom_cfg"},
        {0, "sfc_phy_fw"},
        {1, "sfc_phy_fw"},
        {0, 0},
        {0, "sfc_fpga"}};
static int siena_mtd_probe_partition(struct efx_nic *efx , struct efx_mtd *efx_mtd ,
                                     unsigned int part_id , unsigned int type )
{
  struct efx_mtd_partition *part ;
  struct siena_nvram_type_info const *info ;
  size_t size ;
  size_t erase_size ;
  bool protected ;
  int rc ;
  unsigned int tmp ;
  {
  part = (struct efx_mtd_partition *)(& efx_mtd->part) + (unsigned long )part_id;
  if (type > 13U || (unsigned long )siena_nvram_types[type].name == (unsigned long )((char const * )0)) {
    return (-19);
  } else {
  }
  info = (struct siena_nvram_type_info const *)(& siena_nvram_types) + (unsigned long )type;
  tmp = efx_port_num(efx);
  if ((unsigned int )info->port != tmp) {
    return (-19);
  } else {
  }
  rc = efx_mcdi_nvram_info(efx, type, & size, & erase_size, & protected);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if ((int )protected) {
    return (-19);
  } else {
  }
  part->ldv_42817.mcdi.nvram_type = (u8 )type;
  part->type_name = info->name;
  part->mtd.type = 3U;
  part->mtd.flags = 3072U;
  part->mtd.size = (uint64_t )size;
  part->mtd.erasesize = (uint32_t )erase_size;
  return (0);
}
}
static int siena_mtd_get_fw_subtypes(struct efx_nic *efx , struct efx_mtd *efx_mtd )
{
  struct efx_mtd_partition *part ;
  uint16_t fw_subtype_list[32U] ;
  int rc ;
  {
  rc = efx_mcdi_get_board_cfg(efx, 0, (u16 *)(& fw_subtype_list), 0);
  if (rc != 0) {
    return (rc);
  } else {
  }
  part = (struct efx_mtd_partition *)(& efx_mtd->part);
  goto ldv_43141;
  ldv_43140:
  part->ldv_42817.mcdi.fw_subtype = fw_subtype_list[(int )part->ldv_42817.mcdi.nvram_type];
  part = part + 1;
  ldv_43141: ;
  if ((unsigned long )((struct efx_mtd_partition *)(& efx_mtd->part) + efx_mtd->n_parts) != (unsigned long )part) {
    goto ldv_43140;
  } else {
  }
  return (0);
}
}
static int siena_mtd_probe(struct efx_nic *efx )
{
  struct efx_mtd *efx_mtd ;
  int rc ;
  u32 nvram_types ;
  unsigned int type ;
  int tmp ;
  long tmp___0 ;
  unsigned int tmp___1 ;
  void *tmp___2 ;
  {
  rc = -19;
  tmp = rtnl_is_locked();
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/mtd.c.prepared",
           741);
    dump_stack();
  } else {
  }
  rc = efx_mcdi_nvram_types(efx, & nvram_types);
  if (rc != 0) {
    return (rc);
  } else {
  }
  tmp___1 = __arch_hweight32(nvram_types);
  tmp___2 = kzalloc((unsigned long )tmp___1 * 1576UL + 56UL, 208U);
  efx_mtd = (struct efx_mtd *)tmp___2;
  if ((unsigned long )efx_mtd == (unsigned long )((struct efx_mtd *)0)) {
    return (-12);
  } else {
  }
  efx_mtd->name = "Siena NVRAM manager";
  efx_mtd->ops = & siena_mtd_ops;
  type = 0U;
  efx_mtd->n_parts = 0UL;
  goto ldv_43152;
  ldv_43151: ;
  if ((int )nvram_types & 1) {
    rc = siena_mtd_probe_partition(efx, efx_mtd, (unsigned int )efx_mtd->n_parts,
                                   type);
    if (rc == 0) {
      efx_mtd->n_parts = efx_mtd->n_parts + 1UL;
    } else
    if (rc != -19) {
      goto fail;
    } else {
    }
  } else {
  }
  type = type + 1U;
  nvram_types = nvram_types >> 1;
  ldv_43152: ;
  if (nvram_types != 0U) {
    goto ldv_43151;
  } else {
  }
  rc = siena_mtd_get_fw_subtypes(efx, efx_mtd);
  if (rc != 0) {
    goto fail;
  } else {
  }
  rc = efx_mtd_probe_device(efx, efx_mtd);
  fail: ;
  if (rc != 0) {
    kfree((void const *)efx_mtd);
  } else {
  }
  return (rc);
}
}
void ldv_main20_sequence_infinite_withcheck_stateful(void)
{
  struct mtd_info *var_group1 ;
  loff_t var_falcon_mtd_read_12_p1 ;
  size_t var_falcon_mtd_read_12_p2 ;
  size_t *var_falcon_mtd_read_12_p3 ;
  u8 *var_falcon_mtd_read_12_p4 ;
  loff_t var_falcon_mtd_erase_13_p1 ;
  size_t var_falcon_mtd_erase_13_p2 ;
  loff_t var_falcon_mtd_write_14_p1 ;
  size_t var_falcon_mtd_write_14_p2 ;
  size_t *var_falcon_mtd_write_14_p3 ;
  u8 const *var_falcon_mtd_write_14_p4 ;
  loff_t var_siena_mtd_read_17_p1 ;
  size_t var_siena_mtd_read_17_p2 ;
  size_t *var_siena_mtd_read_17_p3 ;
  u8 *var_siena_mtd_read_17_p4 ;
  loff_t var_siena_mtd_erase_18_p1 ;
  size_t var_siena_mtd_erase_18_p2 ;
  loff_t var_siena_mtd_write_19_p1 ;
  size_t var_siena_mtd_write_19_p2 ;
  size_t *var_siena_mtd_write_19_p3 ;
  u8 const *var_siena_mtd_write_19_p4 ;
  int tmp ;
  int tmp___0 ;
  {
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_43202;
  ldv_43201:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0:
  ldv_handler_precall();
  falcon_mtd_read(var_group1, var_falcon_mtd_read_12_p1, var_falcon_mtd_read_12_p2,
                  var_falcon_mtd_read_12_p3, var_falcon_mtd_read_12_p4);
  goto ldv_43192;
  case 1:
  ldv_handler_precall();
  falcon_mtd_erase(var_group1, var_falcon_mtd_erase_13_p1, var_falcon_mtd_erase_13_p2);
  goto ldv_43192;
  case 2:
  ldv_handler_precall();
  falcon_mtd_write(var_group1, var_falcon_mtd_write_14_p1, var_falcon_mtd_write_14_p2,
                   var_falcon_mtd_write_14_p3, var_falcon_mtd_write_14_p4);
  goto ldv_43192;
  case 3:
  ldv_handler_precall();
  falcon_mtd_sync(var_group1);
  goto ldv_43192;
  case 4:
  ldv_handler_precall();
  siena_mtd_read(var_group1, var_siena_mtd_read_17_p1, var_siena_mtd_read_17_p2, var_siena_mtd_read_17_p3,
                 var_siena_mtd_read_17_p4);
  goto ldv_43192;
  case 5:
  ldv_handler_precall();
  siena_mtd_erase(var_group1, var_siena_mtd_erase_18_p1, var_siena_mtd_erase_18_p2);
  goto ldv_43192;
  case 6:
  ldv_handler_precall();
  siena_mtd_write(var_group1, var_siena_mtd_write_19_p1, var_siena_mtd_write_19_p2,
                  var_siena_mtd_write_19_p3, var_siena_mtd_write_19_p4);
  goto ldv_43192;
  case 7:
  ldv_handler_precall();
  siena_mtd_sync(var_group1);
  goto ldv_43192;
  default: ;
  goto ldv_43192;
  }
  ldv_43192: ;
  ldv_43202:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    goto ldv_43201;
  } else {
  }
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_411(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_412(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_413(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_414(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_415(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_416(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_417(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_lock_interruptible_418(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___6 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_lock_interruptible(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_lock_interruptible_spi_lock(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_419(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_lock_interruptible_420(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___8 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_lock_interruptible(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_lock_interruptible_spi_lock(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_421(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_lock_interruptible_422(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___10 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_lock_interruptible(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_lock_interruptible_spi_lock(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_423(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_424(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_spi_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_425(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_spi_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int __test_and_set_bit(int nr , unsigned long volatile *addr )
{
  int oldbit ;
  {
  __asm__ ("bts %2,%1\n\tsbb %0,%0": "=r" (oldbit), "+m" (*((long volatile *)addr)): "Ir" (nr));
  return (oldbit);
}
}
__inline static void __list_splice(struct list_head const *list , struct list_head *prev ,
                                   struct list_head *next )
{
  struct list_head *first ;
  struct list_head *last ;
  {
  first = list->next;
  last = list->prev;
  first->prev = prev;
  prev->next = first;
  last->next = next;
  next->prev = last;
  return;
}
}
__inline static void list_splice_tail_init(struct list_head *list , struct list_head *head )
{
  int tmp ;
  {
  tmp = list_empty((struct list_head const *)list);
  if (tmp == 0) {
    __list_splice((struct list_head const *)list, head->prev, head);
    INIT_LIST_HEAD(list);
  } else {
  }
  return;
}
}
__inline static int ldv_mutex_is_locked_448(struct mutex *lock ) ;
int ldv_mutex_trylock_444(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_442(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_445(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_447(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_450(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_453(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_454(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_456(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_458(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_461(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_462(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_464(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_466(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_468(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_unlock_470(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_441(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_443(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_446(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_449(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_451(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_452(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_455(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_457(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_459(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_460(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_463(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_465(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_467(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_469(struct mutex *ldv_func_arg1 ) ;
void ldv_mutex_lock_local_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_local_lock(struct mutex *lock ) ;
void ldv_mutex_lock_status_lock(struct mutex *lock ) ;
int ldv_mutex_is_locked_status_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_status_lock(struct mutex *lock ) ;
void ldv_mutex_lock_txq_lock(struct mutex *lock ) ;
void ldv_mutex_unlock_txq_lock(struct mutex *lock ) ;
extern int pci_find_ext_capability(struct pci_dev * , int ) ;
extern int pci_bus_read_config_word(struct pci_bus * , unsigned int , int , u16 * ) ;
__inline static int pci_read_config_word(struct pci_dev const *dev , int where ,
                                         u16 *val )
{
  int tmp ;
  {
  tmp = pci_bus_read_config_word(dev->bus, dev->devfn, where, val);
  return (tmp);
}
}
__inline static int pci_domain_nr(struct pci_bus *bus )
{
  struct pci_sysdata *sd ;
  {
  sd = (struct pci_sysdata *)bus->sysdata;
  return (sd->domain);
}
}
extern int pci_enable_sriov(struct pci_dev * , int ) ;
extern void pci_disable_sriov(struct pci_dev * ) ;
__inline static void efx_filter_init_tx(struct efx_filter_spec *spec , unsigned int txq_id )
{
  {
  spec->type = 15U;
  spec->priority = 2U;
  spec->flags = 16U;
  spec->dmaq_id = (u16 )txq_id;
  return;
}
}
static unsigned int vf_max_tx_channels = 2U;
static int max_vfs = -1;
static struct workqueue_struct *vfdi_workqueue ;
static unsigned int abs_index(struct efx_vf *vf , unsigned int index )
{
  unsigned int tmp ;
  {
  tmp = efx_vf_size(vf->efx);
  return ((vf->index * tmp + index) + 128U);
}
}
static int efx_sriov_cmd(struct efx_nic *efx , bool enable , unsigned int *vi_scale_out ,
                         unsigned int *vf_total_out )
{
  u8 inbuf[12U] ;
  u8 outbuf[8U] ;
  unsigned int vi_scale ;
  unsigned int vf_total ;
  size_t outlen ;
  int rc ;
  {
  ((efx_dword_t *)(& inbuf))->u32[0] = (int )enable ? 1U : 0U;
  ((efx_dword_t *)(& inbuf) + 4U)->u32[0] = 128U;
  ((efx_dword_t *)(& inbuf) + 8U)->u32[0] = efx->vf_count;
  rc = efx_mcdi_rpc(efx, 48U, (u8 const *)(& inbuf), 12UL, (u8 *)(& outbuf), 8UL,
                    & outlen);
  if (rc != 0) {
    return (rc);
  } else {
  }
  if (outlen <= 7UL) {
    return (-5);
  } else {
  }
  vf_total = ((efx_dword_t *)(& outbuf) + 4U)->u32[0];
  vi_scale = ((efx_dword_t *)(& outbuf))->u32[0];
  if (vi_scale > 6U) {
    return (-95);
  } else {
  }
  if ((unsigned long )vi_scale_out != (unsigned long )((unsigned int *)0)) {
    *vi_scale_out = vi_scale;
  } else {
  }
  if ((unsigned long )vf_total_out != (unsigned long )((unsigned int *)0)) {
    *vf_total_out = vf_total;
  } else {
  }
  return (0);
}
}
static void efx_sriov_usrev(struct efx_nic *efx , bool enabled )
{
  efx_oword_t reg ;
  {
  reg.u64[0] = ((unsigned long long )(! enabled) << 16) | (unsigned long long )(efx->vfdi_channel)->channel;
  reg.u64[1] = 0ULL;
  efx_writeo(efx, & reg, 256U);
  return;
}
}
static int efx_sriov_memcpy(struct efx_nic *efx , struct efx_memcpy_req *req , unsigned int count )
{
  u8 *inbuf ;
  u8 *record ;
  unsigned int used ;
  u32 from_rid ;
  u32 from_hi ;
  u32 from_lo ;
  int rc ;
  int __ret_warn_on ;
  long tmp ;
  long tmp___0 ;
  void *tmp___1 ;
  int __ret_warn_on___0 ;
  long tmp___2 ;
  long tmp___3 ;
  size_t __len ;
  void *__ret ;
  unsigned int tmp___4 ;
  {
  __asm__ volatile ("mfence": : : "memory");
  used = count * 32U;
  __ret_warn_on = used > 252U;
  tmp = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
                       339);
  } else {
  }
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    return (-105);
  } else {
  }
  tmp___1 = kzalloc(252UL, 208U);
  inbuf = (u8 *)tmp___1;
  if ((unsigned long )inbuf == (unsigned long )((u8 *)0)) {
    return (-12);
  } else {
  }
  record = inbuf;
  ((efx_dword_t *)record)->u32[0] = count;
  goto ldv_42768;
  ldv_42767:
  ((efx_dword_t *)record + 4U)->u32[0] = req->to_rid;
  ((efx_dword_t *)record + 8U)->u32[0] = (unsigned int )req->to_addr;
  ((efx_dword_t *)record + 12U)->u32[0] = (unsigned int )(req->to_addr >> 32);
  if ((unsigned long )req->from_buf == (unsigned long )((void *)0)) {
    from_rid = req->from_rid;
    from_lo = (unsigned int )req->from_addr;
    from_hi = (unsigned int )(req->from_addr >> 32);
  } else {
    __ret_warn_on___0 = req->length + used > 252U;
    tmp___2 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
    if (tmp___2 != 0L) {
      warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
                         361);
    } else {
    }
    tmp___3 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
    if (tmp___3 != 0L) {
      rc = -105;
      goto out;
    } else {
    }
    from_rid = 256U;
    from_lo = used;
    from_hi = 0U;
    __len = (size_t )req->length;
    __ret = memcpy((void *)inbuf + (unsigned long )used, (void const *)req->from_buf,
                             __len);
    used = req->length + used;
  }
  ((efx_dword_t *)record + 16U)->u32[0] = from_rid;
  ((efx_dword_t *)record + 20U)->u32[0] = from_lo;
  ((efx_dword_t *)record + 24U)->u32[0] = from_hi;
  ((efx_dword_t *)record + 28U)->u32[0] = req->length;
  req = req + 1;
  record = record + 32UL;
  ldv_42768:
  tmp___4 = count;
  count = count - 1U;
  if (tmp___4 != 0U) {
    goto ldv_42767;
  } else {
  }
  rc = efx_mcdi_rpc(efx, 49U, (u8 const *)inbuf, (size_t )used, 0, 0UL, 0);
  out:
  kfree((void const *)inbuf);
  __asm__ volatile ("mfence": : : "memory");
  return (rc);
}
}
static void efx_sriov_reset_tx_filter(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct efx_filter_spec filter ;
  u16 vlan ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  bool tmp___0 ;
  __u16 tmp___1 ;
  unsigned int tmp___2 ;
  long tmp___3 ;
  struct _ddebug descriptor___0 ;
  long tmp___4 ;
  {
  efx = vf->efx;
  if (vf->tx_filter_id != -1) {
    efx_filter_remove_id_safe(efx, 2, (u32 )vf->tx_filter_id);
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_sriov_reset_tx_filter";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
      descriptor.format = "Removed vf %s tx filter %d\n";
      descriptor.lineno = 408U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "Removed vf %s tx filter %d\n", (char *)(& vf->pci_name),
                             vf->tx_filter_id);
      } else {
      }
    } else {
    }
    vf->tx_filter_id = -1;
  } else {
  }
  tmp___0 = is_zero_ether_addr((u8 const *)(& vf->addr.mac_addr));
  if ((int )tmp___0) {
    return;
  } else {
  }
  if ((unsigned int )vf->tx_filter_mode == 1U && vf_max_tx_channels <= 2U) {
    vf->tx_filter_mode = 2;
  } else {
  }
  tmp___1 = __fswab16((int )vf->addr.tci);
  vlan = (unsigned int )tmp___1 & 4095U;
  tmp___2 = abs_index(vf, 0U);
  efx_filter_init_tx(& filter, tmp___2);
  rc = efx_filter_set_eth_local(& filter, (unsigned int )vlan != 0U ? (int )vlan : 65535,
                                (u8 const *)(& vf->addr.mac_addr));
  tmp___3 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___3 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                         "i" (426), "i" (12UL));
    ldv_42779: ;
    goto ldv_42779;
  } else {
  }
  rc = efx_filter_insert_filter(efx, & filter, 1);
  if (rc < 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_warn((struct net_device const *)efx->net_dev, "Unable to migrate tx filter for vf %s\n",
                  (char *)(& vf->pci_name));
    } else {
      if ((efx->msg_enable & 8192U) != 0U) {
        descriptor___0.modname = "sfc";
        descriptor___0.function = "efx_sriov_reset_tx_filter";
        descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
        descriptor___0.format = "Inserted vf %s tx filter %d\n";
        descriptor___0.lineno = 435U;
        descriptor___0.flags = 0U;
        tmp___4 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
        if (tmp___4 != 0L) {
          __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                               "Inserted vf %s tx filter %d\n", (char *)(& vf->pci_name),
                               rc);
        } else {
        }
      } else {
      }
      vf->tx_filter_id = rc;
    }
  } else {
  }
  return;
}
}
static void efx_sriov_reset_rx_filter(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct efx_filter_spec filter ;
  u16 vlan ;
  int rc ;
  struct _ddebug descriptor ;
  long tmp ;
  bool tmp___0 ;
  __u16 tmp___1 ;
  unsigned int tmp___2 ;
  long tmp___3 ;
  struct _ddebug descriptor___0 ;
  long tmp___4 ;
  {
  efx = vf->efx;
  if (vf->rx_filter_id != -1) {
    efx_filter_remove_id_safe(efx, 2, (u32 )vf->rx_filter_id);
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor.modname = "sfc";
      descriptor.function = "efx_sriov_reset_rx_filter";
      descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
      descriptor.format = "Removed vf %s rx filter %d\n";
      descriptor.lineno = 452U;
      descriptor.flags = 0U;
      tmp = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
      if (tmp != 0L) {
        __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                             "Removed vf %s rx filter %d\n", (char *)(& vf->pci_name),
                             vf->rx_filter_id);
      } else {
      }
    } else {
    }
    vf->rx_filter_id = -1;
  } else {
  }
  if (! vf->rx_filtering) {
    return;
  } else {
    tmp___0 = is_zero_ether_addr((u8 const *)(& vf->addr.mac_addr));
    if ((int )tmp___0) {
      return;
    } else {
    }
  }
  tmp___1 = __fswab16((int )vf->addr.tci);
  vlan = (unsigned int )tmp___1 & 4095U;
  tmp___2 = abs_index(vf, vf->rx_filter_qid);
  efx_filter_init_rx(& filter, 2, vf->rx_filter_flags, tmp___2);
  rc = efx_filter_set_eth_local(& filter, (unsigned int )vlan != 0U ? (int )vlan : 65535,
                                (u8 const *)(& vf->addr.mac_addr));
  tmp___3 = ldv__builtin_expect(rc != 0, 0L);
  if (tmp___3 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                         "i" (466), "i" (12UL));
    ldv_42790: ;
    goto ldv_42790;
  } else {
  }
  rc = efx_filter_insert_filter(efx, & filter, 1);
  if (rc < 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_warn((struct net_device const *)efx->net_dev, "Unable to insert rx filter for vf %s\n",
                  (char *)(& vf->pci_name));
    } else {
      if ((efx->msg_enable & 8192U) != 0U) {
        descriptor___0.modname = "sfc";
        descriptor___0.function = "efx_sriov_reset_rx_filter";
        descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
        descriptor___0.format = "Inserted vf %s rx filter %d\n";
        descriptor___0.lineno = 475U;
        descriptor___0.flags = 0U;
        tmp___4 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
        if (tmp___4 != 0L) {
          __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                               "Inserted vf %s rx filter %d\n", (char *)(& vf->pci_name),
                               rc);
        } else {
        }
      } else {
      }
      vf->rx_filter_id = rc;
    }
  } else {
  }
  return;
}
}
static void __efx_sriov_update_vf_addr(struct efx_vf *vf )
{
  {
  efx_sriov_reset_tx_filter(vf);
  efx_sriov_reset_rx_filter(vf);
  queue_work(vfdi_workqueue, & (vf->efx)->peer_work);
  return;
}
}
static void __efx_sriov_push_vf_status(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_status *status ;
  struct efx_memcpy_req copy[4U] ;
  struct efx_endpoint_page *epp ;
  unsigned int pos ;
  unsigned int count ;
  unsigned int data_offset ;
  efx_qword_t event ;
  int __ret_warn_on ;
  int tmp ;
  long tmp___0 ;
  int __ret_warn_on___0 ;
  long tmp___1 ;
  struct list_head const *__mptr ;
  struct list_head const *__mptr___0 ;
  unsigned int tmp___2 ;
  {
  efx = vf->efx;
  status = (struct vfdi_status *)efx->vfdi_status.addr;
  tmp = ldv_mutex_is_locked_448(& vf->status_lock);
  __ret_warn_on = tmp == 0;
  tmp___0 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___0 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
                       502);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __ret_warn_on___0 = vf->status_addr == 0ULL;
  tmp___1 = ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  if (tmp___1 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
                       503);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on___0 != 0, 0L);
  status->local = vf->addr;
  status->generation_start = status->generation_start + 1U;
  status->generation_end = status->generation_start;
  memset((void *)(& copy), 0, 192UL);
  copy[0].from_buf = (void *)(& status->generation_start);
  copy[0].to_rid = vf->pci_rid;
  copy[0].to_addr = vf->status_addr;
  copy[0].length = 4U;
  data_offset = 8U;
  copy[1].from_rid = (efx->pci_dev)->devfn;
  copy[1].from_addr = efx->vfdi_status.dma_addr + (dma_addr_t )data_offset;
  copy[1].to_rid = vf->pci_rid;
  copy[1].to_addr = vf->status_addr + (u64 )data_offset;
  copy[1].length = status->length - data_offset;
  pos = 2U;
  count = 0U;
  __mptr = (struct list_head const *)efx->local_page_list.next;
  epp = (struct efx_endpoint_page *)__mptr;
  goto ldv_42820;
  ldv_42819: ;
  if (vf->peer_page_count == count) {
    goto ldv_42814;
  } else {
  }
  copy[pos].from_buf = 0;
  copy[pos].from_rid = (efx->pci_dev)->devfn;
  copy[pos].from_addr = epp->addr;
  copy[pos].to_rid = vf->pci_rid;
  copy[pos].to_addr = *(vf->peer_page_addrs + (unsigned long )count);
  copy[pos].length = 4096U;
  pos = pos + 1U;
  if (pos == 4U) {
    efx_sriov_memcpy(efx, (struct efx_memcpy_req *)(& copy), 4U);
    pos = 0U;
  } else {
  }
  count = count + 1U;
  __mptr___0 = (struct list_head const *)epp->link.next;
  epp = (struct efx_endpoint_page *)__mptr___0;
  ldv_42820: ;
  if ((unsigned long )(& epp->link) != (unsigned long )(& efx->local_page_list)) {
    goto ldv_42819;
  } else {
  }
  ldv_42814:
  copy[pos].from_buf = (void *)(& status->generation_end);
  copy[pos].to_rid = vf->pci_rid;
  copy[pos].to_addr = vf->status_addr + 4ULL;
  copy[pos].length = 4U;
  efx_sriov_memcpy(efx, (struct efx_memcpy_req *)(& copy), pos + 1U);
  event.u64[0] = (((unsigned long long )vf->msg_seqno << 24) & 4294967295ULL) | 0x8000000000040000ULL;
  vf->msg_seqno = vf->msg_seqno + 1U;
  tmp___2 = efx_vf_size(efx);
  efx_generate_event(efx, vf->index * tmp___2 + 128U, & event);
  return;
}
}
static void efx_sriov_bufs(struct efx_nic *efx , unsigned int offset , u64 *addr ,
                           unsigned int count )
{
  efx_qword_t buf ;
  unsigned int pos ;
  {
  pos = 0U;
  goto ldv_42830;
  ldv_42829:
  buf.u64[0] = (unsigned long )addr != (unsigned long )((u64 *)0) ? (*(addr + (unsigned long )pos) >> 12) << 14 : 0ULL;
  efx_sram_writeq(efx, efx->membase + 8388608UL, & buf, offset + pos);
  pos = pos + 1U;
  ldv_42830: ;
  if (pos < count) {
    goto ldv_42829;
  } else {
  }
  return;
}
}
static bool bad_vf_index(struct efx_nic *efx , unsigned int index )
{
  unsigned int tmp ;
  {
  tmp = efx_vf_size(efx);
  return (tmp <= index);
}
}
static bool bad_buf_count(unsigned int buf_count , unsigned int max_entry_count )
{
  unsigned int max_buf_count ;
  {
  max_buf_count = (unsigned int )(((unsigned long )max_entry_count * 8UL) / 4096UL);
  return ((bool )(((buf_count - 1U) & buf_count) != 0U || buf_count > max_buf_count));
}
}
static bool map_vi_index(struct efx_nic *efx , unsigned int abs_index___0 , struct efx_vf **vf_out ,
                         unsigned int *rel_index_out )
{
  unsigned int vf_i ;
  unsigned int tmp ;
  unsigned int tmp___0 ;
  {
  if (abs_index___0 <= 127U) {
    return (1);
  } else {
  }
  tmp = efx_vf_size(efx);
  vf_i = (abs_index___0 - 128U) / tmp;
  if (efx->vf_init_count <= vf_i) {
    return (1);
  } else {
  }
  if ((unsigned long )vf_out != (unsigned long )((struct efx_vf **)0)) {
    *vf_out = efx->vf + (unsigned long )vf_i;
  } else {
  }
  if ((unsigned long )rel_index_out != (unsigned long )((unsigned int *)0)) {
    tmp___0 = efx_vf_size(efx);
    *rel_index_out = abs_index___0 % tmp___0;
  } else {
  }
  return (0);
}
}
static int efx_vfdi_init_evq(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  unsigned int vf_evq ;
  unsigned int buf_count ;
  unsigned int abs_evq ;
  unsigned int tmp ;
  unsigned int buftbl ;
  efx_oword_t reg ;
  int tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  unsigned long tmp___3 ;
  size_t __len ;
  void *__ret ;
  {
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  vf_evq = req->u.init_evq.index;
  buf_count = req->u.init_evq.buf_count;
  tmp = abs_index(vf, vf_evq);
  abs_evq = tmp;
  buftbl = (vf->buftbl_base + vf_evq * 32U) + 16U;
  tmp___1 = bad_vf_index(efx, vf_evq);
  if ((int )tmp___1) {
    goto _L;
  } else {
    tmp___2 = bad_buf_count(buf_count, 8192U);
    if ((int )tmp___2) {
      _L:
      tmp___0 = net_ratelimit();
      if (tmp___0 != 0) {
        if ((efx->msg_enable & 8192U) != 0U) {
          netdev_err((struct net_device const *)efx->net_dev, "ERROR: Invalid INIT_EVQ from %s: evq %d bufs %d\n",
                     (char *)(& vf->pci_name), vf_evq, buf_count);
        } else {
        }
      } else {
      }
      return (-22);
    } else {
    }
  }
  efx_sriov_bufs(efx, buftbl, (u64 *)(& req->u.init_evq.addr), buf_count);
  reg.u64[0] = 8589934592ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, 16187392U, abs_evq);
  tmp___3 = __ffs((unsigned long )buf_count);
  reg.u64[0] = (((unsigned long long )tmp___3 << 20) | (unsigned long long )buftbl) | 8388608ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, 16121856U, abs_evq);
  if (vf_evq == 0U) {
    __len = (unsigned long )buf_count * 8UL;
    __ret = memcpy((void *)(& vf->evq0_addrs), (void const *)(& req->u.init_evq.addr),
                             __len);
    vf->evq0_count = buf_count;
  } else {
  }
  return (0);
}
}
static int efx_vfdi_init_rxq(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  unsigned int vf_rxq ;
  unsigned int vf_evq ;
  unsigned int buf_count ;
  unsigned int buftbl ;
  unsigned int label ;
  efx_oword_t reg ;
  int tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  unsigned int tmp___4 ;
  unsigned long tmp___5 ;
  unsigned int tmp___6 ;
  {
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  vf_rxq = req->u.init_rxq.index;
  vf_evq = req->u.init_rxq.evq;
  buf_count = req->u.init_rxq.buf_count;
  buftbl = (vf->buftbl_base + vf_rxq * 32U) + 8U;
  tmp___0 = bad_vf_index(efx, vf_evq);
  if ((int )tmp___0) {
    goto _L;
  } else {
    tmp___1 = bad_vf_index(efx, vf_rxq);
    if ((int )tmp___1) {
      goto _L;
    } else
    if (vf_rxq > 62U) {
      goto _L;
    } else {
      tmp___2 = bad_buf_count(buf_count, 4096U);
      if ((int )tmp___2) {
        _L:
        tmp = net_ratelimit();
        if (tmp != 0) {
          if ((efx->msg_enable & 8192U) != 0U) {
            netdev_err((struct net_device const *)efx->net_dev, "ERROR: Invalid INIT_RXQ from %s: rxq %d evq %d buf_count %d\n",
                       (char *)(& vf->pci_name), vf_rxq, vf_evq, buf_count);
          } else {
          }
        } else {
        }
        return (-22);
      } else {
      }
    }
  }
  tmp___3 = __test_and_set_bit((int )req->u.init_rxq.index, (unsigned long volatile *)(& vf->rxq_mask));
  if (tmp___3 != 0) {
    vf->rxq_count = vf->rxq_count + 1U;
  } else {
  }
  efx_sriov_bufs(efx, buftbl, (u64 *)(& req->u.init_rxq.addr), buf_count);
  label = req->u.init_rxq.label & 31U;
  tmp___4 = abs_index(vf, vf_evq);
  tmp___5 = __ffs((unsigned long )buf_count);
  reg.u64[0] = ((((((unsigned long long )buftbl << 36) | ((unsigned long long )tmp___4 << 24)) | ((unsigned long long )label << 5)) | ((unsigned long long )tmp___5 << 3)) | (((unsigned long long )req->u.init_rxq.flags & 1ULL) << 1)) | 1ULL;
  reg.u64[1] = 0ULL;
  tmp___6 = abs_index(vf, vf_rxq);
  efx_writeo_table(efx, & reg, 15990784U, tmp___6);
  return (0);
}
}
static int efx_vfdi_init_txq(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  unsigned int vf_txq ;
  unsigned int vf_evq ;
  unsigned int buf_count ;
  unsigned int buftbl ;
  unsigned int label ;
  unsigned int eth_filt_en ;
  efx_oword_t reg ;
  int tmp ;
  bool tmp___0 ;
  bool tmp___1 ;
  bool tmp___2 ;
  int tmp___3 ;
  unsigned int tmp___4 ;
  unsigned long tmp___5 ;
  unsigned int _min1 ;
  unsigned int _min2 ;
  unsigned int tmp___6 ;
  {
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  vf_txq = req->u.init_txq.index;
  vf_evq = req->u.init_txq.evq;
  buf_count = req->u.init_txq.buf_count;
  buftbl = vf->buftbl_base + vf_txq * 32U;
  tmp___0 = bad_vf_index(efx, vf_evq);
  if ((int )tmp___0) {
    goto _L;
  } else {
    tmp___1 = bad_vf_index(efx, vf_txq);
    if ((int )tmp___1) {
      goto _L;
    } else
    if (vf_txq >= vf_max_tx_channels) {
      goto _L;
    } else {
      tmp___2 = bad_buf_count(buf_count, 4096U);
      if ((int )tmp___2) {
        _L:
        tmp = net_ratelimit();
        if (tmp != 0) {
          if ((efx->msg_enable & 8192U) != 0U) {
            netdev_err((struct net_device const *)efx->net_dev, "ERROR: Invalid INIT_TXQ from %s: txq %d evq %d buf_count %d\n",
                       (char *)(& vf->pci_name), vf_txq, vf_evq, buf_count);
          } else {
          }
        } else {
        }
        return (-22);
      } else {
      }
    }
  }
  ldv_mutex_lock_449(& vf->txq_lock);
  tmp___3 = __test_and_set_bit((int )req->u.init_txq.index, (unsigned long volatile *)(& vf->txq_mask));
  if (tmp___3 != 0) {
    vf->txq_count = vf->txq_count + 1U;
  } else {
  }
  ldv_mutex_unlock_450(& vf->txq_lock);
  efx_sriov_bufs(efx, buftbl, (u64 *)(& req->u.init_txq.addr), buf_count);
  eth_filt_en = (unsigned int )vf->tx_filter_mode == 2U;
  label = req->u.init_txq.label & 31U;
  tmp___4 = abs_index(vf, vf_evq);
  tmp___5 = __ffs((unsigned long )buf_count);
  reg.u64[0] = ((((unsigned long long )buftbl << 36) | ((unsigned long long )tmp___4 << 24)) | ((unsigned long long )label << 5)) | ((unsigned long long )tmp___5 << 3);
  _min1 = efx->vi_scale;
  _min2 = 1U;
  reg.u64[1] = (((unsigned long long )(_min1 < _min2 ? _min1 : _min2) << 30) | ((unsigned long long )eth_filt_en << 29)) | 150994944ULL;
  tmp___6 = abs_index(vf, vf_txq);
  efx_writeo_table(efx, & reg, 16056320U, tmp___6);
  return (0);
}
}
static bool efx_vfdi_flush_wake(struct efx_vf *vf )
{
  int tmp ;
  int tmp___0 ;
  {
  __asm__ volatile ("mfence": : : "memory");
  if (vf->txq_count == 0U && vf->rxq_count == 0U) {
    tmp___0 = 1;
  } else {
    tmp = atomic_read((atomic_t const *)(& vf->rxq_retry_count));
    if (tmp != 0) {
      tmp___0 = 1;
    } else {
      tmp___0 = 0;
    }
  }
  return ((bool )tmp___0);
}
}
static void efx_vfdi_flush_clear(struct efx_vf *vf )
{
  {
  memset((void *)(& vf->txq_mask), 0, 8UL);
  vf->txq_count = 0U;
  memset((void *)(& vf->rxq_mask), 0, 8UL);
  vf->rxq_count = 0U;
  memset((void *)(& vf->rxq_retry_mask), 0, 8UL);
  atomic_set(& vf->rxq_retry_count, 0);
  return;
}
}
static int efx_vfdi_fini_all_queues(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  efx_oword_t reg ;
  unsigned int count ;
  unsigned int tmp ;
  unsigned int vf_offset ;
  unsigned int tmp___0 ;
  unsigned int timeout ;
  unsigned int index ;
  unsigned int rxqs_count ;
  __le32 *rxqs ;
  int rc ;
  void *tmp___1 ;
  int tmp___2 ;
  unsigned int tmp___3 ;
  int tmp___4 ;
  int __ret_warn_on ;
  long tmp___5 ;
  long __ret ;
  wait_queue_t __wait ;
  struct task_struct *tmp___6 ;
  bool tmp___7 ;
  bool tmp___8 ;
  int tmp___9 ;
  unsigned int tmp___10 ;
  int tmp___11 ;
  unsigned int tmp___12 ;
  {
  efx = vf->efx;
  tmp = efx_vf_size(efx);
  count = tmp;
  tmp___0 = efx_vf_size(efx);
  vf_offset = vf->index * tmp___0 + 128U;
  timeout = 250U;
  tmp___1 = kmalloc((unsigned long )count * 4UL, 208U);
  rxqs = (__le32 *)tmp___1;
  if ((unsigned long )rxqs == (unsigned long )((__le32 *)0)) {
    return (-12);
  } else {
  }
  rtnl_lock();
  siena_prepare_flush(efx);
  rtnl_unlock();
  rxqs_count = 0U;
  index = 0U;
  goto ldv_42927;
  ldv_42926:
  tmp___2 = variable_test_bit((int )index, (unsigned long const volatile *)(& vf->txq_mask));
  if (tmp___2 != 0) {
    reg.u64[0] = (unsigned long long )(vf_offset + index) | 4096ULL;
    reg.u64[1] = 0ULL;
    efx_writeo(efx, & reg, 2560U);
  } else {
  }
  tmp___4 = variable_test_bit((int )index, (unsigned long const volatile *)(& vf->rxq_mask));
  if (tmp___4 != 0) {
    tmp___3 = rxqs_count;
    rxqs_count = rxqs_count + 1U;
    *(rxqs + (unsigned long )tmp___3) = vf_offset + index;
  } else {
  }
  index = index + 1U;
  ldv_42927: ;
  if (index < count) {
    goto ldv_42926;
  } else {
  }
  atomic_set(& vf->rxq_retry_count, 0);
  goto ldv_42940;
  ldv_42939:
  rc = efx_mcdi_rpc(efx, 39U, (u8 const *)rxqs, (unsigned long )rxqs_count * 4UL,
                    0, 0UL, 0);
  __ret_warn_on = rc < 0;
  tmp___5 = ldv__builtin_expect(__ret_warn_on != 0, 0L);
  if (tmp___5 != 0L) {
    warn_slowpath_null("/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
                       807);
  } else {
  }
  ldv__builtin_expect(__ret_warn_on != 0, 0L);
  __ret = (long )timeout;
  tmp___8 = efx_vfdi_flush_wake(vf);
  if (tmp___8) {
    tmp___9 = 0;
  } else {
    tmp___9 = 1;
  }
  if (tmp___9) {
    tmp___6 = get_current();
    __wait.flags = 0U;
    __wait.private = (void *)tmp___6;
    __wait.func = & autoremove_wake_function;
    __wait.task_list.next = & __wait.task_list;
    __wait.task_list.prev = & __wait.task_list;
    ldv_42934:
    prepare_to_wait(& vf->flush_waitq, & __wait, 2);
    tmp___7 = efx_vfdi_flush_wake(vf);
    if ((int )tmp___7) {
      goto ldv_42933;
    } else {
    }
    __ret = schedule_timeout(__ret);
    if (__ret == 0L) {
      goto ldv_42933;
    } else {
    }
    goto ldv_42934;
    ldv_42933:
    finish_wait(& vf->flush_waitq, & __wait);
  } else {
  }
  timeout = (unsigned int )__ret;
  rxqs_count = 0U;
  index = 0U;
  goto ldv_42937;
  ldv_42936:
  tmp___11 = test_and_clear_bit((int )index, (unsigned long volatile *)(& vf->rxq_retry_mask));
  if (tmp___11 != 0) {
    atomic_dec(& vf->rxq_retry_count);
    tmp___10 = rxqs_count;
    rxqs_count = rxqs_count + 1U;
    *(rxqs + (unsigned long )tmp___10) = vf_offset + index;
  } else {
  }
  index = index + 1U;
  ldv_42937: ;
  if (index < count) {
    goto ldv_42936;
  } else {
  }
  ldv_42940: ;
  if (timeout != 0U && (vf->rxq_count != 0U || vf->txq_count != 0U)) {
    goto ldv_42939;
  } else {
  }
  rtnl_lock();
  siena_finish_flush(efx);
  rtnl_unlock();
  reg.u64[0] = 0ULL;
  reg.u64[1] = 0ULL;
  index = 0U;
  goto ldv_42943;
  ldv_42942:
  efx_writeo_table(efx, & reg, 15990784U, vf_offset + index);
  efx_writeo_table(efx, & reg, 16056320U, vf_offset + index);
  efx_writeo_table(efx, & reg, 16121856U, vf_offset + index);
  efx_writeo_table(efx, & reg, 16187392U, vf_offset + index);
  index = index + 1U;
  ldv_42943: ;
  if (index < count) {
    goto ldv_42942;
  } else {
  }
  tmp___12 = efx_vf_size(efx);
  efx_sriov_bufs(efx, vf->buftbl_base, 0, tmp___12 * 32U);
  kfree((void const *)rxqs);
  efx_vfdi_flush_clear(vf);
  vf->evq0_count = 0U;
  return (timeout != 0U ? 0 : -110);
}
}
static int efx_vfdi_insert_filter(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  unsigned int vf_rxq ;
  unsigned int flags ;
  int tmp ;
  bool tmp___0 ;
  {
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  vf_rxq = req->u.mac_filter.rxq;
  tmp___0 = bad_vf_index(efx, vf_rxq);
  if ((int )tmp___0 || (int )vf->rx_filtering) {
    tmp = net_ratelimit();
    if (tmp != 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "ERROR: Invalid INSERT_FILTER from %s: rxq %d flags 0x%x\n",
                   (char *)(& vf->pci_name), vf_rxq, req->u.mac_filter.flags);
      } else {
      }
    } else {
    }
    return (-22);
  } else {
  }
  flags = 0U;
  if ((int )req->u.mac_filter.flags & 1) {
    flags = flags | 1U;
  } else {
  }
  if ((req->u.mac_filter.flags & 2U) != 0U) {
    flags = flags | 2U;
  } else {
  }
  vf->rx_filter_flags = (enum efx_filter_flags )flags;
  vf->rx_filter_qid = vf_rxq;
  vf->rx_filtering = 1;
  efx_sriov_reset_rx_filter(vf);
  queue_work(vfdi_workqueue, & efx->peer_work);
  return (0);
}
}
static int efx_vfdi_remove_all_filters(struct efx_vf *vf )
{
  {
  vf->rx_filtering = 0;
  efx_sriov_reset_rx_filter(vf);
  queue_work(vfdi_workqueue, & (vf->efx)->peer_work);
  return (0);
}
}
static int efx_vfdi_set_status_page(struct efx_vf *vf )
{
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  u64 page_count___0 ;
  u64 max_page_count ;
  int tmp ;
  void *tmp___0 ;
  size_t __len ;
  void *__ret ;
  {
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  page_count___0 = req->u.set_status_page.peer_page_count;
  max_page_count = 508ULL;
  if (req->u.set_status_page.dma_addr == 0ULL || page_count___0 > max_page_count) {
    tmp = net_ratelimit();
    if (tmp != 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "ERROR: Invalid SET_STATUS_PAGE from %s\n",
                   (char *)(& vf->pci_name));
      } else {
      }
    } else {
    }
    return (-22);
  } else {
  }
  ldv_mutex_lock_451(& efx->local_lock);
  ldv_mutex_lock_452(& vf->status_lock);
  vf->status_addr = req->u.set_status_page.dma_addr;
  kfree((void const *)vf->peer_page_addrs);
  vf->peer_page_addrs = 0;
  vf->peer_page_count = 0U;
  if (page_count___0 != 0ULL) {
    tmp___0 = kcalloc((size_t )page_count___0, 8UL, 208U);
    vf->peer_page_addrs = (u64 *)tmp___0;
    if ((unsigned long )vf->peer_page_addrs != (unsigned long )((u64 *)0)) {
      __len = (size_t )(page_count___0 * 8ULL);
      __ret = memcpy((void *)vf->peer_page_addrs, (void const *)(& req->u.set_status_page.peer_page_addr),
                               __len);
      vf->peer_page_count = (unsigned int )page_count___0;
    } else {
    }
  } else {
  }
  __efx_sriov_push_vf_status(vf);
  ldv_mutex_unlock_453(& vf->status_lock);
  ldv_mutex_unlock_454(& efx->local_lock);
  return (0);
}
}
static int efx_vfdi_clear_status_page(struct efx_vf *vf )
{
  {
  ldv_mutex_lock_455(& vf->status_lock);
  vf->status_addr = 0ULL;
  ldv_mutex_unlock_456(& vf->status_lock);
  return (0);
}
}
static efx_vfdi_op_t vfdi_ops[9U] =
  { 0, & efx_vfdi_init_evq, & efx_vfdi_init_rxq, & efx_vfdi_init_txq,
        & efx_vfdi_fini_all_queues, & efx_vfdi_insert_filter, & efx_vfdi_remove_all_filters, & efx_vfdi_set_status_page,
        & efx_vfdi_clear_status_page};
static void efx_sriov_vfdi(struct work_struct *work )
{
  struct efx_vf *vf ;
  struct work_struct const *__mptr ;
  struct efx_nic *efx ;
  struct vfdi_req *req ;
  struct efx_memcpy_req copy[2U] ;
  int rc ;
  int tmp ;
  struct _ddebug descriptor ;
  long tmp___0 ;
  struct _ddebug descriptor___0 ;
  long tmp___1 ;
  {
  __mptr = (struct work_struct const *)work;
  vf = (struct efx_vf *)__mptr + 0xffffffffffffffe0UL;
  efx = vf->efx;
  req = (struct vfdi_req *)vf->buf.addr;
  memset((void *)(& copy), 0, 96UL);
  copy[0].from_rid = vf->pci_rid;
  copy[0].from_addr = vf->req_addr;
  copy[0].to_rid = (efx->pci_dev)->devfn;
  copy[0].to_addr = vf->buf.dma_addr;
  copy[0].length = 4096U;
  rc = efx_sriov_memcpy(efx, (struct efx_memcpy_req *)(& copy), 1U);
  if (rc != 0) {
    tmp = net_ratelimit();
    if (tmp != 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "ERROR: Unable to fetch VFDI request from %s rc %d\n",
                   (char *)(& vf->pci_name), - rc);
      } else {
      }
    } else {
    }
    vf->busy = 0;
    return;
  } else {
  }
  if (req->op <= 8U && (unsigned long )vfdi_ops[req->op] != (unsigned long )((efx_vfdi_op_t )0)) {
    rc = (*(vfdi_ops[req->op]))(vf);
    if (rc == 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        descriptor.modname = "sfc";
        descriptor.function = "efx_sriov_vfdi";
        descriptor.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
        descriptor.format = "vfdi request %d from %s ok\n";
        descriptor.lineno = 985U;
        descriptor.flags = 0U;
        tmp___0 = ldv__builtin_expect((long )descriptor.flags & 1L, 0L);
        if (tmp___0 != 0L) {
          __dynamic_netdev_dbg(& descriptor, (struct net_device const *)efx->net_dev,
                               "vfdi request %d from %s ok\n", req->op, (char *)(& vf->pci_name));
        } else {
        }
      } else {
      }
    } else {
    }
  } else {
    if ((efx->msg_enable & 8192U) != 0U) {
      descriptor___0.modname = "sfc";
      descriptor___0.function = "efx_sriov_vfdi";
      descriptor___0.filename = "/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared";
      descriptor___0.format = "ERROR: Unrecognised request %d from VF %s addr %llx\n";
      descriptor___0.lineno = 991U;
      descriptor___0.flags = 0U;
      tmp___1 = ldv__builtin_expect((long )descriptor___0.flags & 1L, 0L);
      if (tmp___1 != 0L) {
        __dynamic_netdev_dbg(& descriptor___0, (struct net_device const *)efx->net_dev,
                             "ERROR: Unrecognised request %d from VF %s addr %llx\n",
                             req->op, (char *)(& vf->pci_name), vf->req_addr);
      } else {
      }
    } else {
    }
    rc = -95;
  }
  vf->busy = 0;
  __asm__ volatile ("": : : "memory");
  req->rc = rc;
  req->op = 0U;
  memset((void *)(& copy), 0, 96UL);
  copy[0].from_buf = (void *)(& req->rc);
  copy[0].to_rid = vf->pci_rid;
  copy[0].to_addr = vf->req_addr + 8ULL;
  copy[0].length = 4U;
  copy[1].from_buf = (void *)(& req->op);
  copy[1].to_rid = vf->pci_rid;
  copy[1].to_addr = vf->req_addr;
  copy[1].length = 4U;
  efx_sriov_memcpy(efx, (struct efx_memcpy_req *)(& copy), 2U);
  return;
}
}
static void efx_sriov_reset_vf(struct efx_vf *vf , struct efx_buffer *buffer )
{
  struct efx_nic *efx ;
  struct efx_memcpy_req copy_req[4U] ;
  efx_qword_t event ;
  unsigned int pos ;
  unsigned int count ;
  unsigned int k ;
  unsigned int buftbl ;
  unsigned int abs_evq ;
  efx_oword_t reg ;
  efx_dword_t ptr ;
  int rc ;
  long tmp ;
  long tmp___0 ;
  size_t __len ;
  void *__ret ;
  unsigned int __min1 ;
  unsigned int __min2 ;
  int tmp___1 ;
  unsigned long tmp___2 ;
  {
  efx = vf->efx;
  tmp = ldv__builtin_expect(buffer->len != 4096U, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                         "i" (1032), "i" (12UL));
    ldv_43001: ;
    goto ldv_43001;
  } else {
  }
  if (vf->evq0_count == 0U) {
    return;
  } else {
  }
  tmp___0 = ldv__builtin_expect((vf->evq0_count & (vf->evq0_count - 1U)) != 0U, 0L);
  if (tmp___0 != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                         "i" (1036), "i" (12UL));
    ldv_43002: ;
    goto ldv_43002;
  } else {
  }
  ldv_mutex_lock_457(& vf->status_lock);
  event.u64[0] = ((unsigned long long )vf->msg_seqno << 24) | 0x8000000000050000ULL;
  vf->msg_seqno = vf->msg_seqno + 1U;
  pos = 0U;
  goto ldv_43007;
  ldv_43006:
  __len = 8UL;
  if (__len > 63UL) {
    __ret = memcpy(buffer->addr + (unsigned long )pos, (void const *)(& event),
                     __len);
  } else {
    __ret = memcpy(buffer->addr + (unsigned long )pos, (void const *)(& event),
                             __len);
  }
  pos = pos + 8U;
  ldv_43007: ;
  if (pos <= 4095U) {
    goto ldv_43006;
  } else {
  }
  pos = 0U;
  goto ldv_43019;
  ldv_43018:
  __min1 = vf->evq0_count - pos;
  __min2 = 4U;
  count = __min1 < __min2 ? __min1 : __min2;
  k = 0U;
  goto ldv_43015;
  ldv_43014:
  copy_req[k].from_buf = 0;
  copy_req[k].from_rid = (efx->pci_dev)->devfn;
  copy_req[k].from_addr = buffer->dma_addr;
  copy_req[k].to_rid = vf->pci_rid;
  copy_req[k].to_addr = vf->evq0_addrs[pos + k];
  copy_req[k].length = 4096U;
  k = k + 1U;
  ldv_43015: ;
  if (k < count) {
    goto ldv_43014;
  } else {
  }
  rc = efx_sriov_memcpy(efx, (struct efx_memcpy_req *)(& copy_req), count);
  if (rc != 0) {
    tmp___1 = net_ratelimit();
    if (tmp___1 != 0) {
      if ((efx->msg_enable & 8192U) != 0U) {
        netdev_err((struct net_device const *)efx->net_dev, "ERROR: Unable to notify %s of reset: %d\n",
                   (char *)(& vf->pci_name), - rc);
      } else {
      }
    } else {
    }
    goto ldv_43017;
  } else {
  }
  pos = pos + count;
  ldv_43019: ;
  if (vf->evq0_count > pos) {
    goto ldv_43018;
  } else {
  }
  ldv_43017:
  abs_evq = abs_index(vf, 0U);
  buftbl = vf->buftbl_base + 16U;
  efx_sriov_bufs(efx, buftbl, (u64 *)(& vf->evq0_addrs), vf->evq0_count);
  reg.u64[0] = 8589934592ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, 16187392U, abs_evq);
  tmp___2 = __ffs((unsigned long )vf->evq0_count);
  reg.u64[0] = (((unsigned long long )tmp___2 << 20) | (unsigned long long )buftbl) | 8388608ULL;
  reg.u64[1] = 0ULL;
  efx_writeo_table(efx, & reg, 16121856U, abs_evq);
  ptr.u32[0] = 0U;
  efx_writed(efx, & ptr, (abs_evq + 1024000U) * 16U);
  ldv_mutex_unlock_458(& vf->status_lock);
  return;
}
}
static void efx_sriov_reset_vf_work(struct work_struct *work )
{
  struct efx_vf *vf ;
  struct work_struct const *__mptr ;
  struct efx_nic *efx ;
  struct efx_buffer buf ;
  int tmp ;
  {
  __mptr = (struct work_struct const *)work;
  vf = (struct efx_vf *)__mptr + 0xffffffffffffffe0UL;
  efx = vf->efx;
  tmp = efx_nic_alloc_buffer(efx, & buf, 4096U);
  if (tmp == 0) {
    efx_sriov_reset_vf(vf, & buf);
    efx_nic_free_buffer(efx, & buf);
  } else {
  }
  return;
}
}
static void efx_sriov_handle_no_channel(struct efx_nic *efx )
{
  {
  if ((int )efx->msg_enable & 1) {
    netdev_err((struct net_device const *)efx->net_dev, "ERROR: IOV requires MSI-X and 1 additional interruptvector. IOV disabled\n");
  } else {
  }
  efx->vf_count = 0U;
  return;
}
}
static int efx_sriov_probe_channel(struct efx_channel *channel )
{
  {
  (channel->efx)->vfdi_channel = channel;
  return (0);
}
}
static void efx_sriov_get_channel_name(struct efx_channel *channel , char *buf , size_t len )
{
  {
  snprintf(buf, len, "%s-iov", (char *)(& (channel->efx)->name));
  return;
}
}
static struct efx_channel_type const efx_sriov_channel_type = {& efx_sriov_handle_no_channel, & efx_sriov_probe_channel, & efx_channel_dummy_op_void,
    & efx_sriov_get_channel_name, 0, 0, 1};
void efx_sriov_probe(struct efx_nic *efx )
{
  unsigned int count ;
  int tmp ;
  {
  if (max_vfs == 0) {
    return;
  } else {
  }
  tmp = efx_sriov_cmd(efx, 0, & efx->vi_scale, & count);
  if (tmp != 0) {
    return;
  } else {
  }
  if (count != 0U && (unsigned int )max_vfs < count) {
    count = (unsigned int )max_vfs;
  } else {
  }
  efx->vf_count = count;
  efx->extra_channel_type[0] = & efx_sriov_channel_type;
  return;
}
}
static void efx_sriov_peer_work(struct work_struct *data )
{
  struct efx_nic *efx ;
  struct work_struct const *__mptr ;
  struct vfdi_status *vfdi_status ;
  struct efx_vf *vf ;
  struct efx_local_addr *local_addr ;
  struct vfdi_endpoint *peer ;
  struct efx_endpoint_page *epp ;
  struct list_head pages ;
  unsigned int peer_space ;
  unsigned int peer_count ;
  unsigned int pos ;
  struct vfdi_endpoint *tmp ;
  long tmp___0 ;
  bool tmp___1 ;
  int tmp___2 ;
  struct list_head const *__mptr___0 ;
  size_t __len ;
  void *__ret ;
  void *tmp___3 ;
  struct list_head const *__mptr___1 ;
  int tmp___4 ;
  struct list_head const *__mptr___2 ;
  struct list_head const *__mptr___3 ;
  int tmp___5 ;
  {
  __mptr = (struct work_struct const *)data;
  efx = (struct efx_nic *)__mptr + 0xfffffffffffff4c0UL;
  vfdi_status = (struct vfdi_status *)efx->vfdi_status.addr;
  ldv_mutex_lock_459(& efx->local_lock);
  INIT_LIST_HEAD(& pages);
  list_splice_tail_init(& efx->local_page_list, & pages);
  peer = (struct vfdi_endpoint *)(& vfdi_status->peers) + 1UL;
  peer_space = 255U;
  peer_count = 1U;
  pos = 0U;
  goto ldv_43063;
  ldv_43062:
  vf = efx->vf + (unsigned long )pos;
  ldv_mutex_lock_460(& vf->status_lock);
  if ((int )vf->rx_filtering) {
    tmp___1 = is_zero_ether_addr((u8 const *)(& vf->addr.mac_addr));
    if (tmp___1) {
      tmp___2 = 0;
    } else {
      tmp___2 = 1;
    }
    if (tmp___2) {
      tmp = peer;
      peer = peer + 1;
      *tmp = vf->addr;
      peer_count = peer_count + 1U;
      peer_space = peer_space - 1U;
      tmp___0 = ldv__builtin_expect(peer_space == 0U, 0L);
      if (tmp___0 != 0L) {
        __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                             "i" (1185), "i" (12UL));
        ldv_43061: ;
        goto ldv_43061;
      } else {
      }
    } else {
    }
  } else {
  }
  ldv_mutex_unlock_461(& vf->status_lock);
  pos = pos + 1U;
  ldv_43063: ;
  if (efx->vf_count > pos) {
    goto ldv_43062;
  } else {
  }
  __mptr___0 = (struct list_head const *)efx->local_addr_list.next;
  local_addr = (struct efx_local_addr *)__mptr___0;
  goto ldv_43076;
  ldv_43075:
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& peer->mac_addr), (void const *)(& local_addr->addr),
                     __len);
  } else {
    __ret = memcpy((void *)(& peer->mac_addr), (void const *)(& local_addr->addr),
                             __len);
  }
  peer->tci = 0U;
  peer = peer + 1;
  peer_count = peer_count + 1U;
  peer_space = peer_space - 1U;
  if (peer_space == 0U) {
    tmp___4 = list_empty((struct list_head const *)(& pages));
    if (tmp___4 != 0) {
      tmp___3 = kmalloc(32UL, 208U);
      epp = (struct efx_endpoint_page *)tmp___3;
      if ((unsigned long )epp == (unsigned long )((struct efx_endpoint_page *)0)) {
        goto ldv_43072;
      } else {
      }
      epp->ptr = dma_alloc_attrs(& (efx->pci_dev)->dev, 4096UL, & epp->addr, 208U,
                                 0);
      if ((unsigned long )epp->ptr == (unsigned long )((void *)0)) {
        kfree((void const *)epp);
        goto ldv_43072;
      } else {
      }
    } else {
      __mptr___1 = (struct list_head const *)pages.next;
      epp = (struct efx_endpoint_page *)__mptr___1;
      list_del(& epp->link);
    }
    list_add_tail(& epp->link, & efx->local_page_list);
    peer = (struct vfdi_endpoint *)epp->ptr;
    peer_space = 512U;
  } else {
  }
  __mptr___2 = (struct list_head const *)local_addr->link.next;
  local_addr = (struct efx_local_addr *)__mptr___2;
  ldv_43076: ;
  if ((unsigned long )(& local_addr->link) != (unsigned long )(& efx->local_addr_list)) {
    goto ldv_43075;
  } else {
  }
  ldv_43072:
  vfdi_status->peer_count = (u16 )peer_count;
  ldv_mutex_unlock_462(& efx->local_lock);
  goto ldv_43080;
  ldv_43079:
  __mptr___3 = (struct list_head const *)pages.next;
  epp = (struct efx_endpoint_page *)__mptr___3;
  list_del(& epp->link);
  dma_free_attrs(& (efx->pci_dev)->dev, 4096UL, epp->ptr, epp->addr, 0);
  kfree((void const *)epp);
  ldv_43080:
  tmp___5 = list_empty((struct list_head const *)(& pages));
  if (tmp___5 == 0) {
    goto ldv_43079;
  } else {
  }
  pos = 0U;
  goto ldv_43083;
  ldv_43082:
  vf = efx->vf + (unsigned long )pos;
  ldv_mutex_lock_463(& vf->status_lock);
  if (vf->status_addr != 0ULL) {
    __efx_sriov_push_vf_status(vf);
  } else {
  }
  ldv_mutex_unlock_464(& vf->status_lock);
  pos = pos + 1U;
  ldv_43083: ;
  if (efx->vf_count > pos) {
    goto ldv_43082;
  } else {
  }
  return;
}
}
static void efx_sriov_free_local(struct efx_nic *efx )
{
  struct efx_local_addr *local_addr ;
  struct efx_endpoint_page *epp ;
  struct list_head const *__mptr ;
  int tmp ;
  struct list_head const *__mptr___0 ;
  int tmp___0 ;
  {
  goto ldv_43093;
  ldv_43092:
  __mptr = (struct list_head const *)efx->local_addr_list.next;
  local_addr = (struct efx_local_addr *)__mptr;
  list_del(& local_addr->link);
  kfree((void const *)local_addr);
  ldv_43093:
  tmp = list_empty((struct list_head const *)(& efx->local_addr_list));
  if (tmp == 0) {
    goto ldv_43092;
  } else {
  }
  goto ldv_43098;
  ldv_43097:
  __mptr___0 = (struct list_head const *)efx->local_page_list.next;
  epp = (struct efx_endpoint_page *)__mptr___0;
  list_del(& epp->link);
  dma_free_attrs(& (efx->pci_dev)->dev, 4096UL, epp->ptr, epp->addr, 0);
  kfree((void const *)epp);
  ldv_43098:
  tmp___0 = list_empty((struct list_head const *)(& efx->local_page_list));
  if (tmp___0 == 0) {
    goto ldv_43097;
  } else {
  }
  return;
}
}
static int efx_sriov_vf_alloc(struct efx_nic *efx )
{
  unsigned int index ;
  struct efx_vf *vf ;
  void *tmp ;
  struct lock_class_key __key ;
  atomic_long_t __constr_expr_0 ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_1 ;
  struct lock_class_key __key___1 ;
  struct lock_class_key __key___2 ;
  struct lock_class_key __key___3 ;
  {
  tmp = kzalloc((unsigned long )efx->vf_count * 912UL, 208U);
  efx->vf = (struct efx_vf *)tmp;
  if ((unsigned long )efx->vf == (unsigned long )((struct efx_vf *)0)) {
    return (-12);
  } else {
  }
  index = 0U;
  goto ldv_43113;
  ldv_43112:
  vf = efx->vf + (unsigned long )index;
  vf->efx = efx;
  vf->index = index;
  vf->rx_filter_id = -1;
  vf->tx_filter_mode = 1;
  vf->tx_filter_id = -1;
  __init_work(& vf->req, 0);
  __constr_expr_0.counter = 4195328L;
  vf->req.data = __constr_expr_0;
  lockdep_init_map(& vf->req.lockdep_map, "(&vf->req)", & __key, 0);
  INIT_LIST_HEAD(& vf->req.entry);
  vf->req.func = & efx_sriov_vfdi;
  __init_work(& vf->reset_work, 0);
  __constr_expr_1.counter = 4195328L;
  vf->reset_work.data = __constr_expr_1;
  lockdep_init_map(& vf->reset_work.lockdep_map, "(&vf->reset_work)", & __key___0,
                   0);
  INIT_LIST_HEAD(& vf->reset_work.entry);
  vf->reset_work.func = & efx_sriov_reset_vf_work;
  __init_waitqueue_head(& vf->flush_waitq, "&vf->flush_waitq", & __key___1);
  __mutex_init(& vf->status_lock, "&vf->status_lock", & __key___2);
  __mutex_init(& vf->txq_lock, "&vf->txq_lock", & __key___3);
  index = index + 1U;
  ldv_43113: ;
  if (efx->vf_count > index) {
    goto ldv_43112;
  } else {
  }
  return (0);
}
}
static void efx_sriov_vfs_fini(struct efx_nic *efx )
{
  struct efx_vf *vf ;
  unsigned int pos ;
  {
  pos = 0U;
  goto ldv_43121;
  ldv_43120:
  vf = efx->vf + (unsigned long )pos;
  efx_nic_free_buffer(efx, & vf->buf);
  kfree((void const *)vf->peer_page_addrs);
  vf->peer_page_addrs = 0;
  vf->peer_page_count = 0U;
  vf->evq0_count = 0U;
  pos = pos + 1U;
  ldv_43121: ;
  if (efx->vf_count > pos) {
    goto ldv_43120;
  } else {
  }
  return;
}
}
static int efx_sriov_vfs_init(struct efx_nic *efx )
{
  struct pci_dev *pci_dev ;
  unsigned int index ;
  unsigned int devfn ;
  unsigned int sriov ;
  unsigned int buftbl_base ;
  u16 offset ;
  u16 stride ;
  struct efx_vf *vf ;
  int rc ;
  int tmp ;
  unsigned int tmp___0 ;
  int tmp___1 ;
  {
  pci_dev = efx->pci_dev;
  tmp = pci_find_ext_capability(pci_dev, 16);
  sriov = (unsigned int )tmp;
  if (sriov == 0U) {
    return (-2);
  } else {
  }
  pci_read_config_word((struct pci_dev const *)pci_dev, (int )(sriov + 20U), & offset);
  pci_read_config_word((struct pci_dev const *)pci_dev, (int )(sriov + 22U), & stride);
  buftbl_base = efx->vf_buftbl_base;
  devfn = pci_dev->devfn + (unsigned int )offset;
  index = 0U;
  goto ldv_43137;
  ldv_43136:
  vf = efx->vf + (unsigned long )index;
  vf->buftbl_base = buftbl_base;
  tmp___0 = efx_vf_size(efx);
  buftbl_base = tmp___0 * 32U + buftbl_base;
  vf->pci_rid = devfn;
  tmp___1 = pci_domain_nr(pci_dev->bus);
  snprintf((char *)(& vf->pci_name), 13UL, "%04x:%02x:%02x.%d", tmp___1, (int )(pci_dev->bus)->number,
           (devfn >> 3) & 31U, devfn & 7U);
  rc = efx_nic_alloc_buffer(efx, & vf->buf, 4096U);
  if (rc != 0) {
    goto fail;
  } else {
  }
  devfn = (unsigned int )stride + devfn;
  index = index + 1U;
  ldv_43137: ;
  if (efx->vf_count > index) {
    goto ldv_43136;
  } else {
  }
  return (0);
  fail:
  efx_sriov_vfs_fini(efx);
  return (rc);
}
}
int efx_sriov_init(struct efx_nic *efx )
{
  struct net_device *net_dev ;
  struct vfdi_status *vfdi_status ;
  int rc ;
  struct lock_class_key __key ;
  struct lock_class_key __key___0 ;
  atomic_long_t __constr_expr_0 ;
  size_t __len ;
  void *__ret ;
  unsigned int tmp ;
  {
  net_dev = efx->net_dev;
  if (efx->vf_count == 0U) {
    return (0);
  } else {
  }
  rc = efx_sriov_cmd(efx, 1, 0, 0);
  if (rc != 0) {
    goto fail_cmd;
  } else {
  }
  rc = efx_nic_alloc_buffer(efx, & efx->vfdi_status, 2084U);
  if (rc != 0) {
    goto fail_status;
  } else {
  }
  vfdi_status = (struct vfdi_status *)efx->vfdi_status.addr;
  memset((void *)vfdi_status, 0, 2084UL);
  vfdi_status->version = 1U;
  vfdi_status->length = 2084U;
  vfdi_status->max_tx_channels = (u8 )vf_max_tx_channels;
  vfdi_status->vi_scale = (u8 )efx->vi_scale;
  vfdi_status->rss_rxq_count = (u8 )efx->rss_spread;
  vfdi_status->peer_count = (unsigned int )((u16 )efx->vf_count) + 1U;
  vfdi_status->timer_quantum_ns = efx->timer_quantum_ns;
  rc = efx_sriov_vf_alloc(efx);
  if (rc != 0) {
    goto fail_alloc;
  } else {
  }
  __mutex_init(& efx->local_lock, "&efx->local_lock", & __key);
  __init_work(& efx->peer_work, 0);
  __constr_expr_0.counter = 4195328L;
  efx->peer_work.data = __constr_expr_0;
  lockdep_init_map(& efx->peer_work.lockdep_map, "(&efx->peer_work)", & __key___0,
                   0);
  INIT_LIST_HEAD(& efx->peer_work.entry);
  efx->peer_work.func = & efx_sriov_peer_work;
  INIT_LIST_HEAD(& efx->local_addr_list);
  INIT_LIST_HEAD(& efx->local_page_list);
  rc = efx_sriov_vfs_init(efx);
  if (rc != 0) {
    goto fail_vfs;
  } else {
  }
  rtnl_lock();
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& vfdi_status->peers[0].mac_addr), (void const *)net_dev->dev_addr,
                     __len);
  } else {
    __ret = memcpy((void *)(& vfdi_status->peers[0].mac_addr), (void const *)net_dev->dev_addr,
                             __len);
  }
  efx->vf_init_count = efx->vf_count;
  rtnl_unlock();
  efx_sriov_usrev(efx, 1);
  rc = pci_enable_sriov(efx->pci_dev, (int )efx->vf_count);
  if (rc != 0) {
    goto fail_pci;
  } else {
  }
  if ((efx->msg_enable & 2U) != 0U) {
    tmp = efx_vf_size(efx);
    netdev_info((struct net_device const *)net_dev, "enabled SR-IOV for %d VFs, %d VI per VF\n",
                efx->vf_count, tmp);
  } else {
  }
  return (0);
  fail_pci:
  efx_sriov_usrev(efx, 0);
  rtnl_lock();
  efx->vf_init_count = 0U;
  rtnl_unlock();
  efx_sriov_vfs_fini(efx);
  fail_vfs:
  cancel_work_sync(& efx->peer_work);
  efx_sriov_free_local(efx);
  kfree((void const *)efx->vf);
  fail_alloc:
  efx_nic_free_buffer(efx, & efx->vfdi_status);
  fail_status:
  efx_sriov_cmd(efx, 0, 0, 0);
  fail_cmd: ;
  return (rc);
}
}
void efx_sriov_fini(struct efx_nic *efx )
{
  struct efx_vf *vf ;
  unsigned int pos ;
  long tmp ;
  {
  if (efx->vf_init_count == 0U) {
    return;
  } else {
  }
  tmp = ldv__builtin_expect((long )(efx->vfdi_channel)->enabled, 0L);
  if (tmp != 0L) {
    __asm__ volatile ("1:\tud2\n.pushsection __bug_table,\"a\"\n2:\t.long 1b - 2b, %c0 - 2b\n\t.word %c1, 0\n\t.org 2b+%c2\n.popsection": : "i" ((char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared"),
                         "i" (1443), "i" (12UL));
    ldv_43161: ;
    goto ldv_43161;
  } else {
  }
  efx_sriov_usrev(efx, 0);
  rtnl_lock();
  efx->vf_init_count = 0U;
  rtnl_unlock();
  pos = 0U;
  goto ldv_43163;
  ldv_43162:
  vf = efx->vf + (unsigned long )pos;
  cancel_work_sync(& vf->req);
  cancel_work_sync(& vf->reset_work);
  pos = pos + 1U;
  ldv_43163: ;
  if (efx->vf_count > pos) {
    goto ldv_43162;
  } else {
  }
  cancel_work_sync(& efx->peer_work);
  pci_disable_sriov(efx->pci_dev);
  efx_sriov_vfs_fini(efx);
  efx_sriov_free_local(efx);
  kfree((void const *)efx->vf);
  efx_nic_free_buffer(efx, & efx->vfdi_status);
  efx_sriov_cmd(efx, 0, 0, 0);
  return;
}
}
void efx_sriov_event(struct efx_channel *channel , efx_qword_t *event )
{
  struct efx_nic *efx ;
  struct efx_vf *vf ;
  unsigned int qid ;
  unsigned int seq ;
  unsigned int type ;
  unsigned int data ;
  bool tmp ;
  unsigned int tmp___0 ;
  int tmp___1 ;
  {
  efx = channel->efx;
  qid = (unsigned int )(event->u64[0] >> 32) & 1023U;
  seq = (unsigned int )(event->u64[0] >> 24) & 255U;
  type = (unsigned int )(event->u64[0] >> 16) & 255U;
  data = (unsigned int )event->u64[0] & 65535U;
  tmp = map_vi_index(efx, qid, & vf, 0);
  if ((int )tmp) {
    return;
  } else {
  }
  if ((int )vf->busy) {
    goto error;
  } else {
  }
  if (type == 0U) {
    vf->req_type = 0;
    vf->req_seqno = seq + 1U;
    vf->req_addr = 0ULL;
  } else {
    tmp___0 = vf->req_seqno;
    vf->req_seqno = vf->req_seqno + 1U;
    if ((tmp___0 & 255U) != seq || (unsigned int )vf->req_type != type) {
      goto error;
    } else {
    }
  }
  switch (vf->req_type) {
  case 0: ;
  case 1: ;
  case 2:
  vf->req_addr = vf->req_addr | ((unsigned long long )data << (vf->req_type << 4));
  vf->req_type = vf->req_type + 1;
  return;
  case 3:
  vf->req_addr = vf->req_addr | ((unsigned long long )data << 48);
  vf->req_type = 0;
  vf->busy = 1;
  queue_work(vfdi_workqueue, & vf->req);
  return;
  }
  error:
  tmp___1 = net_ratelimit();
  if (tmp___1 != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "ERROR: Screaming VFDI request from %s\n",
                 (char *)(& vf->pci_name));
    } else {
    }
  } else {
  }
  vf->req_type = 0;
  vf->req_seqno = seq + 1U;
  return;
}
}
void efx_sriov_flr(struct efx_nic *efx , unsigned int vf_i )
{
  struct efx_vf *vf ;
  {
  if (efx->vf_init_count < vf_i) {
    return;
  } else {
  }
  vf = efx->vf + (unsigned long )vf_i;
  if ((efx->msg_enable & 8192U) != 0U) {
    netdev_info((struct net_device const *)efx->net_dev, "FLR on VF %s\n", (char *)(& vf->pci_name));
  } else {
  }
  vf->status_addr = 0ULL;
  efx_vfdi_remove_all_filters(vf);
  efx_vfdi_flush_clear(vf);
  vf->evq0_count = 0U;
  return;
}
}
void efx_sriov_mac_address_changed(struct efx_nic *efx )
{
  struct vfdi_status *vfdi_status ;
  size_t __len ;
  void *__ret ;
  {
  vfdi_status = (struct vfdi_status *)efx->vfdi_status.addr;
  if (efx->vf_init_count == 0U) {
    return;
  } else {
  }
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& vfdi_status->peers[0].mac_addr), (void const *)(efx->net_dev)->dev_addr,
                     __len);
  } else {
    __ret = memcpy((void *)(& vfdi_status->peers[0].mac_addr), (void const *)(efx->net_dev)->dev_addr,
                             __len);
  }
  queue_work(vfdi_workqueue, & efx->peer_work);
  return;
}
}
void efx_sriov_tx_flush_done(struct efx_nic *efx , efx_qword_t *event )
{
  struct efx_vf *vf ;
  unsigned int queue ;
  unsigned int qid ;
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  queue = (unsigned int )event->u64[0] & 16383U;
  tmp = map_vi_index(efx, queue, & vf, & qid);
  if ((int )tmp) {
    return;
  } else {
  }
  tmp___0 = variable_test_bit((int )qid, (unsigned long const volatile *)(& vf->txq_mask));
  if (tmp___0 == 0) {
    return;
  } else {
  }
  __clear_bit((int )qid, (unsigned long volatile *)(& vf->txq_mask));
  vf->txq_count = vf->txq_count - 1U;
  tmp___1 = efx_vfdi_flush_wake(vf);
  if ((int )tmp___1) {
    __wake_up(& vf->flush_waitq, 3U, 1, 0);
  } else {
  }
  return;
}
}
void efx_sriov_rx_flush_done(struct efx_nic *efx , efx_qword_t *event )
{
  struct efx_vf *vf ;
  unsigned int ev_failed ;
  unsigned int queue ;
  unsigned int qid ;
  bool tmp ;
  int tmp___0 ;
  bool tmp___1 ;
  {
  queue = (unsigned int )event->u64[0] & 4095U;
  ev_failed = (unsigned int )(event->u64[0] >> 12) & 1U;
  tmp = map_vi_index(efx, queue, & vf, & qid);
  if ((int )tmp) {
    return;
  } else {
  }
  tmp___0 = variable_test_bit((int )qid, (unsigned long const volatile *)(& vf->rxq_mask));
  if (tmp___0 == 0) {
    return;
  } else {
  }
  if (ev_failed != 0U) {
    set_bit(qid, (unsigned long volatile *)(& vf->rxq_retry_mask));
    atomic_inc(& vf->rxq_retry_count);
  } else {
    __clear_bit((int )qid, (unsigned long volatile *)(& vf->rxq_mask));
    vf->rxq_count = vf->rxq_count - 1U;
  }
  tmp___1 = efx_vfdi_flush_wake(vf);
  if ((int )tmp___1) {
    __wake_up(& vf->flush_waitq, 3U, 1, 0);
  } else {
  }
  return;
}
}
void efx_sriov_desc_fetch_err(struct efx_nic *efx , unsigned int dmaq )
{
  struct efx_vf *vf ;
  unsigned int rel ;
  bool tmp ;
  int tmp___0 ;
  {
  tmp = map_vi_index(efx, dmaq, & vf, & rel);
  if ((int )tmp) {
    return;
  } else {
  }
  tmp___0 = net_ratelimit();
  if (tmp___0 != 0) {
    if ((efx->msg_enable & 8192U) != 0U) {
      netdev_err((struct net_device const *)efx->net_dev, "VF %d DMA Q %d reports descriptor fetch error.\n",
                 vf->index, rel);
    } else {
    }
  } else {
  }
  queue_work(vfdi_workqueue, & vf->reset_work);
  return;
}
}
void efx_sriov_reset(struct efx_nic *efx )
{
  unsigned int vf_i ;
  struct efx_buffer buf ;
  struct efx_vf *vf ;
  int tmp ;
  long tmp___0 ;
  int tmp___1 ;
  {
  tmp = rtnl_is_locked();
  tmp___0 = ldv__builtin_expect(tmp == 0, 0L);
  if (tmp___0 != 0L) {
    printk("\vRTNL: assertion failed at %s (%d)\n", (char *)"/home/mikhail/launches/cpachecker-regression2/launcher-working-dir/ldv-manager-work-dir/work/current--X--drivers/net/ethernet/sfc/sfc.ko--X--regression-testlinux-3.8-rc1--X--32_7a--X--cpachecker/linux-3.8-rc1/csd_deg_dscv/33/dscv_tempdir/dscv/ri/32_7a/drivers/net/ethernet/sfc/siena_sriov.c.prepared",
           1618);
    dump_stack();
  } else {
  }
  if (efx->vf_init_count == 0U) {
    return;
  } else {
  }
  efx_sriov_usrev(efx, 1);
  efx_sriov_cmd(efx, 1, 0, 0);
  tmp___1 = efx_nic_alloc_buffer(efx, & buf, 4096U);
  if (tmp___1 != 0) {
    return;
  } else {
  }
  vf_i = 0U;
  goto ldv_43221;
  ldv_43220:
  vf = efx->vf + (unsigned long )vf_i;
  efx_sriov_reset_vf(vf, & buf);
  vf_i = vf_i + 1U;
  ldv_43221: ;
  if (efx->vf_init_count > vf_i) {
    goto ldv_43220;
  } else {
  }
  efx_nic_free_buffer(efx, & buf);
  return;
}
}
int efx_init_sriov(void)
{
  struct lock_class_key __key ;
  char const *__lock_name ;
  struct workqueue_struct *tmp ;
  {
  __lock_name = "sfc_vfdi";
  tmp = __alloc_workqueue_key("sfc_vfdi", 10U, 1, & __key, __lock_name);
  vfdi_workqueue = tmp;
  if ((unsigned long )vfdi_workqueue == (unsigned long )((struct workqueue_struct *)0)) {
    return (-12);
  } else {
  }
  return (0);
}
}
void efx_fini_sriov(void)
{
  {
  destroy_workqueue(vfdi_workqueue);
  return;
}
}
int efx_sriov_set_vf_mac(struct net_device *net_dev , int vf_i , u8 *mac )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_vf *vf ;
  size_t __len ;
  void *__ret ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned int )vf_i >= efx->vf_init_count) {
    return (-22);
  } else {
  }
  vf = efx->vf + (unsigned long )vf_i;
  ldv_mutex_lock_465(& vf->status_lock);
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& vf->addr.mac_addr), (void const *)mac, __len);
  } else {
    __ret = memcpy((void *)(& vf->addr.mac_addr), (void const *)mac, __len);
  }
  __efx_sriov_update_vf_addr(vf);
  ldv_mutex_unlock_466(& vf->status_lock);
  return (0);
}
}
int efx_sriov_set_vf_vlan(struct net_device *net_dev , int vf_i , u16 vlan , u8 qos )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_vf *vf ;
  u16 tci ;
  __u16 tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned int )vf_i >= efx->vf_init_count) {
    return (-22);
  } else {
  }
  vf = efx->vf + (unsigned long )vf_i;
  ldv_mutex_lock_467(& vf->status_lock);
  tci = (u16 )(((int )((short )vlan) & 4095) | (int )((short )((int )qos << 13)));
  tmp___0 = __fswab16((int )tci);
  vf->addr.tci = tmp___0;
  __efx_sriov_update_vf_addr(vf);
  ldv_mutex_unlock_468(& vf->status_lock);
  return (0);
}
}
int efx_sriov_set_vf_spoofchk(struct net_device *net_dev , int vf_i , bool spoofchk )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_vf *vf ;
  int rc ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned int )vf_i >= efx->vf_init_count) {
    return (-22);
  } else {
  }
  vf = efx->vf + (unsigned long )vf_i;
  ldv_mutex_lock_469(& vf->txq_lock);
  if (vf->txq_count == 0U) {
    vf->tx_filter_mode = (int )spoofchk ? 2 : 0;
    rc = 0;
  } else {
    rc = -16;
  }
  ldv_mutex_unlock_470(& vf->txq_lock);
  return (rc);
}
}
int efx_sriov_get_vf_config(struct net_device *net_dev , int vf_i , struct ifla_vf_info *ivi )
{
  struct efx_nic *efx ;
  void *tmp ;
  struct efx_vf *vf ;
  u16 tci ;
  size_t __len ;
  void *__ret ;
  __u16 tmp___0 ;
  {
  tmp = netdev_priv((struct net_device const *)net_dev);
  efx = (struct efx_nic *)tmp;
  if ((unsigned int )vf_i >= efx->vf_init_count) {
    return (-22);
  } else {
  }
  vf = efx->vf + (unsigned long )vf_i;
  ivi->vf = (__u32 )vf_i;
  __len = 6UL;
  if (__len > 63UL) {
    __ret = memcpy((void *)(& ivi->mac), (void const *)(& vf->addr.mac_addr),
                     __len);
  } else {
    __ret = memcpy((void *)(& ivi->mac), (void const *)(& vf->addr.mac_addr),
                             __len);
  }
  ivi->tx_rate = 0U;
  tmp___0 = __fswab16((int )vf->addr.tci);
  tci = tmp___0;
  ivi->vlan = (__u32 )tci & 4095U;
  ivi->qos = (__u32 )((int )tci >> 13) & 7U;
  ivi->spoofchk = (unsigned int )vf->tx_filter_mode == 2U;
  return (0);
}
}
void ldv_main21_sequence_infinite_withcheck_stateful(void)
{
  struct efx_nic *var_group1 ;
  struct efx_channel *var_group2 ;
  char *var_efx_sriov_get_channel_name_27_p1 ;
  size_t var_efx_sriov_get_channel_name_27_p2 ;
  int tmp ;
  int tmp___0 ;
  {
  LDV_IN_INTERRUPT = 1;
  ldv_initialize();
  goto ldv_43296;
  ldv_43295:
  tmp = __VERIFIER_nondet_int();
  switch (tmp) {
  case 0:
  ldv_handler_precall();
  efx_sriov_handle_no_channel(var_group1);
  goto ldv_43291;
  case 1:
  ldv_handler_precall();
  efx_sriov_probe_channel(var_group2);
  goto ldv_43291;
  case 2:
  ldv_handler_precall();
  efx_sriov_get_channel_name(var_group2, var_efx_sriov_get_channel_name_27_p1, var_efx_sriov_get_channel_name_27_p2);
  goto ldv_43291;
  default: ;
  goto ldv_43291;
  }
  ldv_43291: ;
  ldv_43296:
  tmp___0 = __VERIFIER_nondet_int();
  if (tmp___0 != 0) {
    goto ldv_43295;
  } else {
  }
  ldv_check_final_state();
  return;
}
}
void ldv_mutex_lock_441(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_442(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_443(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
int ldv_mutex_trylock_444(struct mutex *ldv_func_arg1 )
{
  ldv_func_ret_type___2 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_trylock(ldv_func_arg1);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_trylock_mutex(ldv_func_arg1);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_unlock_445(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_446(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_cred_guard_mutex(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_447(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_cred_guard_mutex(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static int ldv_mutex_is_locked_448(struct mutex *lock )
{
  ldv_func_ret_type___6 ldv_func_res ;
  int tmp ;
  int tmp___0 ;
  {
  tmp = mutex_is_locked(lock);
  ldv_func_res = tmp;
  tmp___0 = ldv_mutex_is_locked_status_lock(lock);
  return (tmp___0);
  return (ldv_func_res);
}
}
void ldv_mutex_lock_449(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_txq_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_450(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_txq_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_451(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_local_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_452(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_453(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_454(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_local_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_455(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_456(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_457(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_458(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_459(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_local_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_460(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_461(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_462(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_local_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_463(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_464(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_465(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_466(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_467(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_status_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_468(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_status_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_lock_469(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_lock_txq_lock(ldv_func_arg1);
  mutex_lock(ldv_func_arg1);
  return;
}
}
void ldv_mutex_unlock_470(struct mutex *ldv_func_arg1 )
{
  {
  ldv_mutex_unlock_txq_lock(ldv_func_arg1);
  mutex_unlock(ldv_func_arg1);
  return;
}
}
__inline static void ldv_error(void) __attribute__((__no_instrument_function__)) ;
__inline static void ldv_error(void)
{
  {
  ERROR: __VERIFIER_error();
}
}
extern int __VERIFIER_nondet_int(void) ;
long ldv__builtin_expect(long exp , long c )
{
  {
  return (exp);
}
}
static int ldv_mutex_cred_guard_mutex ;
int ldv_mutex_lock_interruptible_cred_guard_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_cred_guard_mutex = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_cred_guard_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_cred_guard_mutex = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_cred_guard_mutex(struct mutex *lock )
{
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_cred_guard_mutex = 2;
  return;
}
}
int ldv_mutex_trylock_cred_guard_mutex(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_cred_guard_mutex = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_cred_guard_mutex(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_cred_guard_mutex = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_cred_guard_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_cred_guard_mutex(struct mutex *lock )
{
  {
  if (ldv_mutex_cred_guard_mutex == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_cred_guard_mutex = 1;
  return;
}
}
static int ldv_mutex_local_lock ;
int ldv_mutex_lock_interruptible_local_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_local_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_local_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_local_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_local_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_local_lock = 2;
  return;
}
}
int ldv_mutex_trylock_local_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_local_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_local_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_local_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_local_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_local_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_local_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_local_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_local_lock = 1;
  return;
}
}
static int ldv_mutex_lock ;
int ldv_mutex_lock_interruptible_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_lock = 2;
  return;
}
}
int ldv_mutex_trylock_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_lock = 1;
  return;
}
}
static int ldv_mutex_mac_lock ;
int ldv_mutex_lock_interruptible_mac_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mac_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_mac_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mac_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_mac_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_mac_lock = 2;
  return;
}
}
int ldv_mutex_trylock_mac_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_mac_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_mac_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_mac_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_mac_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mac_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_mac_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_mac_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_mac_lock = 1;
  return;
}
}
static int ldv_mutex_mdio_lock ;
int ldv_mutex_lock_interruptible_mdio_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mdio_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_mdio_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mdio_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_mdio_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_mdio_lock = 2;
  return;
}
}
int ldv_mutex_trylock_mdio_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_mdio_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_mdio_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_mdio_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_mdio_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mdio_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_mdio_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_mdio_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_mdio_lock = 1;
  return;
}
}
static int ldv_mutex_mutex ;
int ldv_mutex_lock_interruptible_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mutex = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_mutex = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_mutex(struct mutex *lock )
{
  {
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_mutex = 2;
  return;
}
}
int ldv_mutex_trylock_mutex(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_mutex = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_mutex(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_mutex = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_mutex(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_mutex == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_mutex(struct mutex *lock )
{
  {
  if (ldv_mutex_mutex == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_mutex = 1;
  return;
}
}
static int ldv_mutex_spi_lock ;
int ldv_mutex_lock_interruptible_spi_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_spi_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_spi_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_spi_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_spi_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_spi_lock = 2;
  return;
}
}
int ldv_mutex_trylock_spi_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_spi_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_spi_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_spi_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_spi_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_spi_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_spi_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_spi_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_spi_lock = 1;
  return;
}
}
static int ldv_mutex_status_lock ;
int ldv_mutex_lock_interruptible_status_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_status_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_status_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_status_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_status_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_status_lock = 2;
  return;
}
}
int ldv_mutex_trylock_status_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_status_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_status_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_status_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_status_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_status_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_status_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_status_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_status_lock = 1;
  return;
}
}
static int ldv_mutex_txq_lock ;
int ldv_mutex_lock_interruptible_txq_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_txq_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_txq_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_txq_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_txq_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_txq_lock = 2;
  return;
}
}
int ldv_mutex_trylock_txq_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_txq_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_txq_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_txq_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_txq_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_txq_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_txq_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_txq_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_txq_lock = 1;
  return;
}
}
static int ldv_mutex_update_lock ;
int ldv_mutex_lock_interruptible_update_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_update_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
int ldv_mutex_lock_killable_update_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  nondetermined = __VERIFIER_nondet_int();
  if (nondetermined) {
    ldv_mutex_update_lock = 2;
    return (0);
  } else {
    return (-4);
  }
}
}
void ldv_mutex_lock_update_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  ldv_mutex_update_lock = 2;
  return;
}
}
int ldv_mutex_trylock_update_lock(struct mutex *lock )
{
  int is_mutex_held_by_another_thread ;
  {
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  is_mutex_held_by_another_thread = __VERIFIER_nondet_int();
  if (is_mutex_held_by_another_thread) {
    return (0);
  } else {
    ldv_mutex_update_lock = 2;
    return (1);
  }
}
}
int ldv_atomic_dec_and_mutex_lock_update_lock(atomic_t *cnt , struct mutex *lock )
{
  int atomic_value_after_dec ;
  {
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  atomic_value_after_dec = __VERIFIER_nondet_int();
  if (atomic_value_after_dec == 0) {
    ldv_mutex_update_lock = 2;
    return (1);
  } else {
  }
  return (0);
}
}
int ldv_mutex_is_locked_update_lock(struct mutex *lock )
{
  int nondetermined ;
  {
  if (ldv_mutex_update_lock == 1) {
    nondetermined = __VERIFIER_nondet_int();
    if (nondetermined) {
      return (0);
    } else {
      return (1);
    }
  } else {
    return (1);
  }
}
}
void ldv_mutex_unlock_update_lock(struct mutex *lock )
{
  {
  if (ldv_mutex_update_lock == 2) {
  } else {
    ldv_error();
  }
  ldv_mutex_update_lock = 1;
  return;
}
}
void ldv_initialize(void)
{
  {
  ldv_mutex_cred_guard_mutex = 1;
  ldv_mutex_local_lock = 1;
  ldv_mutex_lock = 1;
  ldv_mutex_mac_lock = 1;
  ldv_mutex_mdio_lock = 1;
  ldv_mutex_mutex = 1;
  ldv_mutex_spi_lock = 1;
  ldv_mutex_status_lock = 1;
  ldv_mutex_txq_lock = 1;
  ldv_mutex_update_lock = 1;
  return;
}
}
void ldv_check_final_state(void)
{
  {
  if (ldv_mutex_cred_guard_mutex == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_local_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_mac_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_mdio_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_mutex == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_spi_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_status_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_txq_lock == 1) {
  } else {
    ldv_error();
  }
  if (ldv_mutex_update_lock == 1) {
  } else {
    ldv_error();
  }
  return;
}
}
void *external_alloc(void);
struct sk_buff *__alloc_skb(unsigned int arg0, gfp_t arg1, int arg2, int arg3) {
  return (struct sk_buff *)external_alloc();
}
void *external_alloc(void);
struct workqueue_struct *__alloc_workqueue_key(const char *arg0, unsigned int arg1, int arg2, struct lock_class_key *arg3, const char *arg4, ...) {
  return (struct workqueue_struct *)external_alloc();
}
void __bitmap_or(unsigned long *arg0, const unsigned long *arg1, const unsigned long *arg2, int arg3) {
  return;
}
void __const_udelay(unsigned long arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int __dynamic_netdev_dbg(struct _ddebug *arg0, const struct net_device *arg1, const char *arg2, ...) {
  return __VERIFIER_nondet_int();
}
void __free_pages(struct page *arg0, unsigned int arg1) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool __get_page_tail(struct page *arg0) {
  return __VERIFIER_nondet_bool();
}
void __init_waitqueue_head(wait_queue_head_t *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
void __init_work(struct work_struct *arg0, int arg1) {
  return;
}
void __list_add(struct list_head *arg0, struct list_head *arg1, struct list_head *arg2) {
  return;
}
void __list_del_entry(struct list_head *arg0) {
  return;
}
void __might_sleep(const char *arg0, int arg1, int arg2) {
  return;
}
void __mutex_init(struct mutex *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
void __napi_schedule(struct napi_struct *arg0) {
  return;
}
void *external_alloc(void);
struct sk_buff *__netdev_alloc_skb(struct net_device *arg0, unsigned int arg1, gfp_t arg2) {
  return (struct sk_buff *)external_alloc();
}
void __netif_schedule(struct Qdisc *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int __pci_register_driver(struct pci_driver *arg0, struct module *arg1, const char *arg2) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int __phys_addr(unsigned long arg0) {
  return __VERIFIER_nondet_ulong();
}
void *external_alloc(void);
unsigned char *__pskb_pull_tail(struct sk_buff *arg0, int arg1) {
  return (unsigned char *)external_alloc();
}
void __raw_spin_lock_init(raw_spinlock_t *arg0, const char *arg1, struct lock_class_key *arg2) {
  return;
}
void __wake_up(wait_queue_head_t *arg0, unsigned int arg1, int arg2, void *arg3) {
  return;
}
int __VERIFIER_nondet_int(void);
int _cond_resched() {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _copy_from_user(void *arg0, const void *arg1, unsigned int arg2) {
  return __VERIFIER_nondet_ulong();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _copy_to_user(void *arg0, const void *arg1, unsigned int arg2) {
  return __VERIFIER_nondet_ulong();
}
void _raw_spin_lock(raw_spinlock_t *arg0) {
  return;
}
void _raw_spin_lock_bh(raw_spinlock_t *arg0) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int _raw_spin_lock_irqsave(raw_spinlock_t *arg0) {
  return __VERIFIER_nondet_ulong();
}
int __VERIFIER_nondet_int(void);
int _raw_spin_trylock_bh(raw_spinlock_t *arg0) {
  return __VERIFIER_nondet_int();
}
void _raw_spin_unlock(raw_spinlock_t *arg0) {
  return;
}
void _raw_spin_unlock_bh(raw_spinlock_t *arg0) {
  return;
}
void _raw_spin_unlock_irqrestore(raw_spinlock_t *arg0, unsigned long arg1) {
  return;
}
void *external_alloc(void);
struct cpu_rmap *alloc_cpu_rmap(unsigned int arg0, gfp_t arg1) {
  return (struct cpu_rmap *)external_alloc();
}
void *external_alloc(void);
struct net_device *alloc_etherdev_mqs(int arg0, unsigned int arg1, unsigned int arg2) {
  return (struct net_device *)external_alloc();
}
void *external_alloc(void);
struct page *alloc_pages_current(gfp_t arg0, unsigned int arg1) {
  return (struct page *)external_alloc();
}
bool __VERIFIER_nondet_bool(void);
bool cancel_delayed_work_sync(struct delayed_work *arg0) {
  return __VERIFIER_nondet_bool();
}
bool __VERIFIER_nondet_bool(void);
bool cancel_work_sync(struct work_struct *arg0) {
  return __VERIFIER_nondet_bool();
}
void consume_skb(struct sk_buff *arg0) {
  return;
}
unsigned int __VERIFIER_nondet_uint(void);
u32 crc32_le(u32 arg0, const unsigned char *arg1, size_t arg2) {
  return __VERIFIER_nondet_uint();
}
void debug_dma_alloc_coherent(struct device *arg0, size_t arg1, dma_addr_t arg2, void *arg3) {
  return;
}
void debug_dma_free_coherent(struct device *arg0, size_t arg1, void *arg2, dma_addr_t arg3) {
  return;
}
void debug_dma_map_page(struct device *arg0, struct page *arg1, size_t arg2, size_t arg3, int arg4, dma_addr_t arg5, bool arg6) {
  return;
}
void debug_dma_mapping_error(struct device *arg0, dma_addr_t arg1) {
  return;
}
void debug_dma_unmap_page(struct device *arg0, dma_addr_t arg1, size_t arg2, int arg3, bool arg4) {
  return;
}
int __VERIFIER_nondet_int(void);
int del_timer_sync(struct timer_list *arg0) {
  return __VERIFIER_nondet_int();
}
void destroy_workqueue(struct workqueue_struct *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int dev_alloc_name(struct net_device *arg0, const char *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int dev_close(struct net_device *arg0) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
void *dev_get_drvdata(const struct device *arg0) {
  return (void *)external_alloc();
}
void dev_kfree_skb_any(struct sk_buff *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int dev_open(struct net_device *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int dev_set_drvdata(struct device *arg0, void *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int device_create_file(struct device *arg0, const struct device_attribute *arg1) {
  return __VERIFIER_nondet_int();
}
void device_remove_file(struct device *arg0, const struct device_attribute *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int dma_set_mask(struct device *arg0, u64 arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int dma_supported(struct device *arg0, u64 arg1) {
  return __VERIFIER_nondet_int();
}
void dql_completed(struct dql *arg0, unsigned int arg1) {
  return;
}
void dql_reset(struct dql *arg0) {
  return;
}
void dump_stack() {
  return;
}
unsigned short __VERIFIER_nondet_ushort(void);
__be16 eth_type_trans(struct sk_buff *arg0, struct net_device *arg1) {
  return __VERIFIER_nondet_ushort();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int find_next_bit(const unsigned long *arg0, unsigned long arg1, unsigned long arg2) {
  return __VERIFIER_nondet_ulong();
}
void finish_wait(wait_queue_head_t *arg0, wait_queue_t *arg1) {
  return;
}
void free_cpumask_var(cpumask_var_t arg0) {
  return;
}
void free_irq(unsigned int arg0, void *arg1) {
  return;
}
void free_irq_cpu_rmap(struct cpu_rmap *arg0) {
  return;
}
void free_netdev(struct net_device *arg0) {
  return;
}
void get_random_bytes(void *arg0, int arg1) {
  return;
}
void getnstimeofday(struct timespec *arg0) {
  return;
}
void *external_alloc(void);
struct device *hwmon_device_register(struct device *arg0) {
  return (struct device *)external_alloc();
}
void hwmon_device_unregister(struct device *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int i2c_bit_add_bus(struct i2c_adapter *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int i2c_del_adapter(struct i2c_adapter *arg0) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct i2c_client *i2c_new_device(struct i2c_adapter *arg0, const struct i2c_board_info *arg1) {
  return (struct i2c_client *)external_alloc();
}
void *external_alloc(void);
struct i2c_client *i2c_new_dummy(struct i2c_adapter *arg0, u16 arg1) {
  return (struct i2c_client *)external_alloc();
}
int __VERIFIER_nondet_int(void);
s32 i2c_smbus_read_byte_data(const struct i2c_client *arg0, u8 arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
s32 i2c_smbus_write_byte_data(const struct i2c_client *arg0, u8 arg1, u8 arg2) {
  return __VERIFIER_nondet_int();
}
void i2c_unregister_device(struct i2c_client *arg0) {
  return;
}
void init_timer_key(struct timer_list *arg0, unsigned int arg1, const char *arg2, struct lock_class_key *arg3) {
  return;
}
void *external_alloc(void);
void *ioremap_nocache(resource_size_t arg0, unsigned long arg1) {
  return (void *)external_alloc();
}
void iounmap(volatile void *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int irq_cpu_rmap_add(struct cpu_rmap *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void kfree_skb(struct sk_buff *arg0) {
  return;
}
void ldv_check_return_value(int arg0) {
  return;
}
void ldv_check_return_value_probe(int arg0) {
  return;
}
void ldv_handler_precall() {
  return;
}
void list_del(struct list_head *arg0) {
  return;
}
void local_bh_disable() {
  return;
}
void local_bh_enable() {
  return;
}
void lockdep_init_map(struct lockdep_map *arg0, const char *arg1, struct lock_class_key *arg2, int arg3) {
  return;
}
void mdio45_ethtool_gset_npage(const struct mdio_if_info *arg0, struct ethtool_cmd *arg1, u32 arg2, u32 arg3) {
  return;
}
int __VERIFIER_nondet_int(void);
int mdio45_links_ok(const struct mdio_if_info *arg0, u32 arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int mdio45_nway_restart(const struct mdio_if_info *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int mdio_mii_ioctl(const struct mdio_if_info *arg0, struct mii_ioctl_data *arg1, int arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int mdio_set_flag(const struct mdio_if_info *arg0, int arg1, int arg2, u16 arg3, int arg4, bool arg5) {
  return __VERIFIER_nondet_int();
}
void might_fault() {
  return;
}
int __VERIFIER_nondet_int(void);
int mod_timer(struct timer_list *arg0, unsigned long arg1) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int msecs_to_jiffies(const unsigned int arg0) {
  return __VERIFIER_nondet_ulong();
}
void msleep(unsigned int arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int mtd_device_parse_register(struct mtd_info *arg0, const char **arg1, struct mtd_part_parser_data *arg2, const struct mtd_partition *arg3, int arg4) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int mtd_device_unregister(struct mtd_info *arg0) {
  return __VERIFIER_nondet_int();
}
void mtd_erase_callback(struct erase_info *arg0) {
  return;
}
void mutex_lock(struct mutex *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int mutex_lock_interruptible(struct mutex *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int mutex_trylock(struct mutex *arg0) {
  return __VERIFIER_nondet_int();
}
void mutex_unlock(struct mutex *arg0) {
  return;
}
void napi_complete(struct napi_struct *arg0) {
  return;
}
void *external_alloc(void);
struct sk_buff *napi_get_frags(struct napi_struct *arg0) {
  return (struct sk_buff *)external_alloc();
}
int __VERIFIER_nondet_int(void);
gro_result_t napi_gro_frags(struct napi_struct *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
gro_result_t napi_gro_receive(struct napi_struct *arg0, struct sk_buff *arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int net_ratelimit() {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netdev_err(const struct net_device *arg0, const char *arg1, ...) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netdev_info(const struct net_device *arg0, const char *arg1, ...) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netdev_warn(const struct net_device *arg0, const char *arg1, ...) {
  return __VERIFIER_nondet_int();
}
void netif_carrier_off(struct net_device *arg0) {
  return;
}
void netif_carrier_on(struct net_device *arg0) {
  return;
}
void netif_device_attach(struct net_device *arg0) {
  return;
}
void netif_device_detach(struct net_device *arg0) {
  return;
}
void netif_napi_add(struct net_device *arg0, struct napi_struct *arg1, int (*arg2)(struct napi_struct *, int), int arg3) {
  return;
}
void netif_napi_del(struct napi_struct *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int netif_receive_skb(struct sk_buff *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netif_set_real_num_rx_queues(struct net_device *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netif_set_real_num_tx_queues(struct net_device *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int netpoll_trap() {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
void __VERIFIER_assume(int);
struct timespec ns_to_timespec(const s64 arg0) {
  struct timespec *tmp = (struct timespec*)external_alloc();
  __VERIFIER_assume(tmp != 0);
  return *tmp;
}
int __VERIFIER_nondet_int(void);
int pci_bus_read_config_word(struct pci_bus *arg0, unsigned int arg1, int arg2, u16 *arg3) {
  return __VERIFIER_nondet_int();
}
void pci_clear_master(struct pci_dev *arg0) {
  return;
}
void *external_alloc(void);
struct pci_dev *pci_dev_get(struct pci_dev *arg0) {
  return (struct pci_dev *)external_alloc();
}
void pci_dev_put(struct pci_dev *arg0) {
  return;
}
void pci_disable_device(struct pci_dev *arg0) {
  return;
}
void pci_disable_msi(struct pci_dev *arg0) {
  return;
}
void pci_disable_msix(struct pci_dev *arg0) {
  return;
}
void pci_disable_sriov(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_enable_device(struct pci_dev *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_enable_msi_block(struct pci_dev *arg0, unsigned int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_enable_msix(struct pci_dev *arg0, struct msix_entry *arg1, int arg2) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_enable_sriov(struct pci_dev *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_find_ext_capability(struct pci_dev *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct pci_dev *pci_get_device(unsigned int arg0, unsigned int arg1, struct pci_dev *arg2) {
  return (struct pci_dev *)external_alloc();
}
long __VERIFIER_nondet_long(void);
ssize_t pci_read_vpd(struct pci_dev *arg0, loff_t arg1, size_t arg2, void *arg3) {
  return __VERIFIER_nondet_long();
}
void pci_release_region(struct pci_dev *arg0, int arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_request_region(struct pci_dev *arg0, int arg1, const char *arg2) {
  return __VERIFIER_nondet_int();
}
void pci_restore_state(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_save_state(struct pci_dev *arg0) {
  return __VERIFIER_nondet_int();
}
void pci_set_master(struct pci_dev *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_set_power_state(struct pci_dev *arg0, pci_power_t arg1) {
  return __VERIFIER_nondet_int();
}
void pci_unregister_driver(struct pci_driver *arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int pci_vpd_find_info_keyword(const u8 *arg0, unsigned int arg1, unsigned int arg2, const char *arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_vpd_find_tag(const u8 *arg0, unsigned int arg1, unsigned int arg2, u8 arg3) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int pci_wake_from_d3(struct pci_dev *arg0, bool arg1) {
  return __VERIFIER_nondet_int();
}
void prepare_to_wait(wait_queue_head_t *arg0, wait_queue_t *arg1, int arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int printk(const char *arg0, ...) {
  return __VERIFIER_nondet_int();
}
void ptp_clock_event(struct ptp_clock *arg0, struct ptp_clock_event *arg1) {
  return;
}
int __VERIFIER_nondet_int(void);
int ptp_clock_index(struct ptp_clock *arg0) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct ptp_clock *ptp_clock_register(struct ptp_clock_info *arg0, struct device *arg1) {
  return (struct ptp_clock *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int ptp_clock_unregister(struct ptp_clock *arg0) {
  return __VERIFIER_nondet_int();
}
void put_page(struct page *arg0) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool queue_delayed_work(struct workqueue_struct *arg0, struct delayed_work *arg1, unsigned long arg2) {
  return __VERIFIER_nondet_bool();
}
bool __VERIFIER_nondet_bool(void);
bool queue_work(struct workqueue_struct *arg0, struct work_struct *arg1) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int register_netdevice(struct net_device *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int register_netdevice_notifier(struct notifier_block *arg0) {
  return __VERIFIER_nondet_int();
}
int __VERIFIER_nondet_int(void);
int request_threaded_irq(unsigned int arg0, irqreturn_t (*arg1)(int, void *), irqreturn_t (*arg2)(int, void *), unsigned long arg3, const char *arg4, void *arg5) {
  return __VERIFIER_nondet_int();
}
unsigned long __VERIFIER_nondet_ulong(void);
unsigned long int round_jiffies_up(unsigned long arg0) {
  return __VERIFIER_nondet_ulong();
}
bool __VERIFIER_nondet_bool(void);
bool rps_may_expire_flow(struct net_device *arg0, u16 arg1, u32 arg2, u16 arg3) {
  return __VERIFIER_nondet_bool();
}
int __VERIFIER_nondet_int(void);
int rtnl_is_locked() {
  return __VERIFIER_nondet_int();
}
void rtnl_lock() {
  return;
}
void rtnl_unlock() {
  return;
}
void schedule() {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool schedule_delayed_work(struct delayed_work *arg0, unsigned long arg1) {
  return __VERIFIER_nondet_bool();
}
long __VERIFIER_nondet_long(void);
long int schedule_timeout(long arg0) {
  return __VERIFIER_nondet_long();
}
long __VERIFIER_nondet_long(void);
long int schedule_timeout_uninterruptible(long arg0) {
  return __VERIFIER_nondet_long();
}
void set_normalized_timespec(struct timespec *arg0, time_t arg1, s64 arg2) {
  return;
}
int __VERIFIER_nondet_int(void);
int skb_checksum_help(struct sk_buff *arg0) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
struct sk_buff *skb_dequeue(struct sk_buff_head *arg0) {
  return (struct sk_buff *)external_alloc();
}
int __VERIFIER_nondet_int(void);
int skb_pad(struct sk_buff *arg0, int arg1) {
  return __VERIFIER_nondet_int();
}
void *external_alloc(void);
unsigned char *skb_put(struct sk_buff *arg0, unsigned int arg1) {
  return (unsigned char *)external_alloc();
}
void skb_queue_head(struct sk_buff_head *arg0, struct sk_buff *arg1) {
  return;
}
void skb_queue_purge(struct sk_buff_head *arg0) {
  return;
}
void skb_queue_tail(struct sk_buff_head *arg0, struct sk_buff *arg1) {
  return;
}
void skb_tstamp_tx(struct sk_buff *arg0, struct skb_shared_hwtstamps *arg1) {
  return;
}
unsigned long __VERIFIER_nondet_ulong(void);
size_t strlcpy(char *arg0, const char *arg1, size_t arg2) {
  return __VERIFIER_nondet_ulong();
}
void synchronize_irq(unsigned int arg0) {
  return;
}
int __VERIFIER_nondet_int(void);
int unregister_netdevice_notifier(struct notifier_block *arg0) {
  return __VERIFIER_nondet_int();
}
void unregister_netdevice_queue(struct net_device *arg0, struct list_head *arg1) {
  return;
}
void vfree(const void *arg0) {
  return;
}
void *external_alloc(void);
void *vzalloc(unsigned long arg0) {
  return (void *)external_alloc();
}
void warn_slowpath_fmt(const char *arg0, const int arg1, const char *arg2, ...) {
  return;
}
void warn_slowpath_null(const char *arg0, const int arg1) {
  return;
}
bool __VERIFIER_nondet_bool(void);
bool zalloc_cpumask_var(cpumask_var_t **arg0, gfp_t arg1) {
  return __VERIFIER_nondet_bool();
}
void *__VERIFIER_nondet_pointer(void);
void *external_alloc(void) {
  return __VERIFIER_nondet_pointer();
}
void free(void *);
void kfree(void const *p) {
  free((void *)p);
}
